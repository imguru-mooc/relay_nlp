{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49.], shape=(50,), dtype=float32)\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49.]\n",
      "(50,)\n",
      "(50, 1)\n",
      "tf.Tensor(\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [11.]\n",
      " [12.]\n",
      " [13.]\n",
      " [14.]\n",
      " [15.]\n",
      " [16.]\n",
      " [17.]\n",
      " [18.]\n",
      " [19.]\n",
      " [20.]\n",
      " [21.]\n",
      " [22.]\n",
      " [23.]\n",
      " [24.]\n",
      " [25.]\n",
      " [26.]\n",
      " [27.]\n",
      " [28.]\n",
      " [29.]\n",
      " [30.]\n",
      " [31.]\n",
      " [32.]\n",
      " [33.]\n",
      " [34.]\n",
      " [35.]\n",
      " [36.]\n",
      " [37.]\n",
      " [38.]\n",
      " [39.]\n",
      " [40.]\n",
      " [41.]\n",
      " [42.]\n",
      " [43.]\n",
      " [44.]\n",
      " [45.]\n",
      " [46.]\n",
      " [47.]\n",
      " [48.]\n",
      " [49.]], shape=(50, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "position=50\n",
    "a = tf.range(position, dtype=tf.float32)\n",
    "print(a)\n",
    "print(a.numpy())\n",
    "print(a.shape)\n",
    "\n",
    "a = tf.range(position, dtype=tf.float32)[:, tf.newaxis]\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128)\n",
      "tf.Tensor(\n",
      "[[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "   14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "   28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
      "   42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
      "   56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
      "   70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
      "   84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
      "   98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
      "  112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
      "  126. 127.]], shape=(1, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "d_model = 128\n",
    "i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
    "print(i.shape)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "[[ 3]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [12]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(1,5).reshape(4,1)\n",
    "b = 3\n",
    "c = a*b  # (4,1)*3 => (4,1)(4,1)\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "[[ 3]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [12]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(1,5).reshape(4,1)\n",
    "b = np.array([3])\n",
    "c = a*b  # (4,1)*(1,) => (4,1)(4,1)\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "[[ 3  6  9]\n",
      " [12 15 18]\n",
      " [21 24 27]\n",
      " [30 33 36]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(1,13).reshape(4,3)\n",
    "b = np.array([[3],\n",
    "              [3],\n",
    "              [3],\n",
    "              [3]])\n",
    "c = a*b  # (4,3)*(4,1) => (4,3)(4,3)\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]]\n",
      "[[3 6 9]\n",
      " [3 6 9]\n",
      " [3 6 9]\n",
      " [3 6 9]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(1,4).reshape(1,3)\n",
    "b = np.array([[3],\n",
    "              [3],\n",
    "              [3],\n",
    "              [3]])\n",
    "c = a*b  # (1,3)*(4,1) => (4,3)(4,3)\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.math.sin(90.*np.pi/180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "#         print(\"PositionalEncoding.__init__()\", position, d_model)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "#         print(\"PositionalEncoding.get_angles()\")\n",
    "#         print(position)\n",
    "#         print(i)\n",
    "#         print(d_model)\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "#         print(angles.shape)\n",
    "#         print(angles)\n",
    "        return position * angles    #   (5,1)*(1,8) => (5,8)*(5,8)\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "#         print(\"PositionalEncoding.positional_encoding()\", position, d_model)\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "#         print(angle_rads)\n",
    "        # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "#         print(sines)\n",
    "        \n",
    "        # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "#         print(cosines)\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "#         print(pos_encoding.shape)\n",
    "#         print(pos_encoding)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "#         print(pos_encoding.shape)\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.          0.          0.         ...  1.          1.\n",
      "    1.        ]\n",
      "  [ 0.841471    0.7617204   0.68156135 ...  1.          1.\n",
      "    1.        ]\n",
      "  [ 0.90929747  0.98704624  0.99748    ...  0.99999994  0.99999994\n",
      "    1.        ]\n",
      "  ...\n",
      "  [ 0.6569866  -0.21963017 -0.8593135  ...  0.9999994   0.9999996\n",
      "    0.9999997 ]\n",
      "  [ 0.98935825  0.60082203 -0.28022808 ...  0.9999992   0.9999994\n",
      "    0.9999996 ]\n",
      "  [ 0.4121185   0.99818236  0.4491935  ...  0.99999905  0.9999993\n",
      "    0.99999946]]], shape=(1, 10, 128), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_pos_encoding = PositionalEncoding(50, 128)\n",
    "# print(sample_pos_encoding.pos_encoding)\n",
    "                                            # (N,T,D)\n",
    "                                            # tf.shape(inputs)[1]\n",
    "print(sample_pos_encoding.pos_encoding[:, :10, :])\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 128))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 1.  2.  3.]\n",
      "   [ 4.  5.  6.]\n",
      "   [ 7.  8.  9.]\n",
      "   [10. 11. 12.]]\n",
      "\n",
      "  [[13. 14. 15.]\n",
      "   [16. 17. 18.]\n",
      "   [19. 20. 21.]\n",
      "   [22. 23. 24.]]]], shape=(1, 2, 4, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]]], shape=(1, 2, 4, 3), dtype=float32)\n",
      "matmul_qk= tf.Tensor(\n",
      "[[[[ 6.  6.  6.  6.]\n",
      "   [15. 15. 15. 15.]\n",
      "   [24. 24. 24. 24.]\n",
      "   [33. 33. 33. 33.]]\n",
      "\n",
      "  [[42. 42. 42. 42.]\n",
      "   [51. 51. 51. 51.]\n",
      "   [60. 60. 60. 60.]\n",
      "   [69. 69. 69. 69.]]]], shape=(1, 2, 4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "query = tf.constant(np.arange(1,25).reshape(1,2,4,3),dtype=tf.float32)\n",
    "print(query)\n",
    "\n",
    "key = tf.constant(np.ones((1,2,4,3)),dtype=tf.float32)\n",
    "print(key)\n",
    "\n",
    "matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "print(\"matmul_qk=\", matmul_qk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "    # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
    "\n",
    "    # Q와 K의 곱. 어텐션 스코어 행렬.\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True) # (1,4,4,128)(1,4,128,4)\n",
    "    print(\"matmul_qk.shape =\", matmul_qk.shape)         # (1,4,4,4)\n",
    "    # 스케일링\n",
    "    # dk의 루트값으로 나눠준다.\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    print(\"depth=\",depth)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "    print(\"logits=\",logits)\n",
    "\n",
    "    # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n",
    "    # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n",
    "    # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "    print(\"attention_weights.shape =\",attention_weights.shape)  # (1,4,4,4)\n",
    "    # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    output = tf.matmul(attention_weights, value) # (1,4,4,4)(1,4,4,32) => (1,4,4,32)\n",
    "    print(\"output.shape =\", output.shape) \n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul_qk.shape = (1, 4)\n",
      "depth= tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor([[ 0.       57.735027  0.        0.      ]], shape=(1, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 4)\n",
      "out.shape = (1, 4)\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)\n",
    "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
    "print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\n",
    "print(temp_out) # 어텐션 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul_qk= tf.Tensor(\n",
      "[[  0.   0. 100. 100.]\n",
      " [  0. 100.   0.   0.]\n",
      " [100. 100.   0.   0.]], shape=(3, 4), dtype=float32)\n",
      "depth= tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[ 0.        0.       57.735027 57.735027]\n",
      " [ 0.       57.735027  0.        0.      ]\n",
      " [57.735027 57.735027  0.        0.      ]], shape=(3, 4), dtype=float32)\n",
      "attention_weights= tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
    "print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\n",
    "print(temp_out) # 어텐션 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "(1, 3)\n",
      "[<tf.Variable 'dense_9/kernel:0' shape=(3, 4) dtype=float32, numpy=\n",
      "array([[-0.55661595,  0.82509804,  0.26806664,  0.5745529 ],\n",
      "       [ 0.38482738,  0.18366551,  0.59290886, -0.3469832 ],\n",
      "       [ 0.18667257,  0.6465657 , -0.30440807, -0.02304095]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_9/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "dense1 = tf.keras.layers.Dense(4)  # weight = (?,4)  , bias = (4,)\n",
    "# dir(dense1)\n",
    "print(dense1.weights)\n",
    "x = tf.constant([[1,2,3]])\n",
    "print(x.shape)\n",
    "out = dense1(x)   # (1,3)(3,4)+(4,)\n",
    "print(dense1.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAA.foo()\n",
      "AAA.call()\n"
     ]
    }
   ],
   "source": [
    "class MyLayer:\n",
    "    def __call__(self):\n",
    "        self.call()\n",
    "\n",
    "class AAA(MyLayer):\n",
    "    def foo(self):\n",
    "        print(\"AAA.foo()\")\n",
    "        \n",
    "    def call(self):\n",
    "        print(\"AAA.call()\")\n",
    "\n",
    "aaa = AAA()\n",
    "aaa.foo()\n",
    "aaa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        \n",
    "        print(\"MultiHeadAttention.__init__()\")\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        # d_model을 num_heads로 나눈 값.\n",
    "        # 논문 기준 : 64\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        # WQ, WK, WV에 해당하는 밀집층 정의\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        # WO에 해당하는 밀집층 정의\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  # num_heads 개수만큼 q, k, v를 split하는 함수\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        print(\"split_heads()\")\n",
    "        print(inputs.shape)\n",
    "        inputs = tf.reshape(                                             # (1,4,128)\n",
    "                                                                         # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))  # (1,4,4,32)\n",
    "                                                                         # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        print(inputs.shape)                                              # (1,4,4,32)\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "        # q : (batch_size, query의 문장 길이, d_model)  =>  (1,4,128)\n",
    "        # k : (batch_size, key의 문장 길이, d_model)\n",
    "        # v : (batch_size, value의 문장 길이, d_model)\n",
    "        # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "        query = self.query_dense(query)    # (1,4,128)(128,128) => (1,4,128)\n",
    "        key = self.key_dense(key)          # (1,4,128)(128,128) => (1,4,128)\n",
    "        value = self.value_dense(value)    # (1,4,128)(128,128) => (1,4,128)\n",
    "        \n",
    "        print(query.shape)\n",
    "        print(key.shape)\n",
    "        print(value.shape)\n",
    "        \n",
    "#         print(query[0,0,:])\n",
    "#         print(key[0,0,:])\n",
    "\n",
    "        # 2. 헤드 나누기\n",
    "        # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "        # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        print(query.shape)\n",
    "        print(key.shape)\n",
    "        print(value.shape)\n",
    "        \n",
    "\n",
    "        # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
    "        # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "        # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        print('scaled_attention.shape=',scaled_attention.shape)\n",
    "        \n",
    "        # 4. 헤드 연결(concatenate)하기\n",
    "        # (batch_size, query의 문장 길이, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "        print('concat_attention.shape=',concat_attention.shape)\n",
    "\n",
    "        # 5. WO에 해당하는 밀집층 지나기\n",
    "        # (batch_size, query의 문장 길이, d_model)\n",
    "        outputs = self.dense(concat_attention)  # (1,4,128)(128,128) => (1,4,128)\n",
    "        print('outputs.shape=',outputs.shape)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention.__init__()\n",
      "(1, 4, 128)\n",
      "(1, 4, 128)\n",
      "(1, 4, 128)\n",
      "split_heads()\n",
      "(1, 4, 128)\n",
      "(1, 4, 4, 32)\n",
      "split_heads()\n",
      "(1, 4, 128)\n",
      "(1, 4, 4, 32)\n",
      "split_heads()\n",
      "(1, 4, 128)\n",
      "(1, 4, 4, 32)\n",
      "(1, 4, 4, 32)\n",
      "(1, 4, 4, 32)\n",
      "(1, 4, 4, 32)\n",
      "matmul_qk.shape = (1, 4, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 1.7398771  -2.1643188   0.41156888  0.07043239]\n",
      "   [-0.27048534 -0.6996536  -0.61537534  1.8035806 ]\n",
      "   [-1.5737541  -0.2142834  -1.8019285   0.428533  ]\n",
      "   [-0.85640764  1.4194956   0.14027998 -0.09724399]]\n",
      "\n",
      "  [[-1.1643927   0.00268122  0.07210209 -0.2735874 ]\n",
      "   [-0.49486294  0.12640677  0.570364    0.03135174]\n",
      "   [ 0.41558307  0.5080746  -0.10121492  0.548831  ]\n",
      "   [-0.50739986  0.27554736  0.88465756  0.8942639 ]]\n",
      "\n",
      "  [[-1.244622   -0.6136496   0.2805493   0.6679138 ]\n",
      "   [-0.66873485  1.1642151   1.3082943  -1.589387  ]\n",
      "   [-0.1016885  -3.0306976  -0.37181753 -0.11320281]\n",
      "   [ 1.0955998   0.97543067  0.07731257 -0.82188904]]\n",
      "\n",
      "  [[-0.20262912  1.2863495  -1.4252692  -0.68121207]\n",
      "   [-0.5752793   0.2984593   2.0267115   0.19753048]\n",
      "   [-0.6331656  -0.38507903 -1.1441828  -1.1581669 ]\n",
      "   [-0.8900123   0.7164453   0.38631988  0.761838  ]]]], shape=(1, 4, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 4, 4, 4)\n",
      "output.shape = (1, 4, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 4, 32)\n",
      "concat_attention.shape= (1, 4, 128)\n",
      "outputs.shape= (1, 4, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 128), dtype=float32, numpy=\n",
       "array([[[ 1.0035399 , -1.020905  , -0.50631714, -0.36592218,\n",
       "          0.671305  ,  0.03473164,  0.5851499 ,  0.5232848 ,\n",
       "          0.22999378,  1.2364036 ,  1.1699761 , -0.44883388,\n",
       "          0.06525836,  0.31910345,  1.0539893 ,  0.0708446 ,\n",
       "          0.83263826,  0.5591353 ,  1.1927273 ,  0.7277354 ,\n",
       "          1.1419245 , -0.50910413,  0.19744277,  0.7119322 ,\n",
       "         -0.78679925, -0.80747014,  0.2701991 , -0.24448211,\n",
       "          0.09912057,  0.0107861 , -0.31085268,  0.14942917,\n",
       "          0.21453317, -0.26130947,  0.13440429, -0.2960468 ,\n",
       "         -1.1826576 ,  0.6566272 , -1.1392852 ,  1.1487495 ,\n",
       "         -0.7093831 , -0.09061407, -0.47957224,  0.44660354,\n",
       "          0.16301167, -0.753829  ,  0.02921122,  0.23778833,\n",
       "         -0.49838585, -0.16901098,  0.3296993 , -0.1178278 ,\n",
       "         -1.086935  , -0.8376639 ,  0.06499963, -0.34968758,\n",
       "          0.5421539 , -0.21439746,  0.7158248 , -0.24794605,\n",
       "          1.1707609 , -1.3057268 , -0.57195574,  0.08089843,\n",
       "         -0.94712573, -0.83247864,  0.49211243,  0.23021579,\n",
       "         -0.7124779 , -1.8735707 ,  0.07202151, -0.05234681,\n",
       "          0.29095456,  1.3171536 , -0.2740906 ,  0.4469849 ,\n",
       "         -0.2554568 ,  0.20283985, -0.9479457 , -0.11946573,\n",
       "         -0.27941138,  0.00346832,  0.57477784, -0.88838285,\n",
       "         -0.10091971,  0.55967075,  0.04936828,  0.9582187 ,\n",
       "          0.28931051,  0.952101  ,  0.23911443,  0.4914132 ,\n",
       "          0.42895392,  0.19954923,  0.68265975, -0.92483616,\n",
       "          0.0083985 , -0.13206749,  0.15090926, -0.19439273,\n",
       "         -0.2847408 ,  0.35949332, -1.1784545 , -0.6948431 ,\n",
       "         -0.6129895 , -0.9289986 ,  0.309827  , -0.42103174,\n",
       "         -0.1163021 , -1.0178334 , -1.2753761 ,  0.36432186,\n",
       "         -0.08848579,  0.26161167, -0.46681628, -0.23152812,\n",
       "         -0.5491987 ,  0.46535498,  0.34099686,  0.36933059,\n",
       "         -0.37924004,  0.6789948 ,  0.48983476, -0.09223554,\n",
       "         -0.3343018 , -0.54899925,  0.15915576,  0.91535276],\n",
       "        [-0.8075586 , -0.45893222, -0.30763173, -0.1975243 ,\n",
       "          0.5926549 , -0.27513197, -0.56301016,  0.27683413,\n",
       "         -0.03943522,  0.4978525 ,  0.91832036,  0.2281741 ,\n",
       "          0.8678337 ,  1.8407687 ,  0.90679866, -0.08840845,\n",
       "          1.454882  , -0.5185243 ,  0.650792  ,  0.14589131,\n",
       "         -0.50709075,  0.8543176 ,  1.0002779 ,  0.45866522,\n",
       "         -0.2623383 , -0.24147047,  0.9769855 ,  0.276186  ,\n",
       "         -0.6475112 , -0.6288835 ,  0.8513618 ,  0.172946  ,\n",
       "          0.65133035,  0.3255229 , -0.66247433,  0.1641683 ,\n",
       "          0.00695977,  0.02085386, -0.42409515, -0.8080372 ,\n",
       "         -0.4981033 ,  0.08954367,  0.20292103,  0.7357497 ,\n",
       "          0.6549669 , -0.55694264,  0.15706056,  0.67483616,\n",
       "         -0.19161299,  0.5310233 ,  0.7708896 ,  0.5623561 ,\n",
       "         -1.7097313 ,  0.4750719 ,  0.6416418 , -0.23273335,\n",
       "          0.0875512 ,  0.04963463, -0.20146711, -1.0765693 ,\n",
       "          0.04464497, -0.42982256, -0.63479036, -1.3634403 ,\n",
       "          0.13740224,  0.42669567, -0.9595203 , -0.8964787 ,\n",
       "          1.1421088 , -0.55969864,  0.84737617, -0.74983126,\n",
       "         -1.1804602 ,  1.6613944 , -1.0242962 ,  0.34133312,\n",
       "         -0.8574271 ,  0.10122218, -1.176314  , -0.5443331 ,\n",
       "          1.4145101 , -0.05873406,  0.6880063 ,  0.09938598,\n",
       "          0.7291838 , -0.31631124,  0.4227876 ,  0.19111682,\n",
       "          0.09301248, -0.41863778,  0.02036199,  0.00023171,\n",
       "         -0.14848316, -0.17811275, -0.13452023, -1.0901188 ,\n",
       "          0.00691465,  1.2666572 ,  0.81431806, -0.3467397 ,\n",
       "         -0.9556817 ,  0.20394732, -0.06986395, -0.14212193,\n",
       "         -0.78560257, -0.6909712 , -0.4547189 , -0.6043538 ,\n",
       "          0.67344475, -0.8856253 , -0.737428  ,  0.4377737 ,\n",
       "          0.48458302, -0.09488405,  0.02879732, -1.1520324 ,\n",
       "          1.1220735 ,  0.30613834, -0.65784633,  0.17695075,\n",
       "         -0.30580974,  1.373667  ,  0.7444482 ,  0.45743987,\n",
       "         -0.0838344 ,  0.9466729 , -1.246398  , -0.4550436 ],\n",
       "        [ 0.2663128 , -0.11479945, -0.8094714 ,  0.09711965,\n",
       "          0.45989648,  0.34218344, -0.29902613,  0.836953  ,\n",
       "          0.4867473 ,  0.54581606,  0.33346963, -0.46887854,\n",
       "          0.20670564,  1.2101731 ,  0.79401696,  1.1581216 ,\n",
       "          0.99008113,  0.430179  ,  1.1060606 ,  0.08468593,\n",
       "         -0.18829569,  0.10061928,  0.8984144 ,  1.4290483 ,\n",
       "         -0.42752382,  0.00062881,  0.8482295 ,  0.17101339,\n",
       "          0.29632652,  0.10649712,  0.28566095,  0.38943484,\n",
       "         -0.07407635, -0.01690976, -0.7187481 , -0.34283915,\n",
       "         -0.0454485 ,  0.47011632, -0.5929596 ,  0.45259732,\n",
       "         -1.1941372 , -0.06012866, -1.1346518 ,  0.46959186,\n",
       "          0.26227427, -0.6067468 , -0.19202898,  0.44138625,\n",
       "         -0.50771034, -0.26340112,  0.33956102,  0.1827247 ,\n",
       "         -1.2770816 , -0.26331055,  0.4841612 , -0.8072562 ,\n",
       "          0.8224718 ,  0.6036456 ,  0.7570143 , -0.36838818,\n",
       "          0.6408103 , -0.40671623,  0.02538273, -0.43805107,\n",
       "         -1.0106994 ,  0.20831256,  0.02472474, -1.0534033 ,\n",
       "          0.8976014 , -0.42028654,  0.08882494, -0.06889876,\n",
       "         -0.05560764,  1.6213917 , -0.10064123,  0.69408995,\n",
       "         -0.48905107, -0.03367127, -0.44536406, -0.9042699 ,\n",
       "          0.7208835 , -0.7068873 ,  0.21128939, -0.1615126 ,\n",
       "          0.326165  , -0.21654864, -0.12607865,  0.4137893 ,\n",
       "         -0.34957254, -0.11762486,  0.7981069 , -0.01761072,\n",
       "         -0.11560522,  0.25889322,  0.19598122, -0.20484492,\n",
       "         -0.3112002 , -0.13895151,  0.52135235, -0.47756445,\n",
       "         -0.0832659 ,  0.29301378, -1.0364709 , -0.55429995,\n",
       "         -0.43897817,  0.11183942,  0.42668357, -0.7379847 ,\n",
       "          1.2237563 , -1.0841525 , -1.0433604 , -0.25998583,\n",
       "          0.3279937 ,  0.28594124, -1.1593397 , -0.92791253,\n",
       "          0.47585127,  0.6666809 ,  0.01827587, -0.13272163,\n",
       "          0.0447956 ,  1.5168958 ,  0.39680076, -0.23701276,\n",
       "          0.57399166,  0.10815933, -0.03875032,  0.27550814],\n",
       "        [ 0.3605747 ,  0.13590835, -0.8703922 ,  0.20958342,\n",
       "          0.40180767,  0.26960367, -0.01790185, -0.28117788,\n",
       "          0.37339726,  0.09044164, -0.16898513, -0.71984607,\n",
       "         -0.21760845,  0.45219958,  1.2015989 ,  0.3074618 ,\n",
       "          1.1068444 ,  0.53254086,  1.1437198 , -0.08695945,\n",
       "          0.35814238,  0.4795419 ,  1.6894773 ,  1.0584161 ,\n",
       "          0.17319101, -0.19414383,  1.078791  , -0.13743109,\n",
       "          0.4000286 ,  0.6097269 ,  0.11559062,  0.39629248,\n",
       "         -0.1444398 , -0.29251745, -0.15246056, -0.26903358,\n",
       "         -0.4424424 , -0.428224  , -0.34670034,  0.02032728,\n",
       "         -0.3993502 , -0.26248902, -0.7219009 ,  0.6348374 ,\n",
       "          0.7323209 , -0.6624949 ,  0.28910542,  0.7118628 ,\n",
       "         -0.06554238, -0.33018363,  0.9807897 , -0.0378442 ,\n",
       "         -0.41527155,  0.21894507,  0.53821486, -0.12065963,\n",
       "          0.26791102,  0.8379445 ,  0.72473836,  0.52549124,\n",
       "          0.50753266, -0.8067279 , -0.630259  , -0.9955701 ,\n",
       "         -1.1642528 ,  0.2074882 , -0.18943807, -1.0873823 ,\n",
       "          1.1243094 , -0.44845033,  0.4049955 , -0.37751144,\n",
       "         -0.27890372,  1.2296911 , -0.48348555,  0.81004643,\n",
       "         -0.10281923, -0.01361393,  0.01633016, -0.23025489,\n",
       "          0.6870221 , -0.5906566 , -0.09331018,  0.09098646,\n",
       "         -0.628701  , -0.04433252, -0.24317756,  0.02987522,\n",
       "         -0.62896764,  0.14317569,  0.01686288, -0.04675754,\n",
       "          0.10210807,  0.51583415,  0.5234362 , -0.17742538,\n",
       "         -0.1481036 ,  0.20272712, -0.01992086, -0.4818801 ,\n",
       "          0.06267595,  0.15150163, -1.1895384 ,  0.5063302 ,\n",
       "         -0.3439405 , -0.34328404,  0.09953029, -0.26615557,\n",
       "          0.40591404, -0.8651626 , -1.3249986 ,  0.51572233,\n",
       "          0.86425287, -0.23290613, -0.83431685, -1.3264855 ,\n",
       "          0.4969229 ,  0.19350162,  0.48933062,  0.37061208,\n",
       "          0.48872393,  0.92620105, -0.31899327, -0.10399604,\n",
       "         -0.07784713,  0.354825  ,  0.14833377,  0.47395623]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(128,4)\n",
    "x = tf.constant(np.random.randn(1,4,128))\n",
    "inputs = { 'query':x, 'key':x, 'value':x, 'mask':None }\n",
    "mha(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, key의 문장 길이)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_padding_mask(tf.constant([[1, 21, 777, 0, 0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # 인코더는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "            'mask': padding_mask # 패딩 마스크 사용\n",
    "        })\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 인코더는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 포지셔널 인코딩 + 드롭아웃\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # 인코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "      outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "          dropout=dropout, name=\"encoder_layer_{}\".format(i),\n",
    "      )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "#     return look_ahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_look_ahead_mask(tf.constant([[4, 0, 0, 2, 5]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "\n",
    "    # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "            'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "            'mask': look_ahead_mask # 룩어헤드 마스크\n",
    "        })\n",
    "\n",
    "    # 잔차 연결과 층 정규화\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n",
    "            'mask': padding_mask # 패딩 마스크\n",
    "        })\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "\n",
    "    # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 포지셔널 인코딩 + 드롭아웃\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # 디코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "            dropout=dropout, name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size, num_layers, dff,\n",
    "                  d_model, num_heads, dropout,\n",
    "                  name=\"transformer\"):\n",
    "\n",
    "    # 인코더의 입력\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 디코더의 입력\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 인코더의 패딩 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 디코더의 룩어헤드 마스크(첫번째 서브층)\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask, output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 디코더의 패딩 마스크(두번째 서브층)\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n",
    "    enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "        d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n",
    "\n",
    "    # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n",
    "    dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "        d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 다음 단어 예측을 위한 출력층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_transformer = transformer(\n",
    "    vocab_size = 9000,\n",
    "    num_layers = 4,\n",
    "    dff = 512,\n",
    "    d_model = 128,\n",
    "    num_heads = 4,\n",
    "    dropout = 0.3,\n",
    "    name=\"small_transformer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    small_transformer, to_file='small_transformer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n",
    "# Text(0.5, 0, 'Train Step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow_datasets 처음 설치 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow_datasets 업그레이드 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --user --upgrade tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfds.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv\", filename=\"ChatBotData.csv\")\n",
    "train_data = pd.read_csv('ChatBotData.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('챗봇 샘플의 개수 :', len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "for sentence in train_data['Q']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    # ex) 12시 땡! -> 12시 땡 !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    questions.append(sentence)\n",
    "answers = []\n",
    "for sentence in train_data['A']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    # ex) 12시 땡! -> 12시 땡 !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    answers.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questions[:5])\n",
    "print(answers[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서브워드텍스트인코더를 사용하여 질문, 답변 데이터로부터 단어 집합(Vocabulary) 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 대한 정수 부여.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('시작 토큰 번호 :',START_TOKEN)\n",
    "print('종료 토큰 번호 :',END_TOKEN)\n",
    "print('단어 집합의 크기 :',VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서브워드텍스트인코더 토크나이저의 .encode()를 사용하여 텍스트 시퀀스를 정수 시퀀스로 변환.\n",
    "print('임의의 질문 샘플을 정수 인코딩 : {}'.format(tokenizer.encode(questions[20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서브워드텍스트인코더 토크나이저의 .encode()와 .decode() 테스트해보기\n",
    "# 임의의 입력 문장을 sample_string에 저장\n",
    "sample_string = questions[20]\n",
    "\n",
    "# encode() : 텍스트 시퀀스 --> 정수 시퀀스\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# decode() : 정수 시퀀스 --> 텍스트 시퀀스\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in tokenized_string:\n",
    "    print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이를 40으로 정의\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    tokenized_inputs.append(sentence1)\n",
    "    tokenized_outputs.append(sentence2)\n",
    "\n",
    "  # 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "questions, answers = tokenize_and_filter(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('질문 데이터의 크기(shape) :', questions.shape)\n",
    "print('답변 데이터의 크기(shape) :', answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questions[0])\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더의 실제값 시퀀스에서는 시작 토큰을 제거해야 한다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1] # 디코더의 입력. 마지막 패딩 토큰이 제거된다.\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다.\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "# 임의의 샘플에 대해서 [:, :-1]과 [:, 1:]이 어떤 의미를 가지는지 테스트해본다.\n",
    "print(answers[0]) # 기존 샘플\n",
    "print(answers[:1][:, :-1]) # 마지막 패딩 토큰 제거하면서 길이가 39가 된다.\n",
    "print(answers[:1][:, 1:]) # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다. 길이는 역시 39가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 256\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dff=DFF,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  # 레이블의 크기는 (batch_size, MAX_LENGTH - 1)\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 예측 시작\n",
    "  for i in range(MAX_LENGTH):\n",
    "    predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "    # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
    "    # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0)\n",
    "def predict(sentence):\n",
    "  prediction = evaluate(sentence)\n",
    "\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict(\"날씨가 좋넹.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict(\"고민이 있어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict(\"너무 화가나\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict(\"카페갈래?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict(\"게임하고싶당\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predict(\"게임하자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
