{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49.], shape=(50,), dtype=float32)\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49.]\n",
      "(50,)\n",
      "(50, 1)\n",
      "tf.Tensor(\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [11.]\n",
      " [12.]\n",
      " [13.]\n",
      " [14.]\n",
      " [15.]\n",
      " [16.]\n",
      " [17.]\n",
      " [18.]\n",
      " [19.]\n",
      " [20.]\n",
      " [21.]\n",
      " [22.]\n",
      " [23.]\n",
      " [24.]\n",
      " [25.]\n",
      " [26.]\n",
      " [27.]\n",
      " [28.]\n",
      " [29.]\n",
      " [30.]\n",
      " [31.]\n",
      " [32.]\n",
      " [33.]\n",
      " [34.]\n",
      " [35.]\n",
      " [36.]\n",
      " [37.]\n",
      " [38.]\n",
      " [39.]\n",
      " [40.]\n",
      " [41.]\n",
      " [42.]\n",
      " [43.]\n",
      " [44.]\n",
      " [45.]\n",
      " [46.]\n",
      " [47.]\n",
      " [48.]\n",
      " [49.]], shape=(50, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "position=50\n",
    "a = tf.range(position, dtype=tf.float32)\n",
    "print(a)\n",
    "print(a.numpy())\n",
    "print(a.shape)\n",
    "\n",
    "a = tf.range(position, dtype=tf.float32)[:, tf.newaxis]\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128)\n",
      "tf.Tensor(\n",
      "[[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
      "   14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
      "   28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
      "   42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
      "   56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
      "   70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
      "   84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
      "   98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
      "  112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
      "  126. 127.]], shape=(1, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "d_model = 128\n",
    "i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
    "print(i.shape)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "[[ 3]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [12]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(1,5).reshape(4,1)\n",
    "b = 3\n",
    "c = a*b  # (4,1)*3 => (4,1)(4,1)\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "[[ 3]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [12]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(1,5).reshape(4,1)\n",
    "b = np.array([3])\n",
    "c = a*b  # (4,1)*(1,) => (4,1)(4,1)\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "[[ 3  6  9]\n",
      " [12 15 18]\n",
      " [21 24 27]\n",
      " [30 33 36]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(1,13).reshape(4,3)\n",
    "b = np.array([[3],\n",
    "              [3],\n",
    "              [3],\n",
    "              [3]])\n",
    "c = a*b  # (4,3)*(4,1) => (4,3)(4,3)\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]]\n",
      "[[3 6 9]\n",
      " [3 6 9]\n",
      " [3 6 9]\n",
      " [3 6 9]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(1,4).reshape(1,3)\n",
    "b = np.array([[3],\n",
    "              [3],\n",
    "              [3],\n",
    "              [3]])\n",
    "c = a*b  # (1,3)*(4,1) => (4,3)(4,3)\n",
    "print(a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.math.sin(90.*np.pi/180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "#         print(\"PositionalEncoding.__init__()\", position, d_model)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "#         print(\"PositionalEncoding.get_angles()\")\n",
    "#         print(position)\n",
    "#         print(i)\n",
    "#         print(d_model)\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "#         print(angles.shape)\n",
    "#         print(angles)\n",
    "        return position * angles    #   (5,1)*(1,8) => (5,8)*(5,8)\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "#         print(\"PositionalEncoding.positional_encoding()\", position, d_model)\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "#         print(angle_rads)\n",
    "        # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "#         print(sines)\n",
    "        \n",
    "        # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "#         print(cosines)\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "#         print(pos_encoding.shape)\n",
    "#         print(pos_encoding)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "#         print(pos_encoding.shape)\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.          0.          0.         ...  1.          1.\n",
      "    1.        ]\n",
      "  [ 0.841471    0.7617204   0.68156135 ...  1.          1.\n",
      "    1.        ]\n",
      "  [ 0.90929747  0.98704624  0.99748    ...  0.99999994  0.99999994\n",
      "    1.        ]\n",
      "  ...\n",
      "  [ 0.6569866  -0.21963017 -0.8593135  ...  0.9999994   0.9999996\n",
      "    0.9999997 ]\n",
      "  [ 0.98935825  0.60082203 -0.28022808 ...  0.9999992   0.9999994\n",
      "    0.9999996 ]\n",
      "  [ 0.4121185   0.99818236  0.4491935  ...  0.99999905  0.9999993\n",
      "    0.99999946]]], shape=(1, 10, 128), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_pos_encoding = PositionalEncoding(50, 128)\n",
    "# print(sample_pos_encoding.pos_encoding)\n",
    "                                            # (N,T,D)\n",
    "                                            # tf.shape(inputs)[1]\n",
    "print(sample_pos_encoding.pos_encoding[:, :10, :])\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 128))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 1.  2.  3.]\n",
      "   [ 4.  5.  6.]\n",
      "   [ 7.  8.  9.]\n",
      "   [10. 11. 12.]]\n",
      "\n",
      "  [[13. 14. 15.]\n",
      "   [16. 17. 18.]\n",
      "   [19. 20. 21.]\n",
      "   [22. 23. 24.]]]], shape=(1, 2, 4, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]\n",
      "\n",
      "  [[1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]\n",
      "   [1. 1. 1.]]]], shape=(1, 2, 4, 3), dtype=float32)\n",
      "matmul_qk= tf.Tensor(\n",
      "[[[[ 6.  6.  6.  6.]\n",
      "   [15. 15. 15. 15.]\n",
      "   [24. 24. 24. 24.]\n",
      "   [33. 33. 33. 33.]]\n",
      "\n",
      "  [[42. 42. 42. 42.]\n",
      "   [51. 51. 51. 51.]\n",
      "   [60. 60. 60. 60.]\n",
      "   [69. 69. 69. 69.]]]], shape=(1, 2, 4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "query = tf.constant(np.arange(1,25).reshape(1,2,4,3),dtype=tf.float32)\n",
    "print(query)\n",
    "\n",
    "key = tf.constant(np.ones((1,2,4,3)),dtype=tf.float32)\n",
    "print(key)\n",
    "\n",
    "matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "print(\"matmul_qk=\", matmul_qk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "    # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
    "\n",
    "    # Q와 K의 곱. 어텐션 스코어 행렬.\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True) # (1,4,4,128)(1,4,128,4)\n",
    "    print(\"matmul_qk.shape =\", matmul_qk.shape)         # (1,4,4,4)\n",
    "    # 스케일링\n",
    "    # dk의 루트값으로 나눠준다.\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    print(\"depth=\",depth)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "    print(\"logits=\",logits)\n",
    "\n",
    "    # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n",
    "    # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n",
    "    # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "    print(\"attention_weights.shape =\",attention_weights.shape)  # (1,4,4,4)\n",
    "    # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    output = tf.matmul(attention_weights, value) # (1,4,4,4)(1,4,4,32) => (1,4,4,32)\n",
    "    print(\"output.shape =\", output.shape) \n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul_qk.shape = (1, 4)\n",
      "depth= tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor([[ 0.       57.735027  0.        0.      ]], shape=(1, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 4)\n",
      "output.shape = (1, 2)\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)\n",
    "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
    "print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\n",
    "print(temp_out) # 어텐션 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul_qk.shape = (3, 4)\n",
      "depth= tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[ 0.        0.       57.735027 57.735027]\n",
      " [ 0.       57.735027  0.        0.      ]\n",
      " [57.735027 57.735027  0.        0.      ]], shape=(3, 4), dtype=float32)\n",
      "attention_weights.shape = (3, 4)\n",
      "output.shape = (3, 2)\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
    "print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\n",
    "print(temp_out) # 어텐션 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "(1, 3)\n",
      "[<tf.Variable 'dense/kernel:0' shape=(3, 4) dtype=float32, numpy=\n",
      "array([[ 0.51615655,  0.9097786 ,  0.237715  ,  0.30533254],\n",
      "       [-0.26314604,  0.87166953,  0.7337655 ,  0.6187154 ],\n",
      "       [-0.84374774,  0.175565  ,  0.7219163 ,  0.28337204]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "dense1 = tf.keras.layers.Dense(4)  # weight = (?,4)  , bias = (4,)\n",
    "# dir(dense1)\n",
    "print(dense1.weights)\n",
    "x = tf.constant([[1,2,3]])\n",
    "print(x.shape)\n",
    "out = dense1(x)   # (1,3)(3,4)+(4,)\n",
    "print(dense1.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAA.foo()\n",
      "AAA.call()\n"
     ]
    }
   ],
   "source": [
    "class MyLayer:\n",
    "    def __call__(self):\n",
    "        self.call()\n",
    "\n",
    "class AAA(MyLayer):\n",
    "    def foo(self):\n",
    "        print(\"AAA.foo()\")\n",
    "        \n",
    "    def call(self):\n",
    "        print(\"AAA.call()\")\n",
    "\n",
    "aaa = AAA()\n",
    "aaa.foo()\n",
    "aaa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        \n",
    "        print(\"MultiHeadAttention.__init__()\")\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        # d_model을 num_heads로 나눈 값.\n",
    "        # 논문 기준 : 64\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        # WQ, WK, WV에 해당하는 밀집층 정의\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        # WO에 해당하는 밀집층 정의\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  # num_heads 개수만큼 q, k, v를 split하는 함수\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        print(\"split_heads()\")\n",
    "        print(inputs.shape)\n",
    "        inputs = tf.reshape(                                             # (1,4,128)\n",
    "                                                                         # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))  # (1,4,4,32)\n",
    "                                                                         # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        print(inputs.shape)                                              # (1,4,4,32)\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "        # q : (batch_size, query의 문장 길이, d_model)  =>  (1,4,128)\n",
    "        # k : (batch_size, key의 문장 길이, d_model)\n",
    "        # v : (batch_size, value의 문장 길이, d_model)\n",
    "        # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "        query = self.query_dense(query)    # (1,4,128)(128,128) => (1,4,128)\n",
    "        key = self.key_dense(key)          # (1,4,128)(128,128) => (1,4,128)\n",
    "        value = self.value_dense(value)    # (1,4,128)(128,128) => (1,4,128)\n",
    "        \n",
    "        print(query.shape)\n",
    "        print(key.shape)\n",
    "        print(value.shape)\n",
    "        \n",
    "#         print(query[0,0,:])\n",
    "#         print(key[0,0,:])\n",
    "\n",
    "        # 2. 헤드 나누기\n",
    "        # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "        # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        print(query.shape)\n",
    "        print(key.shape)\n",
    "        print(value.shape)\n",
    "        \n",
    "\n",
    "        # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
    "        # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "        # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        print('scaled_attention.shape=',scaled_attention.shape)\n",
    "        \n",
    "        # 4. 헤드 연결(concatenate)하기\n",
    "        # (batch_size, query의 문장 길이, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "        print('concat_attention.shape=',concat_attention.shape)\n",
    "\n",
    "        # 5. WO에 해당하는 밀집층 지나기\n",
    "        # (batch_size, query의 문장 길이, d_model)\n",
    "        outputs = self.dense(concat_attention)  # (1,4,128)(128,128) => (1,4,128)\n",
    "        print('outputs.shape=',outputs.shape)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention.__init__()\n",
      "(1, 4, 128)\n",
      "(1, 4, 128)\n",
      "(1, 4, 128)\n",
      "split_heads()\n",
      "(1, 4, 128)\n",
      "(1, 4, 4, 32)\n",
      "split_heads()\n",
      "(1, 4, 128)\n",
      "(1, 4, 4, 32)\n",
      "split_heads()\n",
      "(1, 4, 128)\n",
      "(1, 4, 4, 32)\n",
      "(1, 4, 4, 32)\n",
      "(1, 4, 4, 32)\n",
      "(1, 4, 4, 32)\n",
      "matmul_qk.shape = (1, 4, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.19531368  0.50730324  0.27039167 -1.3858031 ]\n",
      "   [ 2.2986631  -1.4550354  -0.09641149  1.3451545 ]\n",
      "   [-0.39315796 -0.02242094 -0.1568575   1.4337155 ]\n",
      "   [ 1.0246078   0.6037939  -0.6416749   0.501713  ]]\n",
      "\n",
      "  [[ 0.2971008   0.08381696 -0.39127266  1.0877726 ]\n",
      "   [ 0.8702019   1.0554622  -0.9125948  -0.66327155]\n",
      "   [-0.14250788 -0.9018558   0.0725087   0.3107218 ]\n",
      "   [ 0.5715079   1.0323457   1.0923291   1.8632497 ]]\n",
      "\n",
      "  [[ 3.504584    0.5945449  -1.8455257   1.683574  ]\n",
      "   [ 1.2571493  -0.5631031  -0.52757734  0.24041744]\n",
      "   [ 0.46022156  1.4292681  -0.44054356  0.28061903]\n",
      "   [ 1.1455792  -0.87626773  0.6716843   2.573258  ]]\n",
      "\n",
      "  [[-1.5233837   2.0066652  -0.70365655  1.3891727 ]\n",
      "   [ 0.02604983  0.20811303 -0.30781466 -0.9937291 ]\n",
      "   [ 0.15486407 -0.03370085  0.81092864  0.5507939 ]\n",
      "   [-1.1477249   0.05244219 -1.7847284   0.09912226]]]], shape=(1, 4, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 4, 4, 4)\n",
      "output.shape = (1, 4, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 4, 32)\n",
      "concat_attention.shape= (1, 4, 128)\n",
      "outputs.shape= (1, 4, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 128), dtype=float32, numpy=\n",
       "array([[[ 0.15974349,  0.22028796,  1.2447312 , -0.05068296,\n",
       "         -0.06313427, -0.13827907, -0.2859064 , -0.6557277 ,\n",
       "          1.4953666 , -0.22662306, -0.78201497, -1.9229467 ,\n",
       "          0.36176872,  0.3709221 ,  1.2016816 , -0.4965919 ,\n",
       "          0.6330465 ,  0.10289492, -1.2938311 , -0.592535  ,\n",
       "          0.67396593, -0.11888432, -1.0003039 , -0.92621154,\n",
       "          0.55627143, -0.7763991 ,  1.1317078 , -0.5627951 ,\n",
       "         -0.65364075, -0.02953834,  0.73539895, -1.2670269 ,\n",
       "          0.40682992,  0.5532873 , -0.38615075, -0.23325631,\n",
       "         -1.1747102 ,  0.01204192,  0.18304443,  1.5574468 ,\n",
       "         -0.21592508, -0.22837093, -0.06877883, -1.250449  ,\n",
       "          0.5665851 , -0.06801137, -0.7246217 ,  0.27300033,\n",
       "          0.4910827 ,  0.24465011,  1.1951616 ,  1.4084058 ,\n",
       "         -0.46858442, -0.2879332 ,  0.56235254,  1.1510066 ,\n",
       "          1.1065245 ,  0.28592515,  0.80371237, -0.22829136,\n",
       "          0.0658048 , -0.41090116,  0.75726354,  0.05314547,\n",
       "         -0.6305675 , -0.7487746 ,  0.4247196 , -0.37495795,\n",
       "         -0.8864728 ,  0.34925285,  0.3844561 , -0.9510911 ,\n",
       "         -0.01654417,  0.12231543,  0.9233795 , -0.37049013,\n",
       "         -0.53381294, -0.15617326,  0.60783464,  0.81752264,\n",
       "          1.0501515 ,  0.9783699 , -0.4018281 , -0.9124559 ,\n",
       "         -0.5064603 ,  0.4646531 , -0.7559893 ,  0.7113358 ,\n",
       "         -1.0726776 , -0.84220475, -0.6968625 , -0.8368003 ,\n",
       "          0.8969815 , -0.41297603,  0.08178505,  1.4403111 ,\n",
       "          0.8171039 , -2.0403295 , -0.12772112, -0.3837892 ,\n",
       "          0.17120348,  1.8990906 ,  0.8238069 , -0.52934587,\n",
       "         -0.1366274 ,  0.22634405, -0.19885325,  0.59177417,\n",
       "         -0.14457649, -0.65769976,  1.2163534 , -0.05455036,\n",
       "         -0.2266742 , -0.16222693,  0.34963575,  0.6856419 ,\n",
       "          1.1029466 , -0.6204541 ,  0.23735377,  1.0688593 ,\n",
       "          0.60961825,  0.4435402 , -0.74591374,  0.41285035,\n",
       "          0.44133773, -0.0099518 ,  1.107373  ,  0.47969705],\n",
       "        [ 0.32709712, -0.5096002 ,  0.98639077,  0.16190347,\n",
       "          0.22957307, -0.23234871,  0.2493223 , -0.09169734,\n",
       "          0.8762521 ,  0.5355865 , -0.80994904, -0.8251824 ,\n",
       "          1.1411512 , -0.22330631,  0.76804966, -0.69673043,\n",
       "         -0.2115453 , -0.06297757, -1.3376297 , -0.314061  ,\n",
       "          0.84586185,  0.7623979 , -0.08395149, -0.6342656 ,\n",
       "          0.40474996, -0.5659319 ,  0.67618287, -0.07830855,\n",
       "         -0.33254135,  0.45639563,  0.63314384, -0.8565834 ,\n",
       "         -0.19564126, -0.42313743,  0.05463101,  0.12918103,\n",
       "         -1.0021955 , -0.3776912 ,  0.5056434 ,  0.98177534,\n",
       "          0.9732777 , -0.32296243,  0.19575325, -1.516222  ,\n",
       "         -0.22924574,  0.17894381,  0.52026147,  0.00938198,\n",
       "          0.6168261 ,  1.0604659 ,  0.8000775 ,  0.7654508 ,\n",
       "         -0.60099894, -0.541978  , -0.84973735,  0.84694874,\n",
       "          0.5320744 , -0.2326191 ,  0.14406677, -0.50416696,\n",
       "         -0.13454318, -1.5015649 ,  0.649421  ,  1.067125  ,\n",
       "          0.33806643, -0.6666041 , -0.14204875, -0.4746912 ,\n",
       "         -1.1545343 , -0.40783086, -0.16335173, -0.7908905 ,\n",
       "         -0.05265319, -0.5580905 ,  0.14468965, -0.06525588,\n",
       "          0.30689678, -0.27320495,  0.26520583,  0.3671824 ,\n",
       "          0.1801929 ,  0.3176543 , -0.14502932, -0.7608518 ,\n",
       "         -0.6826785 ,  0.42832536, -0.41397586,  0.45245904,\n",
       "          0.53070134, -0.2943733 , -0.72287995, -1.0549407 ,\n",
       "          1.0068642 , -0.93546677,  0.49382132,  0.8068924 ,\n",
       "          0.17647274, -0.97322357, -0.44029722, -0.30456394,\n",
       "         -0.5958714 ,  1.0751218 ,  0.4273722 ,  0.01256547,\n",
       "         -0.06716152,  0.35303247, -0.25254986,  0.05918742,\n",
       "         -0.4659876 , -0.3377638 ,  0.18286608,  0.9628793 ,\n",
       "          0.06799759, -0.4932237 ,  0.8188643 , -0.01768197,\n",
       "          0.58658415, -0.47639233,  0.6029146 ,  0.7947985 ,\n",
       "          0.6092413 ,  0.5011448 , -0.28715307,  0.58954346,\n",
       "          0.39575794,  0.09883574,  1.0676906 ,  0.12178143],\n",
       "        [-0.31285435, -0.05440471,  0.69518733,  0.5183432 ,\n",
       "          0.24569644,  0.04751986, -0.14091085, -0.6679934 ,\n",
       "         -0.12679912, -0.26348573, -0.5865208 , -0.87654656,\n",
       "         -0.10106954,  0.20355053,  0.45181847,  0.58751535,\n",
       "          0.39576852,  0.33145112, -1.0450951 , -0.01037774,\n",
       "          0.8233157 , -0.7661912 ,  0.01698641, -0.7245561 ,\n",
       "          0.5533318 , -0.5443769 ,  0.10237502, -0.9434328 ,\n",
       "         -0.21611577,  0.6184608 ,  0.23311304, -0.5380586 ,\n",
       "         -0.30403754, -0.18707953, -0.25376302,  0.19952725,\n",
       "         -0.8089968 , -0.72702765, -0.31470996,  1.195351  ,\n",
       "          0.6706451 , -0.42095146,  0.14119811, -0.90317464,\n",
       "          0.20880692, -0.82560956, -0.3214728 ,  0.24690783,\n",
       "          0.55441636,  0.9065527 ,  0.17681164,  1.042296  ,\n",
       "         -0.16863486, -0.38602382, -0.72945017,  0.7618647 ,\n",
       "          0.26950765,  0.1882358 ,  0.29096967, -0.2926454 ,\n",
       "         -1.002902  , -0.87990725,  0.35426193,  1.0265841 ,\n",
       "         -0.12602194,  0.43155658, -0.396596  , -0.19642404,\n",
       "         -1.2961531 , -0.05839459, -0.22054625, -0.9750927 ,\n",
       "          0.5010179 , -0.44524458,  0.2968897 ,  0.20726815,\n",
       "          0.6784447 , -0.2932714 ,  0.52518636, -0.06521155,\n",
       "          0.92575186,  0.5650349 , -0.00409422, -1.0021875 ,\n",
       "         -0.7564427 , -0.5019419 ,  0.6378956 ,  0.14022154,\n",
       "         -0.29068786,  0.04673464, -0.61841184, -0.90882224,\n",
       "          0.22663702, -0.21939531,  0.21589684,  0.32968074,\n",
       "         -0.54510874, -0.64661473,  0.06245303, -0.40890506,\n",
       "          0.03014027,  1.2680179 , -0.6811354 , -0.56739265,\n",
       "          0.35879412,  0.25115067, -0.47409344,  0.14188005,\n",
       "         -0.24274723, -0.29284397,  0.04281003,  0.58798826,\n",
       "          0.7971623 ,  0.08572739,  1.017792  ,  0.13416898,\n",
       "          0.53283715, -0.27511016,  0.21492715,  0.3053788 ,\n",
       "          0.5614385 ,  0.45112965, -0.38440704, -0.35067484,\n",
       "          0.5381789 ,  0.7558844 ,  1.0780424 , -0.04906938],\n",
       "        [ 0.28264183,  0.00002936,  1.2219194 ,  0.6214253 ,\n",
       "          0.5262678 ,  0.56535345,  0.041063  , -0.42482036,\n",
       "          0.33767468,  0.18956731, -0.6874731 , -0.91918105,\n",
       "          0.10973322, -0.3021289 ,  0.7462501 , -0.7848025 ,\n",
       "          0.67951804, -0.5513356 , -1.155083  , -0.2626096 ,\n",
       "          1.2794095 ,  0.1442224 , -0.74693453, -1.1658912 ,\n",
       "          0.67235845, -0.7663928 ,  0.48467535,  0.07406034,\n",
       "          0.20473725,  0.24326074,  0.40074515, -0.9034333 ,\n",
       "         -0.28742123,  1.0153099 , -0.18882549, -0.48929426,\n",
       "         -0.36878607, -0.80909663,  0.54012966,  0.76902866,\n",
       "          0.13690385,  0.33129606,  0.02937792,  0.03849415,\n",
       "         -0.03408061, -0.36782345, -0.86670643,  0.2316432 ,\n",
       "          0.40792426,  1.049845  ,  0.9976656 ,  1.0478135 ,\n",
       "         -0.40173247,  0.21727213,  0.02141027,  0.78123045,\n",
       "          0.2529076 , -0.5673406 , -0.32651708, -0.5969734 ,\n",
       "         -0.18826272, -1.2804608 ,  0.6951744 ,  0.35388473,\n",
       "         -0.04823327, -0.7279381 , -0.12297562, -0.4246331 ,\n",
       "         -0.96031225,  0.05795801,  0.4497826 , -0.29394782,\n",
       "          0.13626446,  0.31318212, -0.10092568,  0.3297346 ,\n",
       "          0.54583704, -0.29713053,  0.06829455,  0.7677266 ,\n",
       "          0.6950954 ,  0.77311444, -0.10198547, -0.7414871 ,\n",
       "         -0.3882771 ,  0.03637049,  0.36390072,  0.2482072 ,\n",
       "         -0.2006969 ,  0.2591463 , -0.2778203 , -0.60252386,\n",
       "          0.53955036, -0.6506013 ,  0.17204832,  0.28621984,\n",
       "          0.2442616 , -0.69401914, -0.9724807 , -0.4622827 ,\n",
       "         -0.32162598,  1.242383  ,  0.7332203 , -1.5347431 ,\n",
       "          0.6096021 ,  0.07540937, -0.7317954 ,  0.78899646,\n",
       "         -0.15947343,  0.02625278, -0.24198543,  0.66808337,\n",
       "          0.4652356 , -0.02924202,  1.0722102 ,  0.8416253 ,\n",
       "          0.72087866, -0.44857275,  0.897968  ,  0.747456  ,\n",
       "          1.0193951 ,  0.22573777, -0.24055341,  0.02014086,\n",
       "          0.8959166 ,  0.26839772,  0.75477076,  0.26607984]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(128,4)\n",
    "x = tf.constant(np.random.randn(1,4,128))\n",
    "inputs = { 'query':x, 'key':x, 'value':x, 'mask':None }\n",
    "mha(inputs)\n",
    "\n",
    "# mha = MultiHeadAttention(128,4)({ 'query':x, 'key':x, 'value':x, 'mask':None })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32) # (2,5)\n",
    "    # (batch_size, 1, 1, key의 문장 길이)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[[[0. 0. 0. 1. 1.]]]], shape=(1, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_padding_mask(tf.constant([[1, 21, 777, 0, 0]])))  # (1,1,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 0. 0. 0.]]]], shape=(2, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_padding_mask(tf.constant([[1, 21, 777, 0, 0],\n",
    "                                       [0, 0, 777, 23, 25]])))  # (2,1,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3. 290.]\n",
      "[  1.41421356 115.75836903]\n",
      "[[-1.41421356  1.38218948]\n",
      " [ 0.          0.51832106]\n",
      " [-0.70710678  0.08638684]\n",
      " [ 0.70710678 -0.34554737]\n",
      " [ 1.41421356 -1.64135001]]\n",
      "[0. 0.]\n",
      "[1. 1.]\n",
      "[[ 0.17157288  5.76437896]\n",
      " [ 3.          4.03664211]\n",
      " [ 1.58578644  3.17277369]\n",
      " [ 4.41421356  2.30890526]\n",
      " [ 5.82842712 -0.28270002]]\n",
      "[3. 3.]\n",
      "[2. 2.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,450],\n",
    "              [3,350],\n",
    "              [2,300],\n",
    "              [4,250],\n",
    "              [5,100]])\n",
    "mean = np.mean(x, axis=0)\n",
    "print(mean)\n",
    "std  = np.std(x, axis=0)\n",
    "print(std)\n",
    "x_scaled = (x - mean) / std   # (5,2) - (5,2)\n",
    "print(x_scaled)\n",
    "print(np.mean(x_scaled, axis=0))\n",
    "print(np.std(x_scaled, axis=0))\n",
    "\n",
    "gamma = 2\n",
    "beta = 3\n",
    "x_scaled = x_scaled*gamma + beta\n",
    "print(x_scaled)\n",
    "print(np.mean(x_scaled, axis=0))\n",
    "print(np.std(x_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "y = relu(x)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # 인코더는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "            'mask': padding_mask # 패딩 마스크 사용\n",
    "        })\n",
    "    \n",
    "    print('attention.shape=', attention.shape)\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    print('attention.shape=', attention.shape)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)\n",
    "    \n",
    "    print('attention.shape=', attention.shape)\n",
    "\n",
    "    # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)  # (1,4,128)(128,256)=>(1,4,256)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)  # (1,4,256)(256,128)=>(1,4,128)\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 인코더는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 포지셔널 인코딩 + 드롭아웃\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # 인코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "      outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "          dropout=dropout, name=\"encoder_layer_{}\".format(i),\n",
    "      )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]], shape=(5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1.]], shape=(5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "seq_len=5\n",
    "print(tf.ones((seq_len, seq_len)))\n",
    "print(tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0))\n",
    "print(1-tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "#     return look_ahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 1. 1.]\n",
      "   [0. 0. 0. 1. 1.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_look_ahead_mask(tf.constant([[4, 3, 1, 0, 0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "\n",
    "    # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "            'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "            'mask': look_ahead_mask # 룩어헤드 마스크\n",
    "        })\n",
    "\n",
    "    # 잔차 연결과 층 정규화\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n",
    "            'mask': padding_mask # 패딩 마스크\n",
    "        })\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "\n",
    "    # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 포지셔널 인코딩 + 드롭아웃\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # 디코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "            dropout=dropout, name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size, num_layers, dff,\n",
    "                  d_model, num_heads, dropout,\n",
    "                  name=\"transformer\"):\n",
    "\n",
    "    # 인코더의 입력\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    # 디코더의 입력\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 인코더의 패딩 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 디코더의 룩어헤드 마스크(첫번째 서브층)\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask, output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 디코더의 패딩 마스크(두번째 서브층)\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n",
    "    enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "        d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n",
    "\n",
    "    # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n",
    "    dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "        d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 다음 단어 예측을 위한 출력층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention.__init__()\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "attention.shape= (None, None, 128)\n",
      "attention.shape= (None, None, 128)\n",
      "attention.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"encoder_layer_0/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"encoder_layer_0/attention/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "attention.shape= (None, None, 128)\n",
      "attention.shape= (None, None, 128)\n",
      "attention.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"encoder_layer_1/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"encoder_layer_1/attention/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "attention.shape= (None, None, 128)\n",
      "attention.shape= (None, None, 128)\n",
      "attention.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"encoder_layer_2/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"encoder_layer_2/attention/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "attention.shape= (None, None, 128)\n",
      "attention.shape= (None, None, 128)\n",
      "attention.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"encoder_layer_3/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"encoder_layer_3/attention/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"encoder/encoder_layer_0/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"encoder/encoder_layer_0/attention/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"encoder/encoder_layer_1/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"encoder/encoder_layer_1/attention/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"encoder/encoder_layer_2/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"encoder/encoder_layer_2/attention/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"encoder/encoder_layer_3/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"encoder/encoder_layer_3/attention/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention_1/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention_2/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder_layer_0/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder_layer_0/attention_1/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder_layer_0/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder_layer_0/attention_2/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention_1/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention_2/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder_layer_1/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder_layer_1/attention_1/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder_layer_1/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder_layer_1/attention_2/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention_1/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention_2/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder_layer_2/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder_layer_2/attention_1/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder_layer_2/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder_layer_2/attention_2/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention_1/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention_2/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder_layer_3/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder_layer_3/attention_1/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder_layer_3/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder_layer_3/attention_2/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder/decoder_layer_0/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder/decoder_layer_0/attention_1/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder/decoder_layer_0/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder/decoder_layer_0/attention_2/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder/decoder_layer_1/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder/decoder_layer_1/attention_1/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder/decoder_layer_1/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder/decoder_layer_1/attention_2/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder/decoder_layer_2/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder/decoder_layer_2/attention_1/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder/decoder_layer_2/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder/decoder_layer_2/attention_2/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder/decoder_layer_3/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder/decoder_layer_3/attention_1/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "(None, None, 128)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "split_heads()\n",
      "(None, None, 128)\n",
      "(None, None, 4, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "(None, 4, None, 32)\n",
      "matmul_qk.shape = (None, 4, None, None)\n",
      "depth= Tensor(\"decoder/decoder_layer_3/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder/decoder_layer_3/attention_2/truediv:0\", shape=(None, 4, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 4, None, None)\n",
      "output.shape = (None, 4, None, 32)\n",
      "scaled_attention.shape= (None, None, 4, 32)\n",
      "concat_attention.shape= (None, None, 128)\n",
      "outputs.shape= (None, None, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_transformer = transformer(\n",
    "    vocab_size = 9000,\n",
    "    num_layers = 4,\n",
    "    dff = 512,\n",
    "    d_model = 128,\n",
    "    num_heads = 4,\n",
    "    dropout = 0.3,\n",
    "    name=\"small_transformer\")\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    small_transformer, to_file='small_transformer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n",
    "# Text(0.5, 0, 'Train Step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow_datasets 처음 설치 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in c:\\programdata\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: absl-py in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.12.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (4.59.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (20.3.0)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.2.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (5.2.2)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (3.17.3)\n",
      "Requirement already satisfied: termcolor in c:\\users\\jikim\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: dill in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.3.4)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: promise in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\jikim\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow_datasets) (1.19.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\jikim\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow_datasets) (2.26.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jikim\\appdata\\roaming\\python\\python38\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jikim\\appdata\\roaming\\python\\python38\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.7)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-resources->tensorflow_datasets) (3.4.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow_datasets 업그레이드 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in c:\\programdata\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jikim\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow_datasets) (1.19.0)\n",
      "Requirement already satisfied: absl-py in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.12.0)\n",
      "Requirement already satisfied: dill in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.3.4)\n",
      "Requirement already satisfied: termcolor in c:\\users\\jikim\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (3.17.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (4.59.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (20.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\jikim\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow_datasets) (2.26.0)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.2.0)\n",
      "Requirement already satisfied: promise in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: importlib-resources in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (5.2.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jikim\\appdata\\roaming\\python\\python38\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jikim\\appdata\\roaming\\python\\python38\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.7)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-resources->tensorflow_datasets) (3.4.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user --upgrade tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4.0+nightly\n"
     ]
    }
   ],
   "source": [
    "print(tfds.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv\", filename=\"ChatBotData.csv\")\n",
    "train_data = pd.read_csv('ChatBotData.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "챗봇 샘플의 개수 : 11823\n"
     ]
    }
   ],
   "source": [
    "print('챗봇 샘플의 개수 :', len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q        0\n",
      "A        0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "for sentence in train_data['Q']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    # ex) 12시 땡! -> 12시 땡 !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    questions.append(sentence)\n",
    "answers = []\n",
    "for sentence in train_data['A']:\n",
    "    # 구두점에 대해서 띄어쓰기\n",
    "    # ex) 12시 땡! -> 12시 땡 !\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    answers.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡 !', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네']\n",
      "['하루가 또 가네요 .', '위로해 드립니다 .', '여행은 언제나 좋죠 .', '여행은 언제나 좋죠 .', '눈살이 찌푸려지죠 .']\n"
     ]
    }
   ],
   "source": [
    "print(questions[:5])\n",
    "print(answers[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서브워드텍스트인코더를 사용하여 질문, 답변 데이터로부터 단어 집합(Vocabulary) 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 대한 정수 부여.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시작 토큰 번호 : [8178]\n",
      "종료 토큰 번호 : [8179]\n",
      "단어 집합의 크기 : 8180\n"
     ]
    }
   ],
   "source": [
    "print('시작 토큰 번호 :',START_TOKEN)\n",
    "print('종료 토큰 번호 :',END_TOKEN)\n",
    "print('단어 집합의 크기 :',VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임의의 질문 샘플을 정수 인코딩 : [5766, 611, 3509, 141, 685, 3747, 849]\n"
     ]
    }
   ],
   "source": [
    "# 서브워드텍스트인코더 토크나이저의 .encode()를 사용하여 텍스트 시퀀스를 정수 시퀀스로 변환.\n",
    "print('임의의 질문 샘플을 정수 인코딩 : {}'.format(tokenizer.encode(questions[20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 [5766, 611, 3509, 141, 685, 3747, 849]\n",
      "기존 문장: 가스비 비싼데 감기 걸리겠어\n"
     ]
    }
   ],
   "source": [
    "# 서브워드텍스트인코더 토크나이저의 .encode()와 .decode() 테스트해보기\n",
    "# 임의의 입력 문장을 sample_string에 저장\n",
    "sample_string = questions[20]\n",
    "\n",
    "# encode() : 텍스트 시퀀스 --> 정수 시퀀스\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# decode() : 정수 시퀀스 --> 텍스트 시퀀스\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5766 ----> 가스\n",
      "611 ----> 비 \n",
      "3509 ----> 비싼\n",
      "141 ----> 데 \n",
      "685 ----> 감기 \n",
      "3747 ----> 걸리\n",
      "849 ----> 겠어\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "    print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이를 40으로 정의\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    tokenized_inputs.append(sentence1)\n",
    "    tokenized_outputs.append(sentence2)\n",
    "\n",
    "  # 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "questions, answers = tokenize_and_filter(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 데이터의 크기(shape) : (11823, 40)\n",
      "답변 데이터의 크기(shape) : (11823, 40)\n"
     ]
    }
   ],
   "source": [
    "print('질문 데이터의 크기(shape) :', questions.shape)\n",
    "print('답변 데이터의 크기(shape) :', answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8178 7915 4207 3060   41 8179    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(questions[0])\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[3844   74 7894    1 8179    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더의 실제값 시퀀스에서는 시작 토큰을 제거해야 한다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1] # 디코더의 입력. 마지막 패딩 토큰이 제거된다.\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다.\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "# 임의의 샘플에 대해서 [:, :-1]과 [:, 1:]이 어떤 의미를 가지는지 테스트해본다.\n",
    "print(answers[0]) # 기존 샘플\n",
    "print(answers[:1][:, :-1]) # 마지막 패딩 토큰 제거하면서 길이가 39가 된다.\n",
    "print(answers[:1][:, 1:]) # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다. 길이는 역시 39가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention.__init__()\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "attention.shape= (None, None, 256)\n",
      "attention.shape= (None, None, 256)\n",
      "attention.shape= (None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"encoder_layer_0/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"encoder_layer_0/attention/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "attention.shape= (None, None, 256)\n",
      "attention.shape= (None, None, 256)\n",
      "attention.shape= (None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"encoder_layer_1/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"encoder_layer_1/attention/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"encoder/encoder_layer_0/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"encoder/encoder_layer_0/attention/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"encoder/encoder_layer_1/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"encoder/encoder_layer_1/attention/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention_1/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention_2/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"decoder_layer_0/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder_layer_0/attention_1/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"decoder_layer_0/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder_layer_0/attention_2/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention_1/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "MultiHeadAttention.__init__()\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"attention_2/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"decoder_layer_1/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder_layer_1/attention_1/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"decoder_layer_1/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder_layer_1/attention_2/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"decoder/decoder_layer_0/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder/decoder_layer_0/attention_1/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"decoder/decoder_layer_0/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder/decoder_layer_0/attention_2/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"decoder/decoder_layer_1/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder/decoder_layer_1/attention_1/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "(None, None, 256)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, None, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"decoder/decoder_layer_1/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"decoder/decoder_layer_1/attention_2/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, None)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n"
     ]
    }
   ],
   "source": [
    "D_MODEL = 256\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dff=DFF,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  # 레이블의 크기는 (batch_size, MAX_LENGTH - 1)\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "(None, 40, 256)\n",
      "(None, 40, 256)\n",
      "(None, 40, 256)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"transformer/encoder/encoder_layer_0/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"transformer/encoder/encoder_layer_0/attention/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, 40)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, 40, 256)\n",
      "(None, 40, 256)\n",
      "(None, 40, 256)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"transformer/encoder/encoder_layer_1/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"transformer/encoder/encoder_layer_1/attention/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, 40)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, 39, 256)\n",
      "(None, 39, 256)\n",
      "(None, 39, 256)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"transformer/decoder/decoder_layer_0/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"transformer/decoder/decoder_layer_0/attention_1/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, 39)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, 39, 256)\n",
      "(None, 40, 256)\n",
      "(None, 40, 256)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"transformer/decoder/decoder_layer_0/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"transformer/decoder/decoder_layer_0/attention_2/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, 40)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, 39, 256)\n",
      "(None, 39, 256)\n",
      "(None, 39, 256)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"transformer/decoder/decoder_layer_1/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"transformer/decoder/decoder_layer_1/attention_1/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, 39)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, 39, 256)\n",
      "(None, 40, 256)\n",
      "(None, 40, 256)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"transformer/decoder/decoder_layer_1/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"transformer/decoder/decoder_layer_1/attention_2/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, 40)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, 40, 256)\n",
      "(None, 40, 256)\n",
      "(None, 40, 256)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"transformer/encoder/encoder_layer_0/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"transformer/encoder/encoder_layer_0/attention/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, 40)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, 40, 256)\n",
      "(None, 40, 256)\n",
      "(None, 40, 256)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"transformer/encoder/encoder_layer_1/attention/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"transformer/encoder/encoder_layer_1/attention/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, 40)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, 39, 256)\n",
      "(None, 39, 256)\n",
      "(None, 39, 256)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"transformer/decoder/decoder_layer_0/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"transformer/decoder/decoder_layer_0/attention_1/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, 39)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, 39, 256)\n",
      "(None, 40, 256)\n",
      "(None, 40, 256)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"transformer/decoder/decoder_layer_0/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"transformer/decoder/decoder_layer_0/attention_2/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, 40)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, 39, 256)\n",
      "(None, 39, 256)\n",
      "(None, 39, 256)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"transformer/decoder/decoder_layer_1/attention_1/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"transformer/decoder/decoder_layer_1/attention_1/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, 39)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n",
      "(None, 39, 256)\n",
      "(None, 40, 256)\n",
      "(None, 40, 256)\n",
      "split_heads()\n",
      "(None, 39, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "split_heads()\n",
      "(None, 40, 256)\n",
      "(None, None, 8, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "(None, 8, None, 32)\n",
      "matmul_qk.shape = (None, 8, None, None)\n",
      "depth= Tensor(\"transformer/decoder/decoder_layer_1/attention_2/Cast:0\", shape=(), dtype=float32)\n",
      "logits= Tensor(\"transformer/decoder/decoder_layer_1/attention_2/truediv:0\", shape=(None, 8, None, None), dtype=float32)\n",
      "attention_weights.shape = (None, 8, None, 40)\n",
      "output.shape = (None, 8, None, 32)\n",
      "scaled_attention.shape= (None, None, 8, 32)\n",
      "concat_attention.shape= (None, None, 256)\n",
      "outputs.shape= (None, None, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 28s 134ms/step - loss: 1.5272 - accuracy: 0.0138\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 24s 131ms/step - loss: 1.2470 - accuracy: 0.0494\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 24s 130ms/step - loss: 1.0193 - accuracy: 0.0503\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 24s 132ms/step - loss: 0.9398 - accuracy: 0.0536\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 14s 73ms/step - loss: 0.8821 - accuracy: 0.0568\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 12s 63ms/step - loss: 0.8180 - accuracy: 0.0609\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 11s 59ms/step - loss: 0.7534 - accuracy: 0.0666\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 11s 59ms/step - loss: 0.6770 - accuracy: 0.0748\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 11s 61ms/step - loss: 0.5956 - accuracy: 0.0835\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 15s 84ms/step - loss: 0.5165 - accuracy: 0.0930\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 27s 148ms/step - loss: 0.4276 - accuracy: 0.1043\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 29s 157ms/step - loss: 0.3424 - accuracy: 0.1158\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 11s 62ms/step - loss: 0.2652 - accuracy: 0.1273\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 12s 64ms/step - loss: 0.1961 - accuracy: 0.1388\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 11s 58ms/step - loss: 0.1410 - accuracy: 0.1481\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 15s 83ms/step - loss: 0.0995 - accuracy: 0.1552\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 19s 104ms/step - loss: 0.0693 - accuracy: 0.1615\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 21s 115ms/step - loss: 0.0546 - accuracy: 0.1643\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 14s 75ms/step - loss: 0.0420 - accuracy: 0.1647\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 10s 55ms/step - loss: 0.0384 - accuracy: 0.1662\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0360 - accuracy: 0.1657\n",
      "Epoch 22/50\n",
      "185/185 [==============================] - 27s 145ms/step - loss: 0.0329 - accuracy: 0.1654\n",
      "Epoch 23/50\n",
      "185/185 [==============================] - 28s 153ms/step - loss: 0.0313 - accuracy: 0.1680\n",
      "Epoch 24/50\n",
      "185/185 [==============================] - 26s 139ms/step - loss: 0.0266 - accuracy: 0.1682\n",
      "Epoch 25/50\n",
      "185/185 [==============================] - 25s 137ms/step - loss: 0.0243 - accuracy: 0.1688\n",
      "Epoch 26/50\n",
      "185/185 [==============================] - 30s 160ms/step - loss: 0.0223 - accuracy: 0.1689\n",
      "Epoch 27/50\n",
      "185/185 [==============================] - 29s 159ms/step - loss: 0.0189 - accuracy: 0.1706\n",
      "Epoch 28/50\n",
      "185/185 [==============================] - 31s 166ms/step - loss: 0.0177 - accuracy: 0.1709\n",
      "Epoch 29/50\n",
      "185/185 [==============================] - 18s 97ms/step - loss: 0.0154 - accuracy: 0.1718\n",
      "Epoch 30/50\n",
      "185/185 [==============================] - 11s 58ms/step - loss: 0.0151 - accuracy: 0.1713\n",
      "Epoch 31/50\n",
      "185/185 [==============================] - 16s 84ms/step - loss: 0.0138 - accuracy: 0.1715\n",
      "Epoch 32/50\n",
      "185/185 [==============================] - 23s 125ms/step - loss: 0.0136 - accuracy: 0.1720\n",
      "Epoch 33/50\n",
      "185/185 [==============================] - 23s 123ms/step - loss: 0.0108 - accuracy: 0.1717\n",
      "Epoch 34/50\n",
      "185/185 [==============================] - 23s 122ms/step - loss: 0.0106 - accuracy: 0.1721\n",
      "Epoch 35/50\n",
      "185/185 [==============================] - 19s 103ms/step - loss: 0.0103 - accuracy: 0.1731\n",
      "Epoch 36/50\n",
      "185/185 [==============================] - 11s 58ms/step - loss: 0.0095 - accuracy: 0.1728\n",
      "Epoch 37/50\n",
      "185/185 [==============================] - 17s 90ms/step - loss: 0.0094 - accuracy: 0.1738\n",
      "Epoch 38/50\n",
      "185/185 [==============================] - 20s 107ms/step - loss: 0.0091 - accuracy: 0.1724\n",
      "Epoch 39/50\n",
      "185/185 [==============================] - 15s 83ms/step - loss: 0.0075 - accuracy: 0.1731\n",
      "Epoch 40/50\n",
      "185/185 [==============================] - 17s 91ms/step - loss: 0.0079 - accuracy: 0.1731\n",
      "Epoch 41/50\n",
      "185/185 [==============================] - 16s 85ms/step - loss: 0.0075 - accuracy: 0.1734\n",
      "Epoch 42/50\n",
      "185/185 [==============================] - 17s 94ms/step - loss: 0.0075 - accuracy: 0.1743\n",
      "Epoch 43/50\n",
      "185/185 [==============================] - 17s 94ms/step - loss: 0.0062 - accuracy: 0.1738\n",
      "Epoch 44/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.0064 - accuracy: 0.1737\n",
      "Epoch 45/50\n",
      "185/185 [==============================] - 20s 108ms/step - loss: 0.0063 - accuracy: 0.1739\n",
      "Epoch 46/50\n",
      "185/185 [==============================] - 13s 73ms/step - loss: 0.0055 - accuracy: 0.1740\n",
      "Epoch 47/50\n",
      "185/185 [==============================] - 13s 71ms/step - loss: 0.0052 - accuracy: 0.1740\n",
      "Epoch 48/50\n",
      "185/185 [==============================] - 18s 97ms/step - loss: 0.0057 - accuracy: 0.1743\n",
      "Epoch 49/50\n",
      "185/185 [==============================] - 18s 96ms/step - loss: 0.0054 - accuracy: 0.1748\n",
      "Epoch 50/50\n",
      "185/185 [==============================] - 13s 72ms/step - loss: 0.0053 - accuracy: 0.1739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2dfb44fcbe0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 예측 시작\n",
    "  for i in range(MAX_LENGTH):\n",
    "    predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "    # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
    "    # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0)\n",
    "def predict(sentence):\n",
    "  prediction = evaluate(sentence)\n",
    "\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724    3.119958     0.8104701   -3.7867832   -0.53479564\n",
      "     -0.68400717  -2.684907    -4.306619  ]\n",
      "   [ -2.3828275    6.9414687    0.72609663  -5.475966    -3.1419044\n",
      "     -4.861004    -1.5760975   -1.7812909 ]\n",
      "   [ -9.4575205    9.2924795   -0.14664039  -7.7748985    0.3141518\n",
      "     -4.7069764   -3.4037216   -7.8661847 ]\n",
      "   [-10.166894    14.483014     3.299315   -11.655867    -6.542652\n",
      "     -5.2036433   -7.0711117   -9.239806  ]\n",
      "   [ -7.9254494   12.0516       3.657774   -14.190444    -6.1069603\n",
      "     -6.00821     -5.337775    -6.7280397 ]\n",
      "   [ -9.514821    12.602872     2.7181993  -11.5606165   -3.807155\n",
      "     -4.5184774   -7.4839892   -9.5418    ]\n",
      "   [ -6.294987     8.895258     0.31260866  -7.041161    -2.2837403\n",
      "     -4.8525386   -4.4346924   -5.9981575 ]\n",
      "   [ -1.627529     0.2572786    0.33721286  -1.0157803   -0.02855057\n",
      "      0.4145152   -2.0903144   -3.1125855 ]]\n",
      "\n",
      "  [[ -5.830585     2.9278567    6.4656816    2.9012501    0.7074189\n",
      "      1.8678039   -0.98469496  -5.710069  ]\n",
      "   [ -0.888771    -1.5304633   -0.92677224   7.0835996   -2.1796093\n",
      "      3.6029007   -0.78117675  -0.31565633]\n",
      "   [ -5.1757755   -0.00269019   5.1771684    2.4006374    1.5459608\n",
      "      4.5166783   -1.3455517   -7.2344646 ]\n",
      "   [ -0.32639125   1.338475     6.20836     -2.123073     1.0238662\n",
      "      0.95025975  -0.588602    -0.29866517]\n",
      "   [ -3.7669215    0.96661407   6.3918867   -2.0059547   -1.667028\n",
      "      2.4628794   -0.9746783   -5.5741014 ]\n",
      "   [ -5.9424763    3.3478012    5.0460277    5.0597334    1.5781783\n",
      "      5.766434    -3.736297    -6.4208727 ]\n",
      "   [ -5.028427     2.1733994    8.720076    -5.247528     2.0949328\n",
      "      0.3581393   -1.2882515   -6.517891  ]\n",
      "   [ -4.8532395    3.40758      5.827717     0.8119974    1.7706672\n",
      "      1.3585875   -2.134433    -4.6770263 ]]\n",
      "\n",
      "  [[  0.21261294   2.1825705    2.6290977   -0.7738304   -0.92338926\n",
      "      1.3111119   -1.7676708   -1.2661753 ]\n",
      "   [ -7.103699    -0.08641089   0.6366692   -1.1594403    0.52412534\n",
      "     -1.2009193   -3.281907    -9.214137  ]\n",
      "   [ -7.0138736    4.6630197   -1.6835009    0.64012516  -0.3236557\n",
      "      6.017604    -2.2194111   -8.46463   ]\n",
      "   [ -5.658084     3.870231     1.6583294   -3.8960452   -0.90947014\n",
      "     -1.150247     0.02895866  -8.4907255 ]\n",
      "   [ -2.1916983    2.1456993   -0.696395     2.0237832    0.4654629\n",
      "      2.97788     -1.8672262   -3.7927125 ]\n",
      "   [ -4.237206     0.8983024   -1.8900397   -1.5124333   -1.4959668\n",
      "     -3.0116844   -4.190044    -5.1174064 ]\n",
      "   [  0.44389454   0.6015544    1.0924258   -0.6734957   -0.14239292\n",
      "     -0.31658053  -3.1918075   -2.5049903 ]\n",
      "   [ -0.5491213    1.3170428    2.416191    -0.72504455   0.17214571\n",
      "      0.6214916   -0.7466009   -2.0425894 ]]\n",
      "\n",
      "  [[ -0.62310183   1.8474774   -0.70651436  -1.8744931    1.9892807\n",
      "      1.1960789    0.93903846  -0.96611404]\n",
      "   [ -2.1139884   -0.40789947   1.2968414   -2.4691556   -0.6430844\n",
      "      2.9704242   -0.49229723  -3.2126625 ]\n",
      "   [ -4.681421     1.071365     7.057754     1.6925517    2.925461\n",
      "      1.2117237   -2.5487785   -7.727114  ]\n",
      "   [ -5.2494354    2.6016214    4.7093654   -2.9185028   11.68196\n",
      "      5.0173087    0.2649115   -6.465863  ]\n",
      "   [ -4.187672     2.2745495    2.1844132   -2.950158     0.4811062\n",
      "      2.5944564   -1.2815626   -5.458976  ]\n",
      "   [ -5.1851773    3.6045432    5.879984    -1.5479906    9.332985\n",
      "      0.7532006   -6.1630864  -10.190875  ]\n",
      "   [ -1.4530067    3.602885     1.8737535   -1.0183158    6.639346\n",
      "     -0.45745188  -3.0397322   -5.1571107 ]\n",
      "   [ -1.3701262    1.7800907    0.4440075   -2.7584684    0.16184288\n",
      "      1.0718383    0.74469614  -2.4832382 ]]\n",
      "\n",
      "  [[ -3.365542     1.7112432   -0.18042703  -0.56367624  -1.0182812\n",
      "      0.91838205   0.3576792   -1.8675333 ]\n",
      "   [ -4.8425922   -4.2636166   -2.6981843    0.22403693   0.68739414\n",
      "     -3.1009803    2.71456     -1.489905  ]\n",
      "   [ -5.1876755    5.671311    -1.2768452   -2.9591484   -0.39675456\n",
      "     -3.3887212    1.0478692   -2.3474293 ]\n",
      "   [ -2.170307     3.4666238    1.0260042   -0.8940675    2.2694418\n",
      "     -2.5140228    1.5545614   -1.9796547 ]\n",
      "   [ -7.992303     0.4727184   -0.13311552  -3.517133    -2.5849926\n",
      "     -4.228556    -1.0881504   -5.6788545 ]\n",
      "   [ -3.6134825    0.19133087   0.298861    -4.0497766   -2.2160487\n",
      "     -3.5523865    0.53147453  -4.259141  ]\n",
      "   [ -2.6600354    4.010139     4.72034     -3.9068446   -1.4782856\n",
      "     -0.05746605  -5.8499537   -1.9302822 ]\n",
      "   [  2.192343    -2.0631888   -0.458438     0.3939276    0.19301154\n",
      "      3.2884934   -1.1612332    1.5978622 ]]\n",
      "\n",
      "  [[ -2.1686854   -0.68126124   1.8873476   -1.0016267    1.665862\n",
      "     -0.39142966  -2.5463562   -2.0510204 ]\n",
      "   [ -3.4723454   -2.521675     3.3836114   -0.29911357   3.7395344\n",
      "      2.2255218   -0.5534243   -3.5096362 ]\n",
      "   [ -1.241272    -1.0362755    0.8233798    0.13532306   0.49612203\n",
      "      2.9168866   -0.06637239   0.07041088]\n",
      "   [  0.16238996  -2.266713    -0.43468216  -1.9490578   -0.7804693\n",
      "     -2.2501192    0.14404878   0.669509  ]\n",
      "   [ -4.5097795    0.04322206   2.9916728    4.4943275    1.4705126\n",
      "     -0.8136589   -2.140882    -5.1061444 ]\n",
      "   [ -6.6359534    3.4645762    4.6767483   -2.6971583    5.242417\n",
      "      3.411301     1.1889111   -5.1261134 ]\n",
      "   [ -2.4793563    1.550663     3.5571485   -3.2087276    2.422179\n",
      "      0.22151406  -3.0109324   -3.599103  ]\n",
      "   [ -1.2139645   -1.8291416    2.1708345   -1.4788015    1.7007154\n",
      "     -1.5888573   -2.220674    -2.1623607 ]]\n",
      "\n",
      "  [[ -0.3049526    1.6942294   -0.7551851   -0.20310196  -1.9716048\n",
      "      0.50340194   0.2921721   -1.8446437 ]\n",
      "   [ -3.5450757    9.210564     3.1409702   -1.4201491   -8.377774\n",
      "     -2.9744062   -6.1630034   -6.277001  ]\n",
      "   [-10.783727     9.743        1.6049056   -7.786639    -9.65972\n",
      "     -3.0615754   -6.16684     -9.820524  ]\n",
      "   [-10.1322155    4.096334     1.2889549   -8.946105    -7.003514\n",
      "     -2.119969    -5.464064    -9.032952  ]\n",
      "   [ -6.2441616    5.4699683    0.8303648   -2.5861697   -6.0254755\n",
      "     -5.8178663   -3.9932399   -6.5110664 ]\n",
      "   [ -7.973193     7.724517     4.6691766   -4.006742    -8.032589\n",
      "     -1.6155707   -8.793758    -7.380326  ]\n",
      "   [ -5.8270884    2.8298204    4.595993    -2.1421301   -1.5096639\n",
      "     -4.502131    -9.880545    -6.981122  ]\n",
      "   [ -1.5424074   -0.03381129   0.8708008    0.84780496  -2.2531416\n",
      "     -0.00659109   0.6038286   -2.874726  ]]\n",
      "\n",
      "  [[ -2.8292158    1.3493172   -1.1630216   -2.1086867   -0.7928409\n",
      "     -1.6628647    0.936982     0.17500922]\n",
      "   [ -4.8446875    2.2061067    5.2808175   -7.194037     1.36124\n",
      "      0.13124909  -5.7810917   -5.9226546 ]\n",
      "   [ -5.9484262    1.2807072    4.3257394   -5.866291    -1.1549981\n",
      "      0.3883991   -5.670845    -4.975041  ]\n",
      "   [ -4.245631     5.4597836    3.617752    -7.2080035   -3.3983228\n",
      "     -4.2374063   -2.325605    -2.7597272 ]\n",
      "   [ -3.7233293    5.6208553    0.9434773   -6.0832963   -2.91809\n",
      "     -0.02632309  -1.9356793   -0.92453605]\n",
      "   [ -2.065824     5.411931     2.41236     -4.3067594    2.3385603\n",
      "     -1.8093777   -4.6885967   -1.7742286 ]\n",
      "   [ -3.8498166    5.2004757   -1.0518612   -7.7332063   -0.27363002\n",
      "      0.8425215   -2.1548126   -2.9958065 ]\n",
      "   [ -1.0712852    3.3348348    0.1982265   -2.701739     0.3845683\n",
      "     -1.0062916    2.1746593    0.6345082 ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.108165     2.77802      7.04615      2.6454995    5.4191833\n",
      "      5.2317686   -5.6294155   -9.130048  ]\n",
      "   [-10.735054     1.7547116    0.76175237  -0.42852682   4.004292\n",
      "      2.5349874   -3.68677    -13.101013  ]\n",
      "   [ -7.5400214    0.57103527  -0.5523173    0.4326064    1.1318936\n",
      "      0.61845857  -3.0292842   -9.038257  ]\n",
      "   [-13.195052     3.5323567    3.9254384    3.0674067    4.645694\n",
      "      4.6829467   -5.6105638  -17.256235  ]\n",
      "   [-10.567291     1.5221579    0.52586746   1.5666054    3.392625\n",
      "      3.7933207   -3.6424325  -13.40815   ]\n",
      "   [-11.348783     1.1702118    0.33376536   1.9208177    4.171567\n",
      "      3.2230349   -4.625526   -14.001499  ]\n",
      "   [-13.822072     4.6995225    6.882736     4.5583987    9.456506\n",
      "      8.984521    -7.40026    -19.019571  ]\n",
      "   [ -4.276782     2.2549555    6.41255      2.478643     5.0064325\n",
      "      5.4905148   -5.49556     -7.172459  ]]\n",
      "\n",
      "  [[ -2.6697712    0.18644784  -3.2964184   -9.0699005   -5.8869786\n",
      "     -8.670129    -1.7457955   -2.4067173 ]\n",
      "   [ -4.892798     5.1958284    0.86651105  -1.7851338   -0.5991809\n",
      "      1.0769178   -6.025225    -5.985986  ]\n",
      "   [ -9.587597     3.4813366   -1.9858788   -0.24309416   0.67243534\n",
      "      4.1456385   -5.7291985  -11.872933  ]\n",
      "   [-10.449494     4.90856     -2.7020588   -1.3338774    0.16304302\n",
      "      4.3895545   -7.198788   -13.19233   ]\n",
      "   [ -6.8310695    3.5543683   -0.808639     0.5658518    1.5122908\n",
      "      3.5409427   -5.93916     -8.560726  ]\n",
      "   [-10.439719     3.3256927   -2.8369732   -1.1681513    0.17411843\n",
      "      4.7924056   -7.1944017  -12.024714  ]\n",
      "   [ -4.1172786    3.8512855   -1.755627    -3.863435    -1.9191699\n",
      "     -2.5844417   -4.672864    -4.7063437 ]\n",
      "   [ -1.7964184   -0.00925941  -3.5144498   -9.731884    -6.624717\n",
      "    -10.108276    -0.76045424  -0.7916409 ]]\n",
      "\n",
      "  [[ -4.7987876    2.3515558   -2.4956558    3.970095     6.9596395\n",
      "      6.1769433    2.5710814   -5.3403516 ]\n",
      "   [ -6.3391857    1.296634    -1.2071677   -2.4126644   -0.8230457\n",
      "      1.5592674   -4.067092    -6.657909  ]\n",
      "   [ -9.1259985    8.161333     4.880271    -1.283182     4.1173687\n",
      "      4.9365716   -5.1432853  -11.959505  ]\n",
      "   [-10.686145     8.565781     5.08118      0.27082056   5.3818784\n",
      "      6.742503    -2.9075274  -13.400082  ]\n",
      "   [ -9.507849    10.487074     6.0747166   -1.3516918    4.202924\n",
      "      5.809277    -3.300832   -12.412424  ]\n",
      "   [ -9.160462    10.127481     7.924963     1.2205707    6.1613603\n",
      "      7.179847    -3.3297603  -12.610715  ]\n",
      "   [ -3.074413    11.459646     8.69444      8.782502    13.618378\n",
      "      9.073093     3.539803    -6.4021416 ]\n",
      "   [ -2.5498397    2.3677871   -2.3312814    5.376789     7.2629266\n",
      "      5.222065     4.6978946   -2.8941624 ]]\n",
      "\n",
      "  [[  5.7272882    2.8488104   -2.2674716    2.4710479    1.0492487\n",
      "     -1.1509322    1.7533069    4.0113854 ]\n",
      "   [ -6.0593033   -0.8574972   -0.14114377   2.036406     0.46856305\n",
      "      3.5360727    1.1927096   -6.1919413 ]\n",
      "   [ -3.0789824   -0.10847238   1.995033     3.6496744    3.8057036\n",
      "      6.084562     4.08223     -3.2766128 ]\n",
      "   [ -4.919659     1.601183     1.003934     3.2019112    0.6709702\n",
      "      4.7461705    1.0749629   -5.1771154 ]\n",
      "   [ -6.994366     0.38075814  -0.8496923    2.2005079    2.1127446\n",
      "      4.8004184    1.2863806   -7.113053  ]\n",
      "   [ -1.9637377    2.5683565    0.11696774   0.11398675  -0.02081025\n",
      "      1.7587206   -0.9755858   -2.719708  ]\n",
      "   [ -4.988607     7.0706754    1.6426951    7.2739515    7.1590147\n",
      "      9.629911     6.3608046   -7.190774  ]\n",
      "   [  7.1238737    5.775128    -1.2283382    3.9956207    3.3504624\n",
      "      0.41485792   3.2472675    4.7716117 ]]\n",
      "\n",
      "  [[ 15.227098    -7.6481614   -6.1704655   -2.9068828   -4.9539256\n",
      "     -6.9440393   -5.5164175   14.977688  ]\n",
      "   [ -9.1538725    2.5506845    3.8603075   -0.97211933  -0.7266477\n",
      "      1.0761876    4.012439    -8.621349  ]\n",
      "   [ -7.3554325    3.0492685    4.882066     1.9635559    1.5533912\n",
      "      2.5195072    5.0332184   -6.660119  ]\n",
      "   [ -7.502227     4.479796     3.8863442    0.8529868    2.5325358\n",
      "      2.6109834    3.7704332   -7.242996  ]\n",
      "   [-14.381295     7.1894965    6.8530087    0.8468516    1.6221398\n",
      "      3.4500875    6.026878   -14.084528  ]\n",
      "   [-12.931715     6.7301073    6.57782      0.15552036   0.9927032\n",
      "      0.9647971    3.3784504  -12.61685   ]\n",
      "   [  0.44247183  -0.69109786  -1.4392438   -1.959099    -3.2892675\n",
      "     -3.1996653   -2.5787928   -0.35673866]\n",
      "   [ 16.702045    -7.9659853   -7.5883594   -3.50606     -4.3180175\n",
      "     -7.998839    -8.3601055   15.943729  ]]\n",
      "\n",
      "  [[ 16.92651     -1.8878899    4.612371     8.297861     8.785893\n",
      "     10.494433    13.696043    18.756512  ]\n",
      "   [ -4.8187237    0.3554866   -0.10856649  -0.7664691   -0.8179527\n",
      "     -2.149825    -1.7334187   -4.167079  ]\n",
      "   [ -9.965244     1.1075305   -3.9921682   -5.4180512   -3.1439044\n",
      "     -9.256229    -8.250524   -10.748956  ]\n",
      "   [ -7.730639     0.702649    -3.5076466   -2.8937106   -1.1201787\n",
      "     -2.574378    -2.7975821   -7.5726776 ]\n",
      "   [-10.1452055   -1.0527366   -4.0010457   -2.8202963   -3.0494757\n",
      "     -5.4484124   -5.228744   -10.110497  ]\n",
      "   [-11.985994     3.1561482   -2.7560956   -3.3319776   -1.583422\n",
      "     -4.690591    -5.913287   -12.567582  ]\n",
      "   [ -5.3008895   -3.0260174   -2.1208181   -1.0755001   -2.3386164\n",
      "     -0.51985437  -1.1730318   -4.255245  ]\n",
      "   [ 17.86222     -1.6225508    5.1695466    7.7438717    8.076046\n",
      "     10.810771    13.726412    19.605438  ]]\n",
      "\n",
      "  [[ -5.9784994   -2.1169765    4.562182     0.5942515    3.0359569\n",
      "      1.3266702   -0.49234566  -3.304644  ]\n",
      "   [-12.7146845    2.0409193    2.1862695   -0.09457824   1.4637623\n",
      "      2.2060208   -4.997251   -13.3957815 ]\n",
      "   [-15.493677     3.6968644    4.4985094    1.5548421    1.8590727\n",
      "      4.2570806   -6.917543   -16.565073  ]\n",
      "   [ -9.458374    -0.38232473   3.5684469    0.3583388    1.3042445\n",
      "      1.8564001   -4.3985405   -9.246963  ]\n",
      "   [-12.0117655    2.1870446    3.3004608    0.608759     0.99599266\n",
      "      1.7472012   -5.940463   -13.297273  ]\n",
      "   [ -9.835355    -0.9845663   -0.7273823   -2.8138676   -0.3688764\n",
      "      0.66789573  -4.370575    -9.257524  ]\n",
      "   [ -3.3896937   -3.072007     1.3647641    1.7215154    1.1578857\n",
      "      2.4925263    3.2728026   -0.55832624]\n",
      "   [ -2.0235658   -2.5166588    4.0697465    1.5893646    3.5398774\n",
      "      1.8933309    2.816157     1.3247585 ]]\n",
      "\n",
      "  [[ 13.876768     5.289463     2.8875844    5.729185    -0.31207085\n",
      "      2.216713     0.970955    11.02701   ]\n",
      "   [-11.855931     7.044625     2.757176    -2.0767522    9.620936\n",
      "      3.4307175   -0.20549646 -11.727619  ]\n",
      "   [-17.164427     9.00703      2.9232194   -2.114338    10.674683\n",
      "      4.7908983    0.6977465  -17.203806  ]\n",
      "   [ -6.0810537    6.5570908    3.815859     1.4116415    7.24501\n",
      "      5.5307817    2.8692966   -6.8714347 ]\n",
      "   [ -9.309462     5.957898     3.5396616   -0.5519851    6.9079504\n",
      "      5.2796082    1.4829706   -9.929194  ]\n",
      "   [ -9.980593     6.659394     3.6414435   -1.2484376    6.108645\n",
      "      4.148278     2.2938857  -10.687042  ]\n",
      "   [-11.461593     8.152552     3.527002    -0.32889172   5.8540664\n",
      "      4.3797064    2.374406   -12.23368   ]\n",
      "   [ 16.641361     4.062904     2.288843     5.4982553   -3.302351\n",
      "      0.8199426    0.47605655  13.646871  ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 1)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106 ]]\n",
      "\n",
      "  [[-0.1123142 ]]\n",
      "\n",
      "  [[-0.10918517]]\n",
      "\n",
      "  [[-0.22114104]]\n",
      "\n",
      "  [[ 0.00076126]]\n",
      "\n",
      "  [[-0.00999826]]\n",
      "\n",
      "  [[-0.20863976]]\n",
      "\n",
      "  [[-0.15726982]]]], shape=(1, 8, 1, 1), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 1)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.64339346 -3.427214   -1.9332778  -1.831178   -2.9725688\n",
      "    -3.3511758  -0.33726037  0.65105456]]\n",
      "\n",
      "  [[ 1.1482323  -9.236041   -8.218754   -6.717816   -9.638861\n",
      "    -8.856656   -2.9334142   1.2082323 ]]\n",
      "\n",
      "  [[-1.0731127  -1.8999621  -2.2802618  -2.3014586  -1.5626087\n",
      "    -1.5433154  -1.408609   -1.0031282 ]]\n",
      "\n",
      "  [[-0.4604545  -1.5868427  -0.91173816 -0.8565646  -1.1848444\n",
      "    -2.0918827  -0.78210574 -0.41651735]]\n",
      "\n",
      "  [[-1.5332501   4.991253    6.4686904   4.0847487   5.368916\n",
      "     4.911813    2.5922883  -1.523277  ]]\n",
      "\n",
      "  [[ 1.6350883  -6.317388   -5.912019   -3.864096   -6.084104\n",
      "    -5.8046737  -1.3961531   1.6473184 ]]\n",
      "\n",
      "  [[ 0.12968476 -2.7599597  -3.3458984  -0.6776076  -1.3498564\n",
      "    -2.4725385  -0.26429352  0.143075  ]]\n",
      "\n",
      "  [[-0.48440975 -0.30788445 -1.4893405  -0.5263086   0.2881688\n",
      "    -0.46770203 -0.27271533 -0.46877885]]]], shape=(1, 8, 1, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 8)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 1)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.82826173]]\n",
      "\n",
      "  [[ 0.05666294]]\n",
      "\n",
      "  [[-3.099432  ]]\n",
      "\n",
      "  [[-2.95289   ]]\n",
      "\n",
      "  [[-0.5372538 ]]\n",
      "\n",
      "  [[-1.4123605 ]]\n",
      "\n",
      "  [[-2.4360013 ]]\n",
      "\n",
      "  [[-1.929182  ]]]], shape=(1, 8, 1, 1), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 1)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.39734983  3.2900825   5.064875    2.9756458   4.0151095\n",
      "     4.007048   -0.2133849  -0.49868223]]\n",
      "\n",
      "  [[-0.64090055  3.2327366   3.5917265   2.0562942   3.046029\n",
      "     4.408309   -0.39604825 -0.6353867 ]]\n",
      "\n",
      "  [[-0.6058031   1.7137845   2.864631    0.59623325  2.7598896\n",
      "     1.3035189  -0.8051331  -0.64268285]]\n",
      "\n",
      "  [[-1.7000011   4.244851    6.1901093   3.7555447   5.435635\n",
      "     6.6990557  -0.34916046 -1.74636   ]]\n",
      "\n",
      "  [[-1.2788014   2.4755447   3.7033203   1.3851215   3.247792\n",
      "     3.6466665   0.5535189  -1.0989617 ]]\n",
      "\n",
      "  [[-0.6709667   1.5037154   1.6425345  -0.3352149   0.96505636\n",
      "     0.0492342  -0.8917425  -0.713599  ]]\n",
      "\n",
      "  [[-0.53040653 -2.1101773  -1.396114   -2.352641   -1.9407948\n",
      "    -0.8967342  -2.052857   -0.505681  ]]\n",
      "\n",
      "  [[-1.6134965   5.6560483   8.457334    4.7208633   6.4695926\n",
      "     6.38709    -0.7719655  -1.6863627 ]]]], shape=(1, 8, 1, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 8)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724    3.119958     0.8104701   -3.7867832   -0.53479564\n",
      "     -0.68400717  -2.684907    -4.306619  ]\n",
      "   [ -2.3828275    6.9414687    0.72609663  -5.475966    -3.1419044\n",
      "     -4.861004    -1.5760975   -1.7812909 ]\n",
      "   [ -9.4575205    9.2924795   -0.14664039  -7.7748985    0.3141518\n",
      "     -4.7069764   -3.4037216   -7.8661847 ]\n",
      "   [-10.166894    14.483014     3.299315   -11.655867    -6.542652\n",
      "     -5.2036433   -7.0711117   -9.239806  ]\n",
      "   [ -7.9254494   12.0516       3.657774   -14.190444    -6.1069603\n",
      "     -6.00821     -5.337775    -6.7280397 ]\n",
      "   [ -9.514821    12.602872     2.7181993  -11.5606165   -3.807155\n",
      "     -4.5184774   -7.4839892   -9.5418    ]\n",
      "   [ -6.294987     8.895258     0.31260866  -7.041161    -2.2837403\n",
      "     -4.8525386   -4.4346924   -5.9981575 ]\n",
      "   [ -1.627529     0.2572786    0.33721286  -1.0157803   -0.02855057\n",
      "      0.4145152   -2.0903144   -3.1125855 ]]\n",
      "\n",
      "  [[ -5.830585     2.9278567    6.4656816    2.9012501    0.7074189\n",
      "      1.8678039   -0.98469496  -5.710069  ]\n",
      "   [ -0.888771    -1.5304633   -0.92677224   7.0835996   -2.1796093\n",
      "      3.6029007   -0.78117675  -0.31565633]\n",
      "   [ -5.1757755   -0.00269019   5.1771684    2.4006374    1.5459608\n",
      "      4.5166783   -1.3455517   -7.2344646 ]\n",
      "   [ -0.32639125   1.338475     6.20836     -2.123073     1.0238662\n",
      "      0.95025975  -0.588602    -0.29866517]\n",
      "   [ -3.7669215    0.96661407   6.3918867   -2.0059547   -1.667028\n",
      "      2.4628794   -0.9746783   -5.5741014 ]\n",
      "   [ -5.9424763    3.3478012    5.0460277    5.0597334    1.5781783\n",
      "      5.766434    -3.736297    -6.4208727 ]\n",
      "   [ -5.028427     2.1733994    8.720076    -5.247528     2.0949328\n",
      "      0.3581393   -1.2882515   -6.517891  ]\n",
      "   [ -4.8532395    3.40758      5.827717     0.8119974    1.7706672\n",
      "      1.3585875   -2.134433    -4.6770263 ]]\n",
      "\n",
      "  [[  0.21261294   2.1825705    2.6290977   -0.7738304   -0.92338926\n",
      "      1.3111119   -1.7676708   -1.2661753 ]\n",
      "   [ -7.103699    -0.08641089   0.6366692   -1.1594403    0.52412534\n",
      "     -1.2009193   -3.281907    -9.214137  ]\n",
      "   [ -7.0138736    4.6630197   -1.6835009    0.64012516  -0.3236557\n",
      "      6.017604    -2.2194111   -8.46463   ]\n",
      "   [ -5.658084     3.870231     1.6583294   -3.8960452   -0.90947014\n",
      "     -1.150247     0.02895866  -8.4907255 ]\n",
      "   [ -2.1916983    2.1456993   -0.696395     2.0237832    0.4654629\n",
      "      2.97788     -1.8672262   -3.7927125 ]\n",
      "   [ -4.237206     0.8983024   -1.8900397   -1.5124333   -1.4959668\n",
      "     -3.0116844   -4.190044    -5.1174064 ]\n",
      "   [  0.44389454   0.6015544    1.0924258   -0.6734957   -0.14239292\n",
      "     -0.31658053  -3.1918075   -2.5049903 ]\n",
      "   [ -0.5491213    1.3170428    2.416191    -0.72504455   0.17214571\n",
      "      0.6214916   -0.7466009   -2.0425894 ]]\n",
      "\n",
      "  [[ -0.62310183   1.8474774   -0.70651436  -1.8744931    1.9892807\n",
      "      1.1960789    0.93903846  -0.96611404]\n",
      "   [ -2.1139884   -0.40789947   1.2968414   -2.4691556   -0.6430844\n",
      "      2.9704242   -0.49229723  -3.2126625 ]\n",
      "   [ -4.681421     1.071365     7.057754     1.6925517    2.925461\n",
      "      1.2117237   -2.5487785   -7.727114  ]\n",
      "   [ -5.2494354    2.6016214    4.7093654   -2.9185028   11.68196\n",
      "      5.0173087    0.2649115   -6.465863  ]\n",
      "   [ -4.187672     2.2745495    2.1844132   -2.950158     0.4811062\n",
      "      2.5944564   -1.2815626   -5.458976  ]\n",
      "   [ -5.1851773    3.6045432    5.879984    -1.5479906    9.332985\n",
      "      0.7532006   -6.1630864  -10.190875  ]\n",
      "   [ -1.4530067    3.602885     1.8737535   -1.0183158    6.639346\n",
      "     -0.45745188  -3.0397322   -5.1571107 ]\n",
      "   [ -1.3701262    1.7800907    0.4440075   -2.7584684    0.16184288\n",
      "      1.0718383    0.74469614  -2.4832382 ]]\n",
      "\n",
      "  [[ -3.365542     1.7112432   -0.18042703  -0.56367624  -1.0182812\n",
      "      0.91838205   0.3576792   -1.8675333 ]\n",
      "   [ -4.8425922   -4.2636166   -2.6981843    0.22403693   0.68739414\n",
      "     -3.1009803    2.71456     -1.489905  ]\n",
      "   [ -5.1876755    5.671311    -1.2768452   -2.9591484   -0.39675456\n",
      "     -3.3887212    1.0478692   -2.3474293 ]\n",
      "   [ -2.170307     3.4666238    1.0260042   -0.8940675    2.2694418\n",
      "     -2.5140228    1.5545614   -1.9796547 ]\n",
      "   [ -7.992303     0.4727184   -0.13311552  -3.517133    -2.5849926\n",
      "     -4.228556    -1.0881504   -5.6788545 ]\n",
      "   [ -3.6134825    0.19133087   0.298861    -4.0497766   -2.2160487\n",
      "     -3.5523865    0.53147453  -4.259141  ]\n",
      "   [ -2.6600354    4.010139     4.72034     -3.9068446   -1.4782856\n",
      "     -0.05746605  -5.8499537   -1.9302822 ]\n",
      "   [  2.192343    -2.0631888   -0.458438     0.3939276    0.19301154\n",
      "      3.2884934   -1.1612332    1.5978622 ]]\n",
      "\n",
      "  [[ -2.1686854   -0.68126124   1.8873476   -1.0016267    1.665862\n",
      "     -0.39142966  -2.5463562   -2.0510204 ]\n",
      "   [ -3.4723454   -2.521675     3.3836114   -0.29911357   3.7395344\n",
      "      2.2255218   -0.5534243   -3.5096362 ]\n",
      "   [ -1.241272    -1.0362755    0.8233798    0.13532306   0.49612203\n",
      "      2.9168866   -0.06637239   0.07041088]\n",
      "   [  0.16238996  -2.266713    -0.43468216  -1.9490578   -0.7804693\n",
      "     -2.2501192    0.14404878   0.669509  ]\n",
      "   [ -4.5097795    0.04322206   2.9916728    4.4943275    1.4705126\n",
      "     -0.8136589   -2.140882    -5.1061444 ]\n",
      "   [ -6.6359534    3.4645762    4.6767483   -2.6971583    5.242417\n",
      "      3.411301     1.1889111   -5.1261134 ]\n",
      "   [ -2.4793563    1.550663     3.5571485   -3.2087276    2.422179\n",
      "      0.22151406  -3.0109324   -3.599103  ]\n",
      "   [ -1.2139645   -1.8291416    2.1708345   -1.4788015    1.7007154\n",
      "     -1.5888573   -2.220674    -2.1623607 ]]\n",
      "\n",
      "  [[ -0.3049526    1.6942294   -0.7551851   -0.20310196  -1.9716048\n",
      "      0.50340194   0.2921721   -1.8446437 ]\n",
      "   [ -3.5450757    9.210564     3.1409702   -1.4201491   -8.377774\n",
      "     -2.9744062   -6.1630034   -6.277001  ]\n",
      "   [-10.783727     9.743        1.6049056   -7.786639    -9.65972\n",
      "     -3.0615754   -6.16684     -9.820524  ]\n",
      "   [-10.1322155    4.096334     1.2889549   -8.946105    -7.003514\n",
      "     -2.119969    -5.464064    -9.032952  ]\n",
      "   [ -6.2441616    5.4699683    0.8303648   -2.5861697   -6.0254755\n",
      "     -5.8178663   -3.9932399   -6.5110664 ]\n",
      "   [ -7.973193     7.724517     4.6691766   -4.006742    -8.032589\n",
      "     -1.6155707   -8.793758    -7.380326  ]\n",
      "   [ -5.8270884    2.8298204    4.595993    -2.1421301   -1.5096639\n",
      "     -4.502131    -9.880545    -6.981122  ]\n",
      "   [ -1.5424074   -0.03381129   0.8708008    0.84780496  -2.2531416\n",
      "     -0.00659109   0.6038286   -2.874726  ]]\n",
      "\n",
      "  [[ -2.8292158    1.3493172   -1.1630216   -2.1086867   -0.7928409\n",
      "     -1.6628647    0.936982     0.17500922]\n",
      "   [ -4.8446875    2.2061067    5.2808175   -7.194037     1.36124\n",
      "      0.13124909  -5.7810917   -5.9226546 ]\n",
      "   [ -5.9484262    1.2807072    4.3257394   -5.866291    -1.1549981\n",
      "      0.3883991   -5.670845    -4.975041  ]\n",
      "   [ -4.245631     5.4597836    3.617752    -7.2080035   -3.3983228\n",
      "     -4.2374063   -2.325605    -2.7597272 ]\n",
      "   [ -3.7233293    5.6208553    0.9434773   -6.0832963   -2.91809\n",
      "     -0.02632309  -1.9356793   -0.92453605]\n",
      "   [ -2.065824     5.411931     2.41236     -4.3067594    2.3385603\n",
      "     -1.8093777   -4.6885967   -1.7742286 ]\n",
      "   [ -3.8498166    5.2004757   -1.0518612   -7.7332063   -0.27363002\n",
      "      0.8425215   -2.1548126   -2.9958065 ]\n",
      "   [ -1.0712852    3.3348348    0.1982265   -2.701739     0.3845683\n",
      "     -1.0062916    2.1746593    0.6345082 ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.108165     2.77802      7.04615      2.6454995    5.4191833\n",
      "      5.2317686   -5.6294155   -9.130048  ]\n",
      "   [-10.735054     1.7547116    0.76175237  -0.42852682   4.004292\n",
      "      2.5349874   -3.68677    -13.101013  ]\n",
      "   [ -7.5400214    0.57103527  -0.5523173    0.4326064    1.1318936\n",
      "      0.61845857  -3.0292842   -9.038257  ]\n",
      "   [-13.195052     3.5323567    3.9254384    3.0674067    4.645694\n",
      "      4.6829467   -5.6105638  -17.256235  ]\n",
      "   [-10.567291     1.5221579    0.52586746   1.5666054    3.392625\n",
      "      3.7933207   -3.6424325  -13.40815   ]\n",
      "   [-11.348783     1.1702118    0.33376536   1.9208177    4.171567\n",
      "      3.2230349   -4.625526   -14.001499  ]\n",
      "   [-13.822072     4.6995225    6.882736     4.5583987    9.456506\n",
      "      8.984521    -7.40026    -19.019571  ]\n",
      "   [ -4.276782     2.2549555    6.41255      2.478643     5.0064325\n",
      "      5.4905148   -5.49556     -7.172459  ]]\n",
      "\n",
      "  [[ -2.6697712    0.18644784  -3.2964184   -9.0699005   -5.8869786\n",
      "     -8.670129    -1.7457955   -2.4067173 ]\n",
      "   [ -4.892798     5.1958284    0.86651105  -1.7851338   -0.5991809\n",
      "      1.0769178   -6.025225    -5.985986  ]\n",
      "   [ -9.587597     3.4813366   -1.9858788   -0.24309416   0.67243534\n",
      "      4.1456385   -5.7291985  -11.872933  ]\n",
      "   [-10.449494     4.90856     -2.7020588   -1.3338774    0.16304302\n",
      "      4.3895545   -7.198788   -13.19233   ]\n",
      "   [ -6.8310695    3.5543683   -0.808639     0.5658518    1.5122908\n",
      "      3.5409427   -5.93916     -8.560726  ]\n",
      "   [-10.439719     3.3256927   -2.8369732   -1.1681513    0.17411843\n",
      "      4.7924056   -7.1944017  -12.024714  ]\n",
      "   [ -4.1172786    3.8512855   -1.755627    -3.863435    -1.9191699\n",
      "     -2.5844417   -4.672864    -4.7063437 ]\n",
      "   [ -1.7964184   -0.00925941  -3.5144498   -9.731884    -6.624717\n",
      "    -10.108276    -0.76045424  -0.7916409 ]]\n",
      "\n",
      "  [[ -4.7987876    2.3515558   -2.4956558    3.970095     6.9596395\n",
      "      6.1769433    2.5710814   -5.3403516 ]\n",
      "   [ -6.3391857    1.296634    -1.2071677   -2.4126644   -0.8230457\n",
      "      1.5592674   -4.067092    -6.657909  ]\n",
      "   [ -9.1259985    8.161333     4.880271    -1.283182     4.1173687\n",
      "      4.9365716   -5.1432853  -11.959505  ]\n",
      "   [-10.686145     8.565781     5.08118      0.27082056   5.3818784\n",
      "      6.742503    -2.9075274  -13.400082  ]\n",
      "   [ -9.507849    10.487074     6.0747166   -1.3516918    4.202924\n",
      "      5.809277    -3.300832   -12.412424  ]\n",
      "   [ -9.160462    10.127481     7.924963     1.2205707    6.1613603\n",
      "      7.179847    -3.3297603  -12.610715  ]\n",
      "   [ -3.074413    11.459646     8.69444      8.782502    13.618378\n",
      "      9.073093     3.539803    -6.4021416 ]\n",
      "   [ -2.5498397    2.3677871   -2.3312814    5.376789     7.2629266\n",
      "      5.222065     4.6978946   -2.8941624 ]]\n",
      "\n",
      "  [[  5.7272882    2.8488104   -2.2674716    2.4710479    1.0492487\n",
      "     -1.1509322    1.7533069    4.0113854 ]\n",
      "   [ -6.0593033   -0.8574972   -0.14114377   2.036406     0.46856305\n",
      "      3.5360727    1.1927096   -6.1919413 ]\n",
      "   [ -3.0789824   -0.10847238   1.995033     3.6496744    3.8057036\n",
      "      6.084562     4.08223     -3.2766128 ]\n",
      "   [ -4.919659     1.601183     1.003934     3.2019112    0.6709702\n",
      "      4.7461705    1.0749629   -5.1771154 ]\n",
      "   [ -6.994366     0.38075814  -0.8496923    2.2005079    2.1127446\n",
      "      4.8004184    1.2863806   -7.113053  ]\n",
      "   [ -1.9637377    2.5683565    0.11696774   0.11398675  -0.02081025\n",
      "      1.7587206   -0.9755858   -2.719708  ]\n",
      "   [ -4.988607     7.0706754    1.6426951    7.2739515    7.1590147\n",
      "      9.629911     6.3608046   -7.190774  ]\n",
      "   [  7.1238737    5.775128    -1.2283382    3.9956207    3.3504624\n",
      "      0.41485792   3.2472675    4.7716117 ]]\n",
      "\n",
      "  [[ 15.227098    -7.6481614   -6.1704655   -2.9068828   -4.9539256\n",
      "     -6.9440393   -5.5164175   14.977688  ]\n",
      "   [ -9.1538725    2.5506845    3.8603075   -0.97211933  -0.7266477\n",
      "      1.0761876    4.012439    -8.621349  ]\n",
      "   [ -7.3554325    3.0492685    4.882066     1.9635559    1.5533912\n",
      "      2.5195072    5.0332184   -6.660119  ]\n",
      "   [ -7.502227     4.479796     3.8863442    0.8529868    2.5325358\n",
      "      2.6109834    3.7704332   -7.242996  ]\n",
      "   [-14.381295     7.1894965    6.8530087    0.8468516    1.6221398\n",
      "      3.4500875    6.026878   -14.084528  ]\n",
      "   [-12.931715     6.7301073    6.57782      0.15552036   0.9927032\n",
      "      0.9647971    3.3784504  -12.61685   ]\n",
      "   [  0.44247183  -0.69109786  -1.4392438   -1.959099    -3.2892675\n",
      "     -3.1996653   -2.5787928   -0.35673866]\n",
      "   [ 16.702045    -7.9659853   -7.5883594   -3.50606     -4.3180175\n",
      "     -7.998839    -8.3601055   15.943729  ]]\n",
      "\n",
      "  [[ 16.92651     -1.8878899    4.612371     8.297861     8.785893\n",
      "     10.494433    13.696043    18.756512  ]\n",
      "   [ -4.8187237    0.3554866   -0.10856649  -0.7664691   -0.8179527\n",
      "     -2.149825    -1.7334187   -4.167079  ]\n",
      "   [ -9.965244     1.1075305   -3.9921682   -5.4180512   -3.1439044\n",
      "     -9.256229    -8.250524   -10.748956  ]\n",
      "   [ -7.730639     0.702649    -3.5076466   -2.8937106   -1.1201787\n",
      "     -2.574378    -2.7975821   -7.5726776 ]\n",
      "   [-10.1452055   -1.0527366   -4.0010457   -2.8202963   -3.0494757\n",
      "     -5.4484124   -5.228744   -10.110497  ]\n",
      "   [-11.985994     3.1561482   -2.7560956   -3.3319776   -1.583422\n",
      "     -4.690591    -5.913287   -12.567582  ]\n",
      "   [ -5.3008895   -3.0260174   -2.1208181   -1.0755001   -2.3386164\n",
      "     -0.51985437  -1.1730318   -4.255245  ]\n",
      "   [ 17.86222     -1.6225508    5.1695466    7.7438717    8.076046\n",
      "     10.810771    13.726412    19.605438  ]]\n",
      "\n",
      "  [[ -5.9784994   -2.1169765    4.562182     0.5942515    3.0359569\n",
      "      1.3266702   -0.49234566  -3.304644  ]\n",
      "   [-12.7146845    2.0409193    2.1862695   -0.09457824   1.4637623\n",
      "      2.2060208   -4.997251   -13.3957815 ]\n",
      "   [-15.493677     3.6968644    4.4985094    1.5548421    1.8590727\n",
      "      4.2570806   -6.917543   -16.565073  ]\n",
      "   [ -9.458374    -0.38232473   3.5684469    0.3583388    1.3042445\n",
      "      1.8564001   -4.3985405   -9.246963  ]\n",
      "   [-12.0117655    2.1870446    3.3004608    0.608759     0.99599266\n",
      "      1.7472012   -5.940463   -13.297273  ]\n",
      "   [ -9.835355    -0.9845663   -0.7273823   -2.8138676   -0.3688764\n",
      "      0.66789573  -4.370575    -9.257524  ]\n",
      "   [ -3.3896937   -3.072007     1.3647641    1.7215154    1.1578857\n",
      "      2.4925263    3.2728026   -0.55832624]\n",
      "   [ -2.0235658   -2.5166588    4.0697465    1.5893646    3.5398774\n",
      "      1.8933309    2.816157     1.3247585 ]]\n",
      "\n",
      "  [[ 13.876768     5.289463     2.8875844    5.729185    -0.31207085\n",
      "      2.216713     0.970955    11.02701   ]\n",
      "   [-11.855931     7.044625     2.757176    -2.0767522    9.620936\n",
      "      3.4307175   -0.20549646 -11.727619  ]\n",
      "   [-17.164427     9.00703      2.9232194   -2.114338    10.674683\n",
      "      4.7908983    0.6977465  -17.203806  ]\n",
      "   [ -6.0810537    6.5570908    3.815859     1.4116415    7.24501\n",
      "      5.5307817    2.8692966   -6.8714347 ]\n",
      "   [ -9.309462     5.957898     3.5396616   -0.5519851    6.9079504\n",
      "      5.2796082    1.4829706   -9.929194  ]\n",
      "   [ -9.980593     6.659394     3.6414435   -1.2484376    6.108645\n",
      "      4.148278     2.2938857  -10.687042  ]\n",
      "   [-11.461593     8.152552     3.527002    -0.32889172   5.8540664\n",
      "      4.3797064    2.374406   -12.23368   ]\n",
      "   [ 16.641361     4.062904     2.288843     5.4982553   -3.302351\n",
      "      0.8199426    0.47605655  13.646871  ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 2)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.09299298]\n",
      "   [-0.18269673 -4.40855   ]]\n",
      "\n",
      "  [[-0.1123142   0.56263393]\n",
      "   [-0.29892173 -1.5473536 ]]\n",
      "\n",
      "  [[-0.10918517  0.11000926]\n",
      "   [ 0.07259987 -4.5046606 ]]\n",
      "\n",
      "  [[-0.22114104 -0.48759893]\n",
      "   [ 0.67009145 -7.431193  ]]\n",
      "\n",
      "  [[ 0.00076126  0.07451362]\n",
      "   [-0.10380323 -2.2713957 ]]\n",
      "\n",
      "  [[-0.00999826  0.21593446]\n",
      "   [ 0.9415558  -2.968748  ]]\n",
      "\n",
      "  [[-0.20863976 -0.27273425]\n",
      "   [ 0.59413576 -7.0776234 ]]\n",
      "\n",
      "  [[-0.15726982 -0.16104104]\n",
      "   [-0.19427125 -5.400611  ]]]], shape=(1, 8, 2, 2), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 2)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.64339346 -3.427214   -1.9332778  -1.831178   -2.9725688\n",
      "    -3.3511758  -0.33726037  0.65105456]\n",
      "   [-0.9194904   0.8968038   0.45222297  1.0594327   1.1557782\n",
      "    -0.8956241  -0.17587543 -0.8396095 ]]\n",
      "\n",
      "  [[ 1.1482323  -9.236041   -8.218754   -6.717816   -9.638861\n",
      "    -8.856656   -2.9334142   1.2082323 ]\n",
      "   [-1.8857075   3.473417    2.5907042   3.5434728   3.9081209\n",
      "     5.844051    1.0954899  -1.8498287 ]]\n",
      "\n",
      "  [[-1.0731127  -1.8999621  -2.2802618  -2.3014586  -1.5626087\n",
      "    -1.5433154  -1.408609   -1.0031282 ]\n",
      "   [-0.15004301 -3.7747118  -4.124579   -3.467151   -3.0495815\n",
      "    -2.7865295  -1.1569786  -0.1172325 ]]\n",
      "\n",
      "  [[-0.4604545  -1.5868427  -0.91173816 -0.8565646  -1.1848444\n",
      "    -2.0918827  -0.78210574 -0.41651735]\n",
      "   [-1.3551308   0.1230931   0.7364044  -0.6643719   0.8351439\n",
      "     0.06500757  0.78761655 -1.2645116 ]]\n",
      "\n",
      "  [[-1.5332501   4.991253    6.4686904   4.0847487   5.368916\n",
      "     4.911813    2.5922883  -1.523277  ]\n",
      "   [-1.8101153   6.2314014   5.68846     4.518671    6.62946\n",
      "     6.433606    3.0988286  -1.8167404 ]]\n",
      "\n",
      "  [[ 1.6350883  -6.317388   -5.912019   -3.864096   -6.084104\n",
      "    -5.8046737  -1.3961531   1.6473184 ]\n",
      "   [ 0.24405284  1.462353    0.9224829   1.581335    1.9375173\n",
      "     2.096835    2.0256872   0.28618026]]\n",
      "\n",
      "  [[ 0.12968476 -2.7599597  -3.3458984  -0.6776076  -1.3498564\n",
      "    -2.4725385  -0.26429352  0.143075  ]\n",
      "   [ 0.16608208 -3.8964067  -3.9270244  -2.1397204  -2.7393942\n",
      "    -2.4727707  -0.7227451   0.23026752]]\n",
      "\n",
      "  [[-0.48440975 -0.30788445 -1.4893405  -0.5263086   0.2881688\n",
      "    -0.46770203 -0.27271533 -0.46877885]\n",
      "   [-0.3127084  -1.174712   -0.8735866  -0.32615614 -0.25206855\n",
      "    -0.55387217  0.23482844 -0.2958877 ]]]], shape=(1, 8, 2, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 8)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 2)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.8282402  -0.42404926]\n",
      "   [-1.2308896   2.5182536 ]]\n",
      "\n",
      "  [[ 0.05669579 -0.14652377]\n",
      "   [-1.3575498   1.1293744 ]]\n",
      "\n",
      "  [[-3.0994341   3.915559  ]\n",
      "   [-6.1072607  -1.5910499 ]]\n",
      "\n",
      "  [[-2.9529114   2.299974  ]\n",
      "   [-7.031095   -0.24240322]]\n",
      "\n",
      "  [[-0.5371959   0.03175565]\n",
      "   [-1.4198313  -6.4444065 ]]\n",
      "\n",
      "  [[-1.412344    1.717014  ]\n",
      "   [-1.9951228  -3.2546551 ]]\n",
      "\n",
      "  [[-2.4360008   0.67301464]\n",
      "   [-0.27072212 -7.063433  ]]\n",
      "\n",
      "  [[-1.929139    0.8935091 ]\n",
      "   [-1.0276425  -4.9508924 ]]]], shape=(1, 8, 2, 2), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 2)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.3973298    3.2900124    5.064679     2.9755645    4.015015\n",
      "      4.0069337   -0.21337411  -0.49866062]\n",
      "   [ -0.33577263   4.7211685    5.425065     3.9286716    3.8147593\n",
      "      3.5898767    0.9110762   -0.36344096]]\n",
      "\n",
      "  [[ -0.6408605    3.2326546    3.591694     2.0563188    3.0459878\n",
      "      4.4082403   -0.3960128   -0.63534915]\n",
      "   [ -1.1932893    5.5270476    5.752577     4.0630784    4.8154626\n",
      "      5.298501     1.4706326   -1.2358047 ]]\n",
      "\n",
      "  [[ -0.6057757    1.7133442    2.8641703    0.5958629    2.7594314\n",
      "      1.3030896   -0.8051967   -0.6426489 ]\n",
      "   [ -0.22311665  -8.5990305  -10.39877     -8.644732   -10.273176\n",
      "     -8.879024    -2.734983    -0.12058785]]\n",
      "\n",
      "  [[ -1.699932     4.244729     6.19011      3.755602     5.4355664\n",
      "      6.699038    -0.3490838   -1.7462924 ]\n",
      "   [  1.4002923    0.54548395  -1.1169968    0.37443233  -1.1300082\n",
      "     -1.9262465    2.2690759    1.3882834 ]]\n",
      "\n",
      "  [[ -1.2787998    2.4755638    3.703323     1.3851306    3.2478092\n",
      "      3.6466854    0.553544    -1.0989556 ]\n",
      "   [ -0.68938744   0.7849126    0.80624384  -0.06641847   0.84722805\n",
      "      1.4088807    0.84816337  -0.6531029 ]]\n",
      "\n",
      "  [[ -0.6709214    1.5035548    1.64232     -0.33530334   0.9649067\n",
      "      0.04905168  -0.89175946  -0.71355253]\n",
      "   [  0.24583115   0.6569769   -0.621118     1.2669846    0.91026103\n",
      "      1.4697332    1.5297475    0.26090285]]\n",
      "\n",
      "  [[ -0.5303791   -2.110146    -1.3960739   -2.3525817   -1.9407862\n",
      "     -0.8966946   -2.0528047   -0.50565517]\n",
      "   [ -0.09481095  -1.1549708    0.31408975  -0.03525389  -0.4600581\n",
      "     -1.6961929    0.7154125   -0.0880874 ]]\n",
      "\n",
      "  [[ -1.6133933    5.655559     8.456806     4.720569     6.469103\n",
      "      6.3866005   -0.77200097  -1.6862562 ]\n",
      "   [  2.6422656   -3.0630863   -6.9324884   -2.84115     -4.448012\n",
      "     -5.5646486    1.8054792    2.638062  ]]]], shape=(1, 8, 2, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 8)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724    3.119958     0.8104701   -3.7867832   -0.53479564\n",
      "     -0.68400717  -2.684907    -4.306619  ]\n",
      "   [ -2.3828275    6.9414687    0.72609663  -5.475966    -3.1419044\n",
      "     -4.861004    -1.5760975   -1.7812909 ]\n",
      "   [ -9.4575205    9.2924795   -0.14664039  -7.7748985    0.3141518\n",
      "     -4.7069764   -3.4037216   -7.8661847 ]\n",
      "   [-10.166894    14.483014     3.299315   -11.655867    -6.542652\n",
      "     -5.2036433   -7.0711117   -9.239806  ]\n",
      "   [ -7.9254494   12.0516       3.657774   -14.190444    -6.1069603\n",
      "     -6.00821     -5.337775    -6.7280397 ]\n",
      "   [ -9.514821    12.602872     2.7181993  -11.5606165   -3.807155\n",
      "     -4.5184774   -7.4839892   -9.5418    ]\n",
      "   [ -6.294987     8.895258     0.31260866  -7.041161    -2.2837403\n",
      "     -4.8525386   -4.4346924   -5.9981575 ]\n",
      "   [ -1.627529     0.2572786    0.33721286  -1.0157803   -0.02855057\n",
      "      0.4145152   -2.0903144   -3.1125855 ]]\n",
      "\n",
      "  [[ -5.830585     2.9278567    6.4656816    2.9012501    0.7074189\n",
      "      1.8678039   -0.98469496  -5.710069  ]\n",
      "   [ -0.888771    -1.5304633   -0.92677224   7.0835996   -2.1796093\n",
      "      3.6029007   -0.78117675  -0.31565633]\n",
      "   [ -5.1757755   -0.00269019   5.1771684    2.4006374    1.5459608\n",
      "      4.5166783   -1.3455517   -7.2344646 ]\n",
      "   [ -0.32639125   1.338475     6.20836     -2.123073     1.0238662\n",
      "      0.95025975  -0.588602    -0.29866517]\n",
      "   [ -3.7669215    0.96661407   6.3918867   -2.0059547   -1.667028\n",
      "      2.4628794   -0.9746783   -5.5741014 ]\n",
      "   [ -5.9424763    3.3478012    5.0460277    5.0597334    1.5781783\n",
      "      5.766434    -3.736297    -6.4208727 ]\n",
      "   [ -5.028427     2.1733994    8.720076    -5.247528     2.0949328\n",
      "      0.3581393   -1.2882515   -6.517891  ]\n",
      "   [ -4.8532395    3.40758      5.827717     0.8119974    1.7706672\n",
      "      1.3585875   -2.134433    -4.6770263 ]]\n",
      "\n",
      "  [[  0.21261294   2.1825705    2.6290977   -0.7738304   -0.92338926\n",
      "      1.3111119   -1.7676708   -1.2661753 ]\n",
      "   [ -7.103699    -0.08641089   0.6366692   -1.1594403    0.52412534\n",
      "     -1.2009193   -3.281907    -9.214137  ]\n",
      "   [ -7.0138736    4.6630197   -1.6835009    0.64012516  -0.3236557\n",
      "      6.017604    -2.2194111   -8.46463   ]\n",
      "   [ -5.658084     3.870231     1.6583294   -3.8960452   -0.90947014\n",
      "     -1.150247     0.02895866  -8.4907255 ]\n",
      "   [ -2.1916983    2.1456993   -0.696395     2.0237832    0.4654629\n",
      "      2.97788     -1.8672262   -3.7927125 ]\n",
      "   [ -4.237206     0.8983024   -1.8900397   -1.5124333   -1.4959668\n",
      "     -3.0116844   -4.190044    -5.1174064 ]\n",
      "   [  0.44389454   0.6015544    1.0924258   -0.6734957   -0.14239292\n",
      "     -0.31658053  -3.1918075   -2.5049903 ]\n",
      "   [ -0.5491213    1.3170428    2.416191    -0.72504455   0.17214571\n",
      "      0.6214916   -0.7466009   -2.0425894 ]]\n",
      "\n",
      "  [[ -0.62310183   1.8474774   -0.70651436  -1.8744931    1.9892807\n",
      "      1.1960789    0.93903846  -0.96611404]\n",
      "   [ -2.1139884   -0.40789947   1.2968414   -2.4691556   -0.6430844\n",
      "      2.9704242   -0.49229723  -3.2126625 ]\n",
      "   [ -4.681421     1.071365     7.057754     1.6925517    2.925461\n",
      "      1.2117237   -2.5487785   -7.727114  ]\n",
      "   [ -5.2494354    2.6016214    4.7093654   -2.9185028   11.68196\n",
      "      5.0173087    0.2649115   -6.465863  ]\n",
      "   [ -4.187672     2.2745495    2.1844132   -2.950158     0.4811062\n",
      "      2.5944564   -1.2815626   -5.458976  ]\n",
      "   [ -5.1851773    3.6045432    5.879984    -1.5479906    9.332985\n",
      "      0.7532006   -6.1630864  -10.190875  ]\n",
      "   [ -1.4530067    3.602885     1.8737535   -1.0183158    6.639346\n",
      "     -0.45745188  -3.0397322   -5.1571107 ]\n",
      "   [ -1.3701262    1.7800907    0.4440075   -2.7584684    0.16184288\n",
      "      1.0718383    0.74469614  -2.4832382 ]]\n",
      "\n",
      "  [[ -3.365542     1.7112432   -0.18042703  -0.56367624  -1.0182812\n",
      "      0.91838205   0.3576792   -1.8675333 ]\n",
      "   [ -4.8425922   -4.2636166   -2.6981843    0.22403693   0.68739414\n",
      "     -3.1009803    2.71456     -1.489905  ]\n",
      "   [ -5.1876755    5.671311    -1.2768452   -2.9591484   -0.39675456\n",
      "     -3.3887212    1.0478692   -2.3474293 ]\n",
      "   [ -2.170307     3.4666238    1.0260042   -0.8940675    2.2694418\n",
      "     -2.5140228    1.5545614   -1.9796547 ]\n",
      "   [ -7.992303     0.4727184   -0.13311552  -3.517133    -2.5849926\n",
      "     -4.228556    -1.0881504   -5.6788545 ]\n",
      "   [ -3.6134825    0.19133087   0.298861    -4.0497766   -2.2160487\n",
      "     -3.5523865    0.53147453  -4.259141  ]\n",
      "   [ -2.6600354    4.010139     4.72034     -3.9068446   -1.4782856\n",
      "     -0.05746605  -5.8499537   -1.9302822 ]\n",
      "   [  2.192343    -2.0631888   -0.458438     0.3939276    0.19301154\n",
      "      3.2884934   -1.1612332    1.5978622 ]]\n",
      "\n",
      "  [[ -2.1686854   -0.68126124   1.8873476   -1.0016267    1.665862\n",
      "     -0.39142966  -2.5463562   -2.0510204 ]\n",
      "   [ -3.4723454   -2.521675     3.3836114   -0.29911357   3.7395344\n",
      "      2.2255218   -0.5534243   -3.5096362 ]\n",
      "   [ -1.241272    -1.0362755    0.8233798    0.13532306   0.49612203\n",
      "      2.9168866   -0.06637239   0.07041088]\n",
      "   [  0.16238996  -2.266713    -0.43468216  -1.9490578   -0.7804693\n",
      "     -2.2501192    0.14404878   0.669509  ]\n",
      "   [ -4.5097795    0.04322206   2.9916728    4.4943275    1.4705126\n",
      "     -0.8136589   -2.140882    -5.1061444 ]\n",
      "   [ -6.6359534    3.4645762    4.6767483   -2.6971583    5.242417\n",
      "      3.411301     1.1889111   -5.1261134 ]\n",
      "   [ -2.4793563    1.550663     3.5571485   -3.2087276    2.422179\n",
      "      0.22151406  -3.0109324   -3.599103  ]\n",
      "   [ -1.2139645   -1.8291416    2.1708345   -1.4788015    1.7007154\n",
      "     -1.5888573   -2.220674    -2.1623607 ]]\n",
      "\n",
      "  [[ -0.3049526    1.6942294   -0.7551851   -0.20310196  -1.9716048\n",
      "      0.50340194   0.2921721   -1.8446437 ]\n",
      "   [ -3.5450757    9.210564     3.1409702   -1.4201491   -8.377774\n",
      "     -2.9744062   -6.1630034   -6.277001  ]\n",
      "   [-10.783727     9.743        1.6049056   -7.786639    -9.65972\n",
      "     -3.0615754   -6.16684     -9.820524  ]\n",
      "   [-10.1322155    4.096334     1.2889549   -8.946105    -7.003514\n",
      "     -2.119969    -5.464064    -9.032952  ]\n",
      "   [ -6.2441616    5.4699683    0.8303648   -2.5861697   -6.0254755\n",
      "     -5.8178663   -3.9932399   -6.5110664 ]\n",
      "   [ -7.973193     7.724517     4.6691766   -4.006742    -8.032589\n",
      "     -1.6155707   -8.793758    -7.380326  ]\n",
      "   [ -5.8270884    2.8298204    4.595993    -2.1421301   -1.5096639\n",
      "     -4.502131    -9.880545    -6.981122  ]\n",
      "   [ -1.5424074   -0.03381129   0.8708008    0.84780496  -2.2531416\n",
      "     -0.00659109   0.6038286   -2.874726  ]]\n",
      "\n",
      "  [[ -2.8292158    1.3493172   -1.1630216   -2.1086867   -0.7928409\n",
      "     -1.6628647    0.936982     0.17500922]\n",
      "   [ -4.8446875    2.2061067    5.2808175   -7.194037     1.36124\n",
      "      0.13124909  -5.7810917   -5.9226546 ]\n",
      "   [ -5.9484262    1.2807072    4.3257394   -5.866291    -1.1549981\n",
      "      0.3883991   -5.670845    -4.975041  ]\n",
      "   [ -4.245631     5.4597836    3.617752    -7.2080035   -3.3983228\n",
      "     -4.2374063   -2.325605    -2.7597272 ]\n",
      "   [ -3.7233293    5.6208553    0.9434773   -6.0832963   -2.91809\n",
      "     -0.02632309  -1.9356793   -0.92453605]\n",
      "   [ -2.065824     5.411931     2.41236     -4.3067594    2.3385603\n",
      "     -1.8093777   -4.6885967   -1.7742286 ]\n",
      "   [ -3.8498166    5.2004757   -1.0518612   -7.7332063   -0.27363002\n",
      "      0.8425215   -2.1548126   -2.9958065 ]\n",
      "   [ -1.0712852    3.3348348    0.1982265   -2.701739     0.3845683\n",
      "     -1.0062916    2.1746593    0.6345082 ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.108165     2.77802      7.04615      2.6454995    5.4191833\n",
      "      5.2317686   -5.6294155   -9.130048  ]\n",
      "   [-10.735054     1.7547116    0.76175237  -0.42852682   4.004292\n",
      "      2.5349874   -3.68677    -13.101013  ]\n",
      "   [ -7.5400214    0.57103527  -0.5523173    0.4326064    1.1318936\n",
      "      0.61845857  -3.0292842   -9.038257  ]\n",
      "   [-13.195052     3.5323567    3.9254384    3.0674067    4.645694\n",
      "      4.6829467   -5.6105638  -17.256235  ]\n",
      "   [-10.567291     1.5221579    0.52586746   1.5666054    3.392625\n",
      "      3.7933207   -3.6424325  -13.40815   ]\n",
      "   [-11.348783     1.1702118    0.33376536   1.9208177    4.171567\n",
      "      3.2230349   -4.625526   -14.001499  ]\n",
      "   [-13.822072     4.6995225    6.882736     4.5583987    9.456506\n",
      "      8.984521    -7.40026    -19.019571  ]\n",
      "   [ -4.276782     2.2549555    6.41255      2.478643     5.0064325\n",
      "      5.4905148   -5.49556     -7.172459  ]]\n",
      "\n",
      "  [[ -2.6697712    0.18644784  -3.2964184   -9.0699005   -5.8869786\n",
      "     -8.670129    -1.7457955   -2.4067173 ]\n",
      "   [ -4.892798     5.1958284    0.86651105  -1.7851338   -0.5991809\n",
      "      1.0769178   -6.025225    -5.985986  ]\n",
      "   [ -9.587597     3.4813366   -1.9858788   -0.24309416   0.67243534\n",
      "      4.1456385   -5.7291985  -11.872933  ]\n",
      "   [-10.449494     4.90856     -2.7020588   -1.3338774    0.16304302\n",
      "      4.3895545   -7.198788   -13.19233   ]\n",
      "   [ -6.8310695    3.5543683   -0.808639     0.5658518    1.5122908\n",
      "      3.5409427   -5.93916     -8.560726  ]\n",
      "   [-10.439719     3.3256927   -2.8369732   -1.1681513    0.17411843\n",
      "      4.7924056   -7.1944017  -12.024714  ]\n",
      "   [ -4.1172786    3.8512855   -1.755627    -3.863435    -1.9191699\n",
      "     -2.5844417   -4.672864    -4.7063437 ]\n",
      "   [ -1.7964184   -0.00925941  -3.5144498   -9.731884    -6.624717\n",
      "    -10.108276    -0.76045424  -0.7916409 ]]\n",
      "\n",
      "  [[ -4.7987876    2.3515558   -2.4956558    3.970095     6.9596395\n",
      "      6.1769433    2.5710814   -5.3403516 ]\n",
      "   [ -6.3391857    1.296634    -1.2071677   -2.4126644   -0.8230457\n",
      "      1.5592674   -4.067092    -6.657909  ]\n",
      "   [ -9.1259985    8.161333     4.880271    -1.283182     4.1173687\n",
      "      4.9365716   -5.1432853  -11.959505  ]\n",
      "   [-10.686145     8.565781     5.08118      0.27082056   5.3818784\n",
      "      6.742503    -2.9075274  -13.400082  ]\n",
      "   [ -9.507849    10.487074     6.0747166   -1.3516918    4.202924\n",
      "      5.809277    -3.300832   -12.412424  ]\n",
      "   [ -9.160462    10.127481     7.924963     1.2205707    6.1613603\n",
      "      7.179847    -3.3297603  -12.610715  ]\n",
      "   [ -3.074413    11.459646     8.69444      8.782502    13.618378\n",
      "      9.073093     3.539803    -6.4021416 ]\n",
      "   [ -2.5498397    2.3677871   -2.3312814    5.376789     7.2629266\n",
      "      5.222065     4.6978946   -2.8941624 ]]\n",
      "\n",
      "  [[  5.7272882    2.8488104   -2.2674716    2.4710479    1.0492487\n",
      "     -1.1509322    1.7533069    4.0113854 ]\n",
      "   [ -6.0593033   -0.8574972   -0.14114377   2.036406     0.46856305\n",
      "      3.5360727    1.1927096   -6.1919413 ]\n",
      "   [ -3.0789824   -0.10847238   1.995033     3.6496744    3.8057036\n",
      "      6.084562     4.08223     -3.2766128 ]\n",
      "   [ -4.919659     1.601183     1.003934     3.2019112    0.6709702\n",
      "      4.7461705    1.0749629   -5.1771154 ]\n",
      "   [ -6.994366     0.38075814  -0.8496923    2.2005079    2.1127446\n",
      "      4.8004184    1.2863806   -7.113053  ]\n",
      "   [ -1.9637377    2.5683565    0.11696774   0.11398675  -0.02081025\n",
      "      1.7587206   -0.9755858   -2.719708  ]\n",
      "   [ -4.988607     7.0706754    1.6426951    7.2739515    7.1590147\n",
      "      9.629911     6.3608046   -7.190774  ]\n",
      "   [  7.1238737    5.775128    -1.2283382    3.9956207    3.3504624\n",
      "      0.41485792   3.2472675    4.7716117 ]]\n",
      "\n",
      "  [[ 15.227098    -7.6481614   -6.1704655   -2.9068828   -4.9539256\n",
      "     -6.9440393   -5.5164175   14.977688  ]\n",
      "   [ -9.1538725    2.5506845    3.8603075   -0.97211933  -0.7266477\n",
      "      1.0761876    4.012439    -8.621349  ]\n",
      "   [ -7.3554325    3.0492685    4.882066     1.9635559    1.5533912\n",
      "      2.5195072    5.0332184   -6.660119  ]\n",
      "   [ -7.502227     4.479796     3.8863442    0.8529868    2.5325358\n",
      "      2.6109834    3.7704332   -7.242996  ]\n",
      "   [-14.381295     7.1894965    6.8530087    0.8468516    1.6221398\n",
      "      3.4500875    6.026878   -14.084528  ]\n",
      "   [-12.931715     6.7301073    6.57782      0.15552036   0.9927032\n",
      "      0.9647971    3.3784504  -12.61685   ]\n",
      "   [  0.44247183  -0.69109786  -1.4392438   -1.959099    -3.2892675\n",
      "     -3.1996653   -2.5787928   -0.35673866]\n",
      "   [ 16.702045    -7.9659853   -7.5883594   -3.50606     -4.3180175\n",
      "     -7.998839    -8.3601055   15.943729  ]]\n",
      "\n",
      "  [[ 16.92651     -1.8878899    4.612371     8.297861     8.785893\n",
      "     10.494433    13.696043    18.756512  ]\n",
      "   [ -4.8187237    0.3554866   -0.10856649  -0.7664691   -0.8179527\n",
      "     -2.149825    -1.7334187   -4.167079  ]\n",
      "   [ -9.965244     1.1075305   -3.9921682   -5.4180512   -3.1439044\n",
      "     -9.256229    -8.250524   -10.748956  ]\n",
      "   [ -7.730639     0.702649    -3.5076466   -2.8937106   -1.1201787\n",
      "     -2.574378    -2.7975821   -7.5726776 ]\n",
      "   [-10.1452055   -1.0527366   -4.0010457   -2.8202963   -3.0494757\n",
      "     -5.4484124   -5.228744   -10.110497  ]\n",
      "   [-11.985994     3.1561482   -2.7560956   -3.3319776   -1.583422\n",
      "     -4.690591    -5.913287   -12.567582  ]\n",
      "   [ -5.3008895   -3.0260174   -2.1208181   -1.0755001   -2.3386164\n",
      "     -0.51985437  -1.1730318   -4.255245  ]\n",
      "   [ 17.86222     -1.6225508    5.1695466    7.7438717    8.076046\n",
      "     10.810771    13.726412    19.605438  ]]\n",
      "\n",
      "  [[ -5.9784994   -2.1169765    4.562182     0.5942515    3.0359569\n",
      "      1.3266702   -0.49234566  -3.304644  ]\n",
      "   [-12.7146845    2.0409193    2.1862695   -0.09457824   1.4637623\n",
      "      2.2060208   -4.997251   -13.3957815 ]\n",
      "   [-15.493677     3.6968644    4.4985094    1.5548421    1.8590727\n",
      "      4.2570806   -6.917543   -16.565073  ]\n",
      "   [ -9.458374    -0.38232473   3.5684469    0.3583388    1.3042445\n",
      "      1.8564001   -4.3985405   -9.246963  ]\n",
      "   [-12.0117655    2.1870446    3.3004608    0.608759     0.99599266\n",
      "      1.7472012   -5.940463   -13.297273  ]\n",
      "   [ -9.835355    -0.9845663   -0.7273823   -2.8138676   -0.3688764\n",
      "      0.66789573  -4.370575    -9.257524  ]\n",
      "   [ -3.3896937   -3.072007     1.3647641    1.7215154    1.1578857\n",
      "      2.4925263    3.2728026   -0.55832624]\n",
      "   [ -2.0235658   -2.5166588    4.0697465    1.5893646    3.5398774\n",
      "      1.8933309    2.816157     1.3247585 ]]\n",
      "\n",
      "  [[ 13.876768     5.289463     2.8875844    5.729185    -0.31207085\n",
      "      2.216713     0.970955    11.02701   ]\n",
      "   [-11.855931     7.044625     2.757176    -2.0767522    9.620936\n",
      "      3.4307175   -0.20549646 -11.727619  ]\n",
      "   [-17.164427     9.00703      2.9232194   -2.114338    10.674683\n",
      "      4.7908983    0.6977465  -17.203806  ]\n",
      "   [ -6.0810537    6.5570908    3.815859     1.4116415    7.24501\n",
      "      5.5307817    2.8692966   -6.8714347 ]\n",
      "   [ -9.309462     5.957898     3.5396616   -0.5519851    6.9079504\n",
      "      5.2796082    1.4829706   -9.929194  ]\n",
      "   [ -9.980593     6.659394     3.6414435   -1.2484376    6.108645\n",
      "      4.148278     2.2938857  -10.687042  ]\n",
      "   [-11.461593     8.152552     3.527002    -0.32889172   5.8540664\n",
      "      4.3797064    2.374406   -12.23368   ]\n",
      "   [ 16.641361     4.062904     2.288843     5.4982553   -3.302351\n",
      "      0.8199426    0.47605655  13.646871  ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 3)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.09299298  0.60130334]\n",
      "   [-0.18269673 -4.40855     0.07642998]\n",
      "   [-0.11618186  3.2863328  -3.025875  ]]\n",
      "\n",
      "  [[-0.1123142   0.56263393  0.21921782]\n",
      "   [-0.29892173 -1.5473536   1.286812  ]\n",
      "   [-0.25255403  2.1486242  -6.0938    ]]\n",
      "\n",
      "  [[-0.10918517  0.11000926  0.21102953]\n",
      "   [ 0.07259987 -4.5046606  -4.6542673 ]\n",
      "   [-0.0773119  -0.45317498  3.3376467 ]]\n",
      "\n",
      "  [[-0.22114104 -0.48759893  0.03473653]\n",
      "   [ 0.67009145 -7.431193   -0.8688647 ]\n",
      "   [-0.6334822   2.1947665  -1.3594295 ]]\n",
      "\n",
      "  [[ 0.00076126  0.07451362  0.17366233]\n",
      "   [-0.10380323 -2.2713957  -1.0344037 ]\n",
      "   [ 0.00120817 -2.261606   -4.7580304 ]]\n",
      "\n",
      "  [[-0.00999826  0.21593446 -0.3890052 ]\n",
      "   [ 0.9415558  -2.968748   -1.1732452 ]\n",
      "   [-0.03670223 -0.73637694  3.4228125 ]]\n",
      "\n",
      "  [[-0.20863976 -0.27273425  0.42647386]\n",
      "   [ 0.59413576 -7.0776234   0.3937803 ]\n",
      "   [ 0.96936405  1.54816    -5.38302   ]]\n",
      "\n",
      "  [[-0.15726982 -0.16104104  0.07374533]\n",
      "   [-0.19427125 -5.400611    2.8342748 ]\n",
      "   [ 0.9997108  -1.4077754   0.58703816]]]], shape=(1, 8, 3, 3), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 3)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.64339346 -3.427214   -1.9332778  -1.831178   -2.9725688\n",
      "    -3.3511758  -0.33726037  0.65105456]\n",
      "   [-0.9194904   0.8968038   0.45222297  1.0594327   1.1557782\n",
      "    -0.8956241  -0.17587543 -0.8396095 ]\n",
      "   [-0.04047573 -0.15472572 -0.69453156  0.27039978 -0.9049967\n",
      "     0.7182969  -0.13295098 -0.00661357]]\n",
      "\n",
      "  [[ 1.1482323  -9.236041   -8.218754   -6.717816   -9.638861\n",
      "    -8.856656   -2.9334142   1.2082323 ]\n",
      "   [-1.8857075   3.473417    2.5907042   3.5434728   3.9081209\n",
      "     5.844051    1.0954899  -1.8498287 ]\n",
      "   [ 0.47446606 -0.28332782  0.63271886 -0.17188838  0.33722964\n",
      "     0.37442216  0.03080073  0.46110418]]\n",
      "\n",
      "  [[-1.0731127  -1.8999621  -2.2802618  -2.3014586  -1.5626087\n",
      "    -1.5433154  -1.408609   -1.0031282 ]\n",
      "   [-0.15004301 -3.7747118  -4.124579   -3.467151   -3.0495815\n",
      "    -2.7865295  -1.1569786  -0.1172325 ]\n",
      "   [ 1.0226461   0.42154983  0.85200024  1.2949932  -0.23638739\n",
      "     1.2461828   0.683046    0.9681701 ]]\n",
      "\n",
      "  [[-0.4604545  -1.5868427  -0.91173816 -0.8565646  -1.1848444\n",
      "    -2.0918827  -0.78210574 -0.41651735]\n",
      "   [-1.3551308   0.1230931   0.7364044  -0.6643719   0.8351439\n",
      "     0.06500757  0.78761655 -1.2645116 ]\n",
      "   [ 0.68831766  0.8079782  -0.10353167  1.935387    0.18036631\n",
      "     2.0298638  -0.18135574  0.6273567 ]]\n",
      "\n",
      "  [[-1.5332501   4.991253    6.4686904   4.0847487   5.368916\n",
      "     4.911813    2.5922883  -1.523277  ]\n",
      "   [-1.8101153   6.2314014   5.68846     4.518671    6.62946\n",
      "     6.433606    3.0988286  -1.8167404 ]\n",
      "   [ 1.1217315  -4.2282114  -4.6459684  -2.7598808  -3.6334445\n",
      "    -3.9288173  -2.1453893   1.1145377 ]]\n",
      "\n",
      "  [[ 1.6350883  -6.317388   -5.912019   -3.864096   -6.084104\n",
      "    -5.8046737  -1.3961531   1.6473184 ]\n",
      "   [ 0.24405284  1.462353    0.9224829   1.581335    1.9375173\n",
      "     2.096835    2.0256872   0.28618026]\n",
      "   [ 1.4571862  -8.192412   -7.3626842  -4.9873695  -7.7452354\n",
      "    -6.7284403  -1.935825    1.4636596 ]]\n",
      "\n",
      "  [[ 0.12968476 -2.7599597  -3.3458984  -0.6776076  -1.3498564\n",
      "    -2.4725385  -0.26429352  0.143075  ]\n",
      "   [ 0.16608208 -3.8964067  -3.9270244  -2.1397204  -2.7393942\n",
      "    -2.4727707  -0.7227451   0.23026752]\n",
      "   [ 0.2666929   0.5002075   0.8165109   1.2050005   1.1429677\n",
      "     2.0832279   1.431956    0.24901326]]\n",
      "\n",
      "  [[-0.48440975 -0.30788445 -1.4893405  -0.5263086   0.2881688\n",
      "    -0.46770203 -0.27271533 -0.46877885]\n",
      "   [-0.3127084  -1.174712   -0.8735866  -0.32615614 -0.25206855\n",
      "    -0.55387217  0.23482844 -0.2958877 ]\n",
      "   [ 0.75498635  1.107361    1.7389252   1.5763456   1.1201576\n",
      "     1.3887876   0.36387968  0.70142406]]]], shape=(1, 8, 3, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 8)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 3)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.8282402  -0.42404926 -0.8946604 ]\n",
      "   [-1.2308896   2.5182536   0.9160354 ]\n",
      "   [ 1.7116143   0.36283326 -0.38897648]]\n",
      "\n",
      "  [[ 0.05669579 -0.14652377  0.7225302 ]\n",
      "   [-1.3575498   1.1293744  -1.4357362 ]\n",
      "   [ 3.1011949   1.2757022  -1.3072673 ]]\n",
      "\n",
      "  [[-3.0994341   3.915559    3.6157894 ]\n",
      "   [-6.1072607  -1.5910499   4.307164  ]\n",
      "   [-1.8124647   3.9855342   1.44573   ]]\n",
      "\n",
      "  [[-2.9529114   2.299974    3.0738797 ]\n",
      "   [-7.031095   -0.24240322  8.241785  ]\n",
      "   [-4.861674   -1.6940056   2.8614326 ]]\n",
      "\n",
      "  [[-0.5371959   0.03175565  0.72881126]\n",
      "   [-1.4198313  -6.4444065  -0.92142916]\n",
      "   [ 2.8078191   2.1120253  -2.642926  ]]\n",
      "\n",
      "  [[-1.412344    1.717014   -0.30293384]\n",
      "   [-1.9951228  -3.2546551   1.0123291 ]\n",
      "   [ 0.73561543  0.3251526  -3.0940294 ]]\n",
      "\n",
      "  [[-2.4360008   0.67301464 -0.9478727 ]\n",
      "   [-0.27072212 -7.063433    1.4467108 ]\n",
      "   [ 3.4422889  -1.1130698  -3.4816942 ]]\n",
      "\n",
      "  [[-1.929139    0.8935091   1.5377133 ]\n",
      "   [-1.0276425  -4.9508924   3.0305479 ]\n",
      "   [ 0.29604688 -1.3716084  -0.20351243]]]], shape=(1, 8, 3, 3), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 3)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.3973298    3.2900124    5.064679     2.9755645    4.015015\n",
      "      4.0069337   -0.21337411  -0.49866062]\n",
      "   [ -0.33577263   4.7211685    5.425065     3.9286716    3.8147593\n",
      "      3.5898767    0.9110762   -0.36344096]\n",
      "   [ -0.05790175   0.3653249   -0.63628477  -0.08784539  -0.41260353\n",
      "     -1.178433     0.6327939   -0.04360336]]\n",
      "\n",
      "  [[ -0.6408605    3.2326546    3.591694     2.0563188    3.0459878\n",
      "      4.4082403   -0.3960128   -0.63534915]\n",
      "   [ -1.1932893    5.5270476    5.752577     4.0630784    4.8154626\n",
      "      5.298501     1.4706326   -1.2358047 ]\n",
      "   [ -0.52651423   0.23078535   0.45221442   1.5162919    0.8854996\n",
      "      0.6778835    1.459687    -0.5176164 ]]\n",
      "\n",
      "  [[ -0.6057757    1.7133442    2.8641703    0.5958629    2.7594314\n",
      "      1.3030896   -0.8051967   -0.6426489 ]\n",
      "   [ -0.22311665  -8.5990305  -10.39877     -8.644732   -10.273176\n",
      "     -8.879024    -2.734983    -0.12058785]\n",
      "   [  0.5510202   -5.272624    -5.8298817   -4.458143    -6.6167383\n",
      "     -5.437187    -0.91685784   0.5980707 ]]\n",
      "\n",
      "  [[ -1.699932     4.244729     6.19011      3.755602     5.4355664\n",
      "      6.699038    -0.3490838   -1.7462924 ]\n",
      "   [  1.4002923    0.54548395  -1.1169968    0.37443233  -1.1300082\n",
      "     -1.9262465    2.2690759    1.3882834 ]\n",
      "   [  1.6556793   -4.4972763   -5.2453957   -3.274147    -4.7581916\n",
      "     -5.8438854    1.4371101    1.7192887 ]]\n",
      "\n",
      "  [[ -1.2787998    2.4755638    3.703323     1.3851306    3.2478092\n",
      "      3.6466854    0.553544    -1.0989556 ]\n",
      "   [ -0.68938744   0.7849126    0.80624384  -0.06641847   0.84722805\n",
      "      1.4088807    0.84816337  -0.6531029 ]\n",
      "   [  0.3542869   -2.6495297   -3.9148326   -2.3332536   -4.0371146\n",
      "     -2.1969824   -0.3534847    0.32806468]]\n",
      "\n",
      "  [[ -0.6709214    1.5035548    1.64232     -0.33530334   0.9649067\n",
      "      0.04905168  -0.89175946  -0.71355253]\n",
      "   [  0.24583115   0.6569769   -0.621118     1.2669846    0.91026103\n",
      "      1.4697332    1.5297475    0.26090285]\n",
      "   [  0.9396693    1.0186071    0.9804496    3.2190533    1.4295805\n",
      "      2.6770215    2.6240253    0.95563656]]\n",
      "\n",
      "  [[ -0.5303791   -2.110146    -1.3960739   -2.3525817   -1.9407862\n",
      "     -0.8966946   -2.0528047   -0.50565517]\n",
      "   [ -0.09481095  -1.1549708    0.31408975  -0.03525389  -0.4600581\n",
      "     -1.6961929    0.7154125   -0.0880874 ]\n",
      "   [  0.5121612   -0.1614938    0.4048692    0.8203044    0.5385774\n",
      "     -0.67759335   1.620004     0.4480935 ]]\n",
      "\n",
      "  [[ -1.6133933    5.655559     8.456806     4.720569     6.469103\n",
      "      6.3866005   -0.77200097  -1.6862562 ]\n",
      "   [  2.6422656   -3.0630863   -6.9324884   -2.84115     -4.448012\n",
      "     -5.5646486    1.8054792    2.638062  ]\n",
      "   [  2.5069184   -6.87229    -10.355062    -4.8297977   -8.231734\n",
      "     -8.594013     0.42899096   2.5198808 ]]]], shape=(1, 8, 3, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 8)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724    3.119958     0.8104701   -3.7867832   -0.53479564\n",
      "     -0.68400717  -2.684907    -4.306619  ]\n",
      "   [ -2.3828275    6.9414687    0.72609663  -5.475966    -3.1419044\n",
      "     -4.861004    -1.5760975   -1.7812909 ]\n",
      "   [ -9.4575205    9.2924795   -0.14664039  -7.7748985    0.3141518\n",
      "     -4.7069764   -3.4037216   -7.8661847 ]\n",
      "   [-10.166894    14.483014     3.299315   -11.655867    -6.542652\n",
      "     -5.2036433   -7.0711117   -9.239806  ]\n",
      "   [ -7.9254494   12.0516       3.657774   -14.190444    -6.1069603\n",
      "     -6.00821     -5.337775    -6.7280397 ]\n",
      "   [ -9.514821    12.602872     2.7181993  -11.5606165   -3.807155\n",
      "     -4.5184774   -7.4839892   -9.5418    ]\n",
      "   [ -6.294987     8.895258     0.31260866  -7.041161    -2.2837403\n",
      "     -4.8525386   -4.4346924   -5.9981575 ]\n",
      "   [ -1.627529     0.2572786    0.33721286  -1.0157803   -0.02855057\n",
      "      0.4145152   -2.0903144   -3.1125855 ]]\n",
      "\n",
      "  [[ -5.830585     2.9278567    6.4656816    2.9012501    0.7074189\n",
      "      1.8678039   -0.98469496  -5.710069  ]\n",
      "   [ -0.888771    -1.5304633   -0.92677224   7.0835996   -2.1796093\n",
      "      3.6029007   -0.78117675  -0.31565633]\n",
      "   [ -5.1757755   -0.00269019   5.1771684    2.4006374    1.5459608\n",
      "      4.5166783   -1.3455517   -7.2344646 ]\n",
      "   [ -0.32639125   1.338475     6.20836     -2.123073     1.0238662\n",
      "      0.95025975  -0.588602    -0.29866517]\n",
      "   [ -3.7669215    0.96661407   6.3918867   -2.0059547   -1.667028\n",
      "      2.4628794   -0.9746783   -5.5741014 ]\n",
      "   [ -5.9424763    3.3478012    5.0460277    5.0597334    1.5781783\n",
      "      5.766434    -3.736297    -6.4208727 ]\n",
      "   [ -5.028427     2.1733994    8.720076    -5.247528     2.0949328\n",
      "      0.3581393   -1.2882515   -6.517891  ]\n",
      "   [ -4.8532395    3.40758      5.827717     0.8119974    1.7706672\n",
      "      1.3585875   -2.134433    -4.6770263 ]]\n",
      "\n",
      "  [[  0.21261294   2.1825705    2.6290977   -0.7738304   -0.92338926\n",
      "      1.3111119   -1.7676708   -1.2661753 ]\n",
      "   [ -7.103699    -0.08641089   0.6366692   -1.1594403    0.52412534\n",
      "     -1.2009193   -3.281907    -9.214137  ]\n",
      "   [ -7.0138736    4.6630197   -1.6835009    0.64012516  -0.3236557\n",
      "      6.017604    -2.2194111   -8.46463   ]\n",
      "   [ -5.658084     3.870231     1.6583294   -3.8960452   -0.90947014\n",
      "     -1.150247     0.02895866  -8.4907255 ]\n",
      "   [ -2.1916983    2.1456993   -0.696395     2.0237832    0.4654629\n",
      "      2.97788     -1.8672262   -3.7927125 ]\n",
      "   [ -4.237206     0.8983024   -1.8900397   -1.5124333   -1.4959668\n",
      "     -3.0116844   -4.190044    -5.1174064 ]\n",
      "   [  0.44389454   0.6015544    1.0924258   -0.6734957   -0.14239292\n",
      "     -0.31658053  -3.1918075   -2.5049903 ]\n",
      "   [ -0.5491213    1.3170428    2.416191    -0.72504455   0.17214571\n",
      "      0.6214916   -0.7466009   -2.0425894 ]]\n",
      "\n",
      "  [[ -0.62310183   1.8474774   -0.70651436  -1.8744931    1.9892807\n",
      "      1.1960789    0.93903846  -0.96611404]\n",
      "   [ -2.1139884   -0.40789947   1.2968414   -2.4691556   -0.6430844\n",
      "      2.9704242   -0.49229723  -3.2126625 ]\n",
      "   [ -4.681421     1.071365     7.057754     1.6925517    2.925461\n",
      "      1.2117237   -2.5487785   -7.727114  ]\n",
      "   [ -5.2494354    2.6016214    4.7093654   -2.9185028   11.68196\n",
      "      5.0173087    0.2649115   -6.465863  ]\n",
      "   [ -4.187672     2.2745495    2.1844132   -2.950158     0.4811062\n",
      "      2.5944564   -1.2815626   -5.458976  ]\n",
      "   [ -5.1851773    3.6045432    5.879984    -1.5479906    9.332985\n",
      "      0.7532006   -6.1630864  -10.190875  ]\n",
      "   [ -1.4530067    3.602885     1.8737535   -1.0183158    6.639346\n",
      "     -0.45745188  -3.0397322   -5.1571107 ]\n",
      "   [ -1.3701262    1.7800907    0.4440075   -2.7584684    0.16184288\n",
      "      1.0718383    0.74469614  -2.4832382 ]]\n",
      "\n",
      "  [[ -3.365542     1.7112432   -0.18042703  -0.56367624  -1.0182812\n",
      "      0.91838205   0.3576792   -1.8675333 ]\n",
      "   [ -4.8425922   -4.2636166   -2.6981843    0.22403693   0.68739414\n",
      "     -3.1009803    2.71456     -1.489905  ]\n",
      "   [ -5.1876755    5.671311    -1.2768452   -2.9591484   -0.39675456\n",
      "     -3.3887212    1.0478692   -2.3474293 ]\n",
      "   [ -2.170307     3.4666238    1.0260042   -0.8940675    2.2694418\n",
      "     -2.5140228    1.5545614   -1.9796547 ]\n",
      "   [ -7.992303     0.4727184   -0.13311552  -3.517133    -2.5849926\n",
      "     -4.228556    -1.0881504   -5.6788545 ]\n",
      "   [ -3.6134825    0.19133087   0.298861    -4.0497766   -2.2160487\n",
      "     -3.5523865    0.53147453  -4.259141  ]\n",
      "   [ -2.6600354    4.010139     4.72034     -3.9068446   -1.4782856\n",
      "     -0.05746605  -5.8499537   -1.9302822 ]\n",
      "   [  2.192343    -2.0631888   -0.458438     0.3939276    0.19301154\n",
      "      3.2884934   -1.1612332    1.5978622 ]]\n",
      "\n",
      "  [[ -2.1686854   -0.68126124   1.8873476   -1.0016267    1.665862\n",
      "     -0.39142966  -2.5463562   -2.0510204 ]\n",
      "   [ -3.4723454   -2.521675     3.3836114   -0.29911357   3.7395344\n",
      "      2.2255218   -0.5534243   -3.5096362 ]\n",
      "   [ -1.241272    -1.0362755    0.8233798    0.13532306   0.49612203\n",
      "      2.9168866   -0.06637239   0.07041088]\n",
      "   [  0.16238996  -2.266713    -0.43468216  -1.9490578   -0.7804693\n",
      "     -2.2501192    0.14404878   0.669509  ]\n",
      "   [ -4.5097795    0.04322206   2.9916728    4.4943275    1.4705126\n",
      "     -0.8136589   -2.140882    -5.1061444 ]\n",
      "   [ -6.6359534    3.4645762    4.6767483   -2.6971583    5.242417\n",
      "      3.411301     1.1889111   -5.1261134 ]\n",
      "   [ -2.4793563    1.550663     3.5571485   -3.2087276    2.422179\n",
      "      0.22151406  -3.0109324   -3.599103  ]\n",
      "   [ -1.2139645   -1.8291416    2.1708345   -1.4788015    1.7007154\n",
      "     -1.5888573   -2.220674    -2.1623607 ]]\n",
      "\n",
      "  [[ -0.3049526    1.6942294   -0.7551851   -0.20310196  -1.9716048\n",
      "      0.50340194   0.2921721   -1.8446437 ]\n",
      "   [ -3.5450757    9.210564     3.1409702   -1.4201491   -8.377774\n",
      "     -2.9744062   -6.1630034   -6.277001  ]\n",
      "   [-10.783727     9.743        1.6049056   -7.786639    -9.65972\n",
      "     -3.0615754   -6.16684     -9.820524  ]\n",
      "   [-10.1322155    4.096334     1.2889549   -8.946105    -7.003514\n",
      "     -2.119969    -5.464064    -9.032952  ]\n",
      "   [ -6.2441616    5.4699683    0.8303648   -2.5861697   -6.0254755\n",
      "     -5.8178663   -3.9932399   -6.5110664 ]\n",
      "   [ -7.973193     7.724517     4.6691766   -4.006742    -8.032589\n",
      "     -1.6155707   -8.793758    -7.380326  ]\n",
      "   [ -5.8270884    2.8298204    4.595993    -2.1421301   -1.5096639\n",
      "     -4.502131    -9.880545    -6.981122  ]\n",
      "   [ -1.5424074   -0.03381129   0.8708008    0.84780496  -2.2531416\n",
      "     -0.00659109   0.6038286   -2.874726  ]]\n",
      "\n",
      "  [[ -2.8292158    1.3493172   -1.1630216   -2.1086867   -0.7928409\n",
      "     -1.6628647    0.936982     0.17500922]\n",
      "   [ -4.8446875    2.2061067    5.2808175   -7.194037     1.36124\n",
      "      0.13124909  -5.7810917   -5.9226546 ]\n",
      "   [ -5.9484262    1.2807072    4.3257394   -5.866291    -1.1549981\n",
      "      0.3883991   -5.670845    -4.975041  ]\n",
      "   [ -4.245631     5.4597836    3.617752    -7.2080035   -3.3983228\n",
      "     -4.2374063   -2.325605    -2.7597272 ]\n",
      "   [ -3.7233293    5.6208553    0.9434773   -6.0832963   -2.91809\n",
      "     -0.02632309  -1.9356793   -0.92453605]\n",
      "   [ -2.065824     5.411931     2.41236     -4.3067594    2.3385603\n",
      "     -1.8093777   -4.6885967   -1.7742286 ]\n",
      "   [ -3.8498166    5.2004757   -1.0518612   -7.7332063   -0.27363002\n",
      "      0.8425215   -2.1548126   -2.9958065 ]\n",
      "   [ -1.0712852    3.3348348    0.1982265   -2.701739     0.3845683\n",
      "     -1.0062916    2.1746593    0.6345082 ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.108165     2.77802      7.04615      2.6454995    5.4191833\n",
      "      5.2317686   -5.6294155   -9.130048  ]\n",
      "   [-10.735054     1.7547116    0.76175237  -0.42852682   4.004292\n",
      "      2.5349874   -3.68677    -13.101013  ]\n",
      "   [ -7.5400214    0.57103527  -0.5523173    0.4326064    1.1318936\n",
      "      0.61845857  -3.0292842   -9.038257  ]\n",
      "   [-13.195052     3.5323567    3.9254384    3.0674067    4.645694\n",
      "      4.6829467   -5.6105638  -17.256235  ]\n",
      "   [-10.567291     1.5221579    0.52586746   1.5666054    3.392625\n",
      "      3.7933207   -3.6424325  -13.40815   ]\n",
      "   [-11.348783     1.1702118    0.33376536   1.9208177    4.171567\n",
      "      3.2230349   -4.625526   -14.001499  ]\n",
      "   [-13.822072     4.6995225    6.882736     4.5583987    9.456506\n",
      "      8.984521    -7.40026    -19.019571  ]\n",
      "   [ -4.276782     2.2549555    6.41255      2.478643     5.0064325\n",
      "      5.4905148   -5.49556     -7.172459  ]]\n",
      "\n",
      "  [[ -2.6697712    0.18644784  -3.2964184   -9.0699005   -5.8869786\n",
      "     -8.670129    -1.7457955   -2.4067173 ]\n",
      "   [ -4.892798     5.1958284    0.86651105  -1.7851338   -0.5991809\n",
      "      1.0769178   -6.025225    -5.985986  ]\n",
      "   [ -9.587597     3.4813366   -1.9858788   -0.24309416   0.67243534\n",
      "      4.1456385   -5.7291985  -11.872933  ]\n",
      "   [-10.449494     4.90856     -2.7020588   -1.3338774    0.16304302\n",
      "      4.3895545   -7.198788   -13.19233   ]\n",
      "   [ -6.8310695    3.5543683   -0.808639     0.5658518    1.5122908\n",
      "      3.5409427   -5.93916     -8.560726  ]\n",
      "   [-10.439719     3.3256927   -2.8369732   -1.1681513    0.17411843\n",
      "      4.7924056   -7.1944017  -12.024714  ]\n",
      "   [ -4.1172786    3.8512855   -1.755627    -3.863435    -1.9191699\n",
      "     -2.5844417   -4.672864    -4.7063437 ]\n",
      "   [ -1.7964184   -0.00925941  -3.5144498   -9.731884    -6.624717\n",
      "    -10.108276    -0.76045424  -0.7916409 ]]\n",
      "\n",
      "  [[ -4.7987876    2.3515558   -2.4956558    3.970095     6.9596395\n",
      "      6.1769433    2.5710814   -5.3403516 ]\n",
      "   [ -6.3391857    1.296634    -1.2071677   -2.4126644   -0.8230457\n",
      "      1.5592674   -4.067092    -6.657909  ]\n",
      "   [ -9.1259985    8.161333     4.880271    -1.283182     4.1173687\n",
      "      4.9365716   -5.1432853  -11.959505  ]\n",
      "   [-10.686145     8.565781     5.08118      0.27082056   5.3818784\n",
      "      6.742503    -2.9075274  -13.400082  ]\n",
      "   [ -9.507849    10.487074     6.0747166   -1.3516918    4.202924\n",
      "      5.809277    -3.300832   -12.412424  ]\n",
      "   [ -9.160462    10.127481     7.924963     1.2205707    6.1613603\n",
      "      7.179847    -3.3297603  -12.610715  ]\n",
      "   [ -3.074413    11.459646     8.69444      8.782502    13.618378\n",
      "      9.073093     3.539803    -6.4021416 ]\n",
      "   [ -2.5498397    2.3677871   -2.3312814    5.376789     7.2629266\n",
      "      5.222065     4.6978946   -2.8941624 ]]\n",
      "\n",
      "  [[  5.7272882    2.8488104   -2.2674716    2.4710479    1.0492487\n",
      "     -1.1509322    1.7533069    4.0113854 ]\n",
      "   [ -6.0593033   -0.8574972   -0.14114377   2.036406     0.46856305\n",
      "      3.5360727    1.1927096   -6.1919413 ]\n",
      "   [ -3.0789824   -0.10847238   1.995033     3.6496744    3.8057036\n",
      "      6.084562     4.08223     -3.2766128 ]\n",
      "   [ -4.919659     1.601183     1.003934     3.2019112    0.6709702\n",
      "      4.7461705    1.0749629   -5.1771154 ]\n",
      "   [ -6.994366     0.38075814  -0.8496923    2.2005079    2.1127446\n",
      "      4.8004184    1.2863806   -7.113053  ]\n",
      "   [ -1.9637377    2.5683565    0.11696774   0.11398675  -0.02081025\n",
      "      1.7587206   -0.9755858   -2.719708  ]\n",
      "   [ -4.988607     7.0706754    1.6426951    7.2739515    7.1590147\n",
      "      9.629911     6.3608046   -7.190774  ]\n",
      "   [  7.1238737    5.775128    -1.2283382    3.9956207    3.3504624\n",
      "      0.41485792   3.2472675    4.7716117 ]]\n",
      "\n",
      "  [[ 15.227098    -7.6481614   -6.1704655   -2.9068828   -4.9539256\n",
      "     -6.9440393   -5.5164175   14.977688  ]\n",
      "   [ -9.1538725    2.5506845    3.8603075   -0.97211933  -0.7266477\n",
      "      1.0761876    4.012439    -8.621349  ]\n",
      "   [ -7.3554325    3.0492685    4.882066     1.9635559    1.5533912\n",
      "      2.5195072    5.0332184   -6.660119  ]\n",
      "   [ -7.502227     4.479796     3.8863442    0.8529868    2.5325358\n",
      "      2.6109834    3.7704332   -7.242996  ]\n",
      "   [-14.381295     7.1894965    6.8530087    0.8468516    1.6221398\n",
      "      3.4500875    6.026878   -14.084528  ]\n",
      "   [-12.931715     6.7301073    6.57782      0.15552036   0.9927032\n",
      "      0.9647971    3.3784504  -12.61685   ]\n",
      "   [  0.44247183  -0.69109786  -1.4392438   -1.959099    -3.2892675\n",
      "     -3.1996653   -2.5787928   -0.35673866]\n",
      "   [ 16.702045    -7.9659853   -7.5883594   -3.50606     -4.3180175\n",
      "     -7.998839    -8.3601055   15.943729  ]]\n",
      "\n",
      "  [[ 16.92651     -1.8878899    4.612371     8.297861     8.785893\n",
      "     10.494433    13.696043    18.756512  ]\n",
      "   [ -4.8187237    0.3554866   -0.10856649  -0.7664691   -0.8179527\n",
      "     -2.149825    -1.7334187   -4.167079  ]\n",
      "   [ -9.965244     1.1075305   -3.9921682   -5.4180512   -3.1439044\n",
      "     -9.256229    -8.250524   -10.748956  ]\n",
      "   [ -7.730639     0.702649    -3.5076466   -2.8937106   -1.1201787\n",
      "     -2.574378    -2.7975821   -7.5726776 ]\n",
      "   [-10.1452055   -1.0527366   -4.0010457   -2.8202963   -3.0494757\n",
      "     -5.4484124   -5.228744   -10.110497  ]\n",
      "   [-11.985994     3.1561482   -2.7560956   -3.3319776   -1.583422\n",
      "     -4.690591    -5.913287   -12.567582  ]\n",
      "   [ -5.3008895   -3.0260174   -2.1208181   -1.0755001   -2.3386164\n",
      "     -0.51985437  -1.1730318   -4.255245  ]\n",
      "   [ 17.86222     -1.6225508    5.1695466    7.7438717    8.076046\n",
      "     10.810771    13.726412    19.605438  ]]\n",
      "\n",
      "  [[ -5.9784994   -2.1169765    4.562182     0.5942515    3.0359569\n",
      "      1.3266702   -0.49234566  -3.304644  ]\n",
      "   [-12.7146845    2.0409193    2.1862695   -0.09457824   1.4637623\n",
      "      2.2060208   -4.997251   -13.3957815 ]\n",
      "   [-15.493677     3.6968644    4.4985094    1.5548421    1.8590727\n",
      "      4.2570806   -6.917543   -16.565073  ]\n",
      "   [ -9.458374    -0.38232473   3.5684469    0.3583388    1.3042445\n",
      "      1.8564001   -4.3985405   -9.246963  ]\n",
      "   [-12.0117655    2.1870446    3.3004608    0.608759     0.99599266\n",
      "      1.7472012   -5.940463   -13.297273  ]\n",
      "   [ -9.835355    -0.9845663   -0.7273823   -2.8138676   -0.3688764\n",
      "      0.66789573  -4.370575    -9.257524  ]\n",
      "   [ -3.3896937   -3.072007     1.3647641    1.7215154    1.1578857\n",
      "      2.4925263    3.2728026   -0.55832624]\n",
      "   [ -2.0235658   -2.5166588    4.0697465    1.5893646    3.5398774\n",
      "      1.8933309    2.816157     1.3247585 ]]\n",
      "\n",
      "  [[ 13.876768     5.289463     2.8875844    5.729185    -0.31207085\n",
      "      2.216713     0.970955    11.02701   ]\n",
      "   [-11.855931     7.044625     2.757176    -2.0767522    9.620936\n",
      "      3.4307175   -0.20549646 -11.727619  ]\n",
      "   [-17.164427     9.00703      2.9232194   -2.114338    10.674683\n",
      "      4.7908983    0.6977465  -17.203806  ]\n",
      "   [ -6.0810537    6.5570908    3.815859     1.4116415    7.24501\n",
      "      5.5307817    2.8692966   -6.8714347 ]\n",
      "   [ -9.309462     5.957898     3.5396616   -0.5519851    6.9079504\n",
      "      5.2796082    1.4829706   -9.929194  ]\n",
      "   [ -9.980593     6.659394     3.6414435   -1.2484376    6.108645\n",
      "      4.148278     2.2938857  -10.687042  ]\n",
      "   [-11.461593     8.152552     3.527002    -0.32889172   5.8540664\n",
      "      4.3797064    2.374406   -12.23368   ]\n",
      "   [ 16.641361     4.062904     2.288843     5.4982553   -3.302351\n",
      "      0.8199426    0.47605655  13.646871  ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.09299298  0.60130334  1.3376766 ]\n",
      "   [-0.18269673 -4.40855     0.07642998  2.0146773 ]\n",
      "   [-0.11618186  3.2863328  -3.025875    0.4890776 ]\n",
      "   [ 1.0314696  -1.1489769  -4.595038   -7.4570365 ]]\n",
      "\n",
      "  [[-0.1123142   0.56263393  0.21921782 -0.1352104 ]\n",
      "   [-0.29892173 -1.5473536   1.286812    2.3739398 ]\n",
      "   [-0.25255403  2.1486242  -6.0938      3.577338  ]\n",
      "   [ 0.24040057 -0.7833131  -1.2749462   2.9239094 ]]\n",
      "\n",
      "  [[-0.10918517  0.11000926  0.21102953  0.4795525 ]\n",
      "   [ 0.07259987 -4.5046606  -4.6542673  -1.4462825 ]\n",
      "   [-0.0773119  -0.45317498  3.3376467   3.8257363 ]\n",
      "   [ 0.6502231   2.718805    2.302118    3.527786  ]]\n",
      "\n",
      "  [[-0.22114104 -0.48759893  0.03473653  1.0193444 ]\n",
      "   [ 0.67009145 -7.431193   -0.8688647  -3.4817095 ]\n",
      "   [-0.6334822   2.1947665  -1.3594295   3.5641603 ]\n",
      "   [ 0.43472287 -3.067627    0.94878364 -1.7823364 ]]\n",
      "\n",
      "  [[ 0.00076126  0.07451362  0.17366233 -0.0934004 ]\n",
      "   [-0.10380323 -2.2713957  -1.0344037  -0.31538382]\n",
      "   [ 0.00120817 -2.261606   -4.7580304   3.1489854 ]\n",
      "   [ 0.92358315 -4.130864   -1.2742832   0.6084041 ]]\n",
      "\n",
      "  [[-0.00999826  0.21593446 -0.3890052   0.1919408 ]\n",
      "   [ 0.9415558  -2.968748   -1.1732452  -2.6510923 ]\n",
      "   [-0.03670223 -0.73637694  3.4228125  -0.21042274]\n",
      "   [ 0.88830537 -2.3718863   3.6929164   0.17536415]]\n",
      "\n",
      "  [[-0.20863976 -0.27273425  0.42647386  0.07348019]\n",
      "   [ 0.59413576 -7.0776234   0.3937803   0.7662048 ]\n",
      "   [ 0.96936405  1.54816    -5.38302    -2.581315  ]\n",
      "   [ 0.38710782 -1.8460459   0.8744281  -2.146471  ]]\n",
      "\n",
      "  [[-0.15726982 -0.16104104  0.07374533  1.040318  ]\n",
      "   [-0.19427125 -5.400611    2.8342748   0.9576878 ]\n",
      "   [ 0.9997108  -1.4077754   0.58703816 -2.564867  ]\n",
      "   [ 0.04265315  4.068154    0.62130535 -1.1917355 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.64339346 -3.427214   -1.9332778  -1.831178   -2.9725688\n",
      "    -3.3511758  -0.33726037  0.65105456]\n",
      "   [-0.9194904   0.8968038   0.45222297  1.0594327   1.1557782\n",
      "    -0.8956241  -0.17587543 -0.8396095 ]\n",
      "   [-0.04047573 -0.15472572 -0.69453156  0.27039978 -0.9049967\n",
      "     0.7182969  -0.13295098 -0.00661357]\n",
      "   [ 0.40479198 -0.3806878  -0.4154958  -0.36258787 -1.1813488\n",
      "     0.5046691   0.05306904  0.42328048]]\n",
      "\n",
      "  [[ 1.1482323  -9.236041   -8.218754   -6.717816   -9.638861\n",
      "    -8.856656   -2.9334142   1.2082323 ]\n",
      "   [-1.8857075   3.473417    2.5907042   3.5434728   3.9081209\n",
      "     5.844051    1.0954899  -1.8498287 ]\n",
      "   [ 0.47446606 -0.28332782  0.63271886 -0.17188838  0.33722964\n",
      "     0.37442216  0.03080073  0.46110418]\n",
      "   [ 1.8575145   0.71112     1.3011446   1.2523906  -0.17590559\n",
      "    -1.8068957   0.19210525  1.7457882 ]]\n",
      "\n",
      "  [[-1.0731127  -1.8999621  -2.2802618  -2.3014586  -1.5626087\n",
      "    -1.5433154  -1.408609   -1.0031282 ]\n",
      "   [-0.15004301 -3.7747118  -4.124579   -3.467151   -3.0495815\n",
      "    -2.7865295  -1.1569786  -0.1172325 ]\n",
      "   [ 1.0226461   0.42154983  0.85200024  1.2949932  -0.23638739\n",
      "     1.2461828   0.683046    0.9681701 ]\n",
      "   [ 2.5858383  -3.6268303  -4.0738564  -2.3015351  -5.114497\n",
      "    -3.4861133   0.7499384   2.550022  ]]\n",
      "\n",
      "  [[-0.4604545  -1.5868427  -0.91173816 -0.8565646  -1.1848444\n",
      "    -2.0918827  -0.78210574 -0.41651735]\n",
      "   [-1.3551308   0.1230931   0.7364044  -0.6643719   0.8351439\n",
      "     0.06500757  0.78761655 -1.2645116 ]\n",
      "   [ 0.68831766  0.8079782  -0.10353167  1.935387    0.18036631\n",
      "     2.0298638  -0.18135574  0.6273567 ]\n",
      "   [ 1.9218479  -8.242031   -7.458511   -5.4300194  -7.086632\n",
      "    -7.069091   -2.0435832   1.9204775 ]]\n",
      "\n",
      "  [[-1.5332501   4.991253    6.4686904   4.0847487   5.368916\n",
      "     4.911813    2.5922883  -1.523277  ]\n",
      "   [-1.8101153   6.2314014   5.68846     4.518671    6.62946\n",
      "     6.433606    3.0988286  -1.8167404 ]\n",
      "   [ 1.1217315  -4.2282114  -4.6459684  -2.7598808  -3.6334445\n",
      "    -3.9288173  -2.1453893   1.1145377 ]\n",
      "   [ 3.112364   -8.154078   -6.9861646  -4.1005797  -7.100312\n",
      "    -6.5753813  -2.6710312   3.1095238 ]]\n",
      "\n",
      "  [[ 1.6350883  -6.317388   -5.912019   -3.864096   -6.084104\n",
      "    -5.8046737  -1.3961531   1.6473184 ]\n",
      "   [ 0.24405284  1.462353    0.9224829   1.581335    1.9375173\n",
      "     2.096835    2.0256872   0.28618026]\n",
      "   [ 1.4571862  -8.192412   -7.3626842  -4.9873695  -7.7452354\n",
      "    -6.7284403  -1.935825    1.4636596 ]\n",
      "   [ 1.2628276  -5.2162313  -6.7320704  -3.7959094  -6.008326\n",
      "    -4.775027   -1.3642161   1.3028276 ]]\n",
      "\n",
      "  [[ 0.12968476 -2.7599597  -3.3458984  -0.6776076  -1.3498564\n",
      "    -2.4725385  -0.26429352  0.143075  ]\n",
      "   [ 0.16608208 -3.8964067  -3.9270244  -2.1397204  -2.7393942\n",
      "    -2.4727707  -0.7227451   0.23026752]\n",
      "   [ 0.2666929   0.5002075   0.8165109   1.2050005   1.1429677\n",
      "     2.0832279   1.431956    0.24901326]\n",
      "   [ 1.9745605  -5.4055204  -6.2844653  -4.0218945  -6.1748066\n",
      "    -4.6382313  -1.3935999   1.9711884 ]]\n",
      "\n",
      "  [[-0.48440975 -0.30788445 -1.4893405  -0.5263086   0.2881688\n",
      "    -0.46770203 -0.27271533 -0.46877885]\n",
      "   [-0.3127084  -1.174712   -0.8735866  -0.32615614 -0.25206855\n",
      "    -0.55387217  0.23482844 -0.2958877 ]\n",
      "   [ 0.75498635  1.107361    1.7389252   1.5763456   1.1201576\n",
      "     1.3887876   0.36387968  0.70142406]\n",
      "   [ 1.9517523  -3.5100362  -4.6243114  -2.928465   -5.088273\n",
      "    -4.245649   -1.9016756   1.927332  ]]]], shape=(1, 8, 4, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 8)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.8282402  -0.42404926 -0.8946604  -0.24211022]\n",
      "   [-1.2308896   2.5182536   0.9160354  -0.32361427]\n",
      "   [ 1.7116143   0.36283326 -0.38897648 -4.488289  ]\n",
      "   [ 1.2025449  -1.5277051  -0.21148735 -2.888625  ]]\n",
      "\n",
      "  [[ 0.05669579 -0.14652377  0.7225302  -0.12972002]\n",
      "   [-1.3575498   1.1293744  -1.4357362   0.78531826]\n",
      "   [ 3.1011949   1.2757022  -1.3072673  -3.8735275 ]\n",
      "   [ 0.23908208  0.42009813 -1.7638853   0.09815715]]\n",
      "\n",
      "  [[-3.0994341   3.915559    3.6157894   1.844174  ]\n",
      "   [-6.1072607  -1.5910499   4.307164    3.4624133 ]\n",
      "   [-1.8124647   3.9855342   1.44573     0.20429029]\n",
      "   [-0.36083025 -3.97918    -1.1556294   0.4108099 ]]\n",
      "\n",
      "  [[-2.9529114   2.299974    3.0738797   3.844925  ]\n",
      "   [-7.031095   -0.24240322  8.241785    9.95658   ]\n",
      "   [-4.861674   -1.6940056   2.8614326   6.290135  ]\n",
      "   [-1.8445894  -2.6421747   0.42038977  3.2298315 ]]\n",
      "\n",
      "  [[-0.5371959   0.03175565  0.72881126  0.9492595 ]\n",
      "   [-1.4198313  -6.4444065  -0.92142916  2.328551  ]\n",
      "   [ 2.8078191   2.1120253  -2.642926   -6.017806  ]\n",
      "   [ 1.2005723  -0.03687301  0.87963223 -2.9841392 ]]\n",
      "\n",
      "  [[-1.412344    1.717014   -0.30293384 -0.56581223]\n",
      "   [-1.9951228  -3.2546551   1.0123291  -0.44657713]\n",
      "   [ 0.73561543  0.3251526  -3.0940294   0.9392657 ]\n",
      "   [ 1.3327664  -0.05496131 -0.6325569  -0.44301042]]\n",
      "\n",
      "  [[-2.4360008   0.67301464 -0.9478727   1.727219  ]\n",
      "   [-0.27072212 -7.063433    1.4467108   3.1845582 ]\n",
      "   [ 3.4422889  -1.1130698  -3.4816942  -3.4157388 ]\n",
      "   [ 1.1087734   0.70760036 -1.7606698  -2.657325  ]]\n",
      "\n",
      "  [[-1.929139    0.8935091   1.5377133   1.4501257 ]\n",
      "   [-1.0276425  -4.9508924   3.0305479   1.7626755 ]\n",
      "   [ 0.29604688 -1.3716084  -0.20351243  1.1358527 ]\n",
      "   [ 0.7165042  -1.7467748  -0.3651124  -0.20605943]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.3973298    3.2900124    5.064679     2.9755645    4.015015\n",
      "      4.0069337   -0.21337411  -0.49866062]\n",
      "   [ -0.33577263   4.7211685    5.425065     3.9286716    3.8147593\n",
      "      3.5898767    0.9110762   -0.36344096]\n",
      "   [ -0.05790175   0.3653249   -0.63628477  -0.08784539  -0.41260353\n",
      "     -1.178433     0.6327939   -0.04360336]\n",
      "   [  0.6763596   -3.6069698   -5.242045    -2.8647668   -4.1328135\n",
      "     -4.5069427   -0.0389813    0.7387742 ]]\n",
      "\n",
      "  [[ -0.6408605    3.2326546    3.591694     2.0563188    3.0459878\n",
      "      4.4082403   -0.3960128   -0.63534915]\n",
      "   [ -1.1932893    5.5270476    5.752577     4.0630784    4.8154626\n",
      "      5.298501     1.4706326   -1.2358047 ]\n",
      "   [ -0.52651423   0.23078535   0.45221442   1.5162919    0.8854996\n",
      "      0.6778835    1.459687    -0.5176164 ]\n",
      "   [  0.6784578   -3.6849337   -3.4605443   -1.5862267   -2.9143507\n",
      "     -4.225469     0.56281316   0.6827177 ]]\n",
      "\n",
      "  [[ -0.6057757    1.7133442    2.8641703    0.5958629    2.7594314\n",
      "      1.3030896   -0.8051967   -0.6426489 ]\n",
      "   [ -0.22311665  -8.5990305  -10.39877     -8.644732   -10.273176\n",
      "     -8.879024    -2.734983    -0.12058785]\n",
      "   [  0.5510202   -5.272624    -5.8298817   -4.458143    -6.6167383\n",
      "     -5.437187    -0.91685784   0.5980707 ]\n",
      "   [  1.7031115   -3.4144928   -4.1628165   -2.5063388   -5.1732554\n",
      "     -4.233163    -0.27847725   1.7545633 ]]\n",
      "\n",
      "  [[ -1.699932     4.244729     6.19011      3.755602     5.4355664\n",
      "      6.699038    -0.3490838   -1.7462924 ]\n",
      "   [  1.4002923    0.54548395  -1.1169968    0.37443233  -1.1300082\n",
      "     -1.9262465    2.2690759    1.3882834 ]\n",
      "   [  1.6556793   -4.4972763   -5.2453957   -3.274147    -4.7581916\n",
      "     -5.8438854    1.4371101    1.7192887 ]\n",
      "   [  2.8702378   -4.0238047   -4.879114    -2.2400463   -4.472778\n",
      "     -6.184055     1.650003     2.8772376 ]]\n",
      "\n",
      "  [[ -1.2787998    2.4755638    3.703323     1.3851306    3.2478092\n",
      "      3.6466854    0.553544    -1.0989556 ]\n",
      "   [ -0.68938744   0.7849126    0.80624384  -0.06641847   0.84722805\n",
      "      1.4088807    0.84816337  -0.6531029 ]\n",
      "   [  0.3542869   -2.6495297   -3.9148326   -2.3332536   -4.0371146\n",
      "     -2.1969824   -0.3534847    0.32806468]\n",
      "   [  1.7841424   -3.3586555   -5.717826    -3.0791197   -6.833018\n",
      "     -4.7764482   -0.9208296    1.659839  ]]\n",
      "\n",
      "  [[ -0.6709214    1.5035548    1.64232     -0.33530334   0.9649067\n",
      "      0.04905168  -0.89175946  -0.71355253]\n",
      "   [  0.24583115   0.6569769   -0.621118     1.2669846    0.91026103\n",
      "      1.4697332    1.5297475    0.26090285]\n",
      "   [  0.9396693    1.0186071    0.9804496    3.2190533    1.4295805\n",
      "      2.6770215    2.6240253    0.95563656]\n",
      "   [  0.6117326   -0.6111772   -0.8288269    0.2871933   -0.67439234\n",
      "     -0.41122133   0.9504747    0.6410048 ]]\n",
      "\n",
      "  [[ -0.5303791   -2.110146    -1.3960739   -2.3525817   -1.9407862\n",
      "     -0.8966946   -2.0528047   -0.50565517]\n",
      "   [ -0.09481095  -1.1549708    0.31408975  -0.03525389  -0.4600581\n",
      "     -1.6961929    0.7154125   -0.0880874 ]\n",
      "   [  0.5121612   -0.1614938    0.4048692    0.8203044    0.5385774\n",
      "     -0.67759335   1.620004     0.4480935 ]\n",
      "   [  0.47279775   0.89751595   0.56361896   1.2335601    0.97617674\n",
      "      0.72556263   0.7255807    0.45003852]]\n",
      "\n",
      "  [[ -1.6133933    5.655559     8.456806     4.720569     6.469103\n",
      "      6.3866005   -0.77200097  -1.6862562 ]\n",
      "   [  2.6422656   -3.0630863   -6.9324884   -2.84115     -4.448012\n",
      "     -5.5646486    1.8054792    2.638062  ]\n",
      "   [  2.5069184   -6.87229    -10.355062    -4.8297977   -8.231734\n",
      "     -8.594013     0.42899096   2.5198808 ]\n",
      "   [  2.0609508   -6.442012    -9.504503    -4.5295987   -6.941006\n",
      "     -7.39375      0.4449742    2.1119938 ]]]], shape=(1, 8, 4, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 8)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724    3.119958     0.8104701   -3.7867832   -0.53479564\n",
      "     -0.68400717  -2.684907    -4.306619  ]\n",
      "   [ -2.3828275    6.9414687    0.72609663  -5.475966    -3.1419044\n",
      "     -4.861004    -1.5760975   -1.7812909 ]\n",
      "   [ -9.4575205    9.2924795   -0.14664039  -7.7748985    0.3141518\n",
      "     -4.7069764   -3.4037216   -7.8661847 ]\n",
      "   [-10.166894    14.483014     3.299315   -11.655867    -6.542652\n",
      "     -5.2036433   -7.0711117   -9.239806  ]\n",
      "   [ -7.9254494   12.0516       3.657774   -14.190444    -6.1069603\n",
      "     -6.00821     -5.337775    -6.7280397 ]\n",
      "   [ -9.514821    12.602872     2.7181993  -11.5606165   -3.807155\n",
      "     -4.5184774   -7.4839892   -9.5418    ]\n",
      "   [ -6.294987     8.895258     0.31260866  -7.041161    -2.2837403\n",
      "     -4.8525386   -4.4346924   -5.9981575 ]\n",
      "   [ -1.627529     0.2572786    0.33721286  -1.0157803   -0.02855057\n",
      "      0.4145152   -2.0903144   -3.1125855 ]]\n",
      "\n",
      "  [[ -5.830585     2.9278567    6.4656816    2.9012501    0.7074189\n",
      "      1.8678039   -0.98469496  -5.710069  ]\n",
      "   [ -0.888771    -1.5304633   -0.92677224   7.0835996   -2.1796093\n",
      "      3.6029007   -0.78117675  -0.31565633]\n",
      "   [ -5.1757755   -0.00269019   5.1771684    2.4006374    1.5459608\n",
      "      4.5166783   -1.3455517   -7.2344646 ]\n",
      "   [ -0.32639125   1.338475     6.20836     -2.123073     1.0238662\n",
      "      0.95025975  -0.588602    -0.29866517]\n",
      "   [ -3.7669215    0.96661407   6.3918867   -2.0059547   -1.667028\n",
      "      2.4628794   -0.9746783   -5.5741014 ]\n",
      "   [ -5.9424763    3.3478012    5.0460277    5.0597334    1.5781783\n",
      "      5.766434    -3.736297    -6.4208727 ]\n",
      "   [ -5.028427     2.1733994    8.720076    -5.247528     2.0949328\n",
      "      0.3581393   -1.2882515   -6.517891  ]\n",
      "   [ -4.8532395    3.40758      5.827717     0.8119974    1.7706672\n",
      "      1.3585875   -2.134433    -4.6770263 ]]\n",
      "\n",
      "  [[  0.21261294   2.1825705    2.6290977   -0.7738304   -0.92338926\n",
      "      1.3111119   -1.7676708   -1.2661753 ]\n",
      "   [ -7.103699    -0.08641089   0.6366692   -1.1594403    0.52412534\n",
      "     -1.2009193   -3.281907    -9.214137  ]\n",
      "   [ -7.0138736    4.6630197   -1.6835009    0.64012516  -0.3236557\n",
      "      6.017604    -2.2194111   -8.46463   ]\n",
      "   [ -5.658084     3.870231     1.6583294   -3.8960452   -0.90947014\n",
      "     -1.150247     0.02895866  -8.4907255 ]\n",
      "   [ -2.1916983    2.1456993   -0.696395     2.0237832    0.4654629\n",
      "      2.97788     -1.8672262   -3.7927125 ]\n",
      "   [ -4.237206     0.8983024   -1.8900397   -1.5124333   -1.4959668\n",
      "     -3.0116844   -4.190044    -5.1174064 ]\n",
      "   [  0.44389454   0.6015544    1.0924258   -0.6734957   -0.14239292\n",
      "     -0.31658053  -3.1918075   -2.5049903 ]\n",
      "   [ -0.5491213    1.3170428    2.416191    -0.72504455   0.17214571\n",
      "      0.6214916   -0.7466009   -2.0425894 ]]\n",
      "\n",
      "  [[ -0.62310183   1.8474774   -0.70651436  -1.8744931    1.9892807\n",
      "      1.1960789    0.93903846  -0.96611404]\n",
      "   [ -2.1139884   -0.40789947   1.2968414   -2.4691556   -0.6430844\n",
      "      2.9704242   -0.49229723  -3.2126625 ]\n",
      "   [ -4.681421     1.071365     7.057754     1.6925517    2.925461\n",
      "      1.2117237   -2.5487785   -7.727114  ]\n",
      "   [ -5.2494354    2.6016214    4.7093654   -2.9185028   11.68196\n",
      "      5.0173087    0.2649115   -6.465863  ]\n",
      "   [ -4.187672     2.2745495    2.1844132   -2.950158     0.4811062\n",
      "      2.5944564   -1.2815626   -5.458976  ]\n",
      "   [ -5.1851773    3.6045432    5.879984    -1.5479906    9.332985\n",
      "      0.7532006   -6.1630864  -10.190875  ]\n",
      "   [ -1.4530067    3.602885     1.8737535   -1.0183158    6.639346\n",
      "     -0.45745188  -3.0397322   -5.1571107 ]\n",
      "   [ -1.3701262    1.7800907    0.4440075   -2.7584684    0.16184288\n",
      "      1.0718383    0.74469614  -2.4832382 ]]\n",
      "\n",
      "  [[ -3.365542     1.7112432   -0.18042703  -0.56367624  -1.0182812\n",
      "      0.91838205   0.3576792   -1.8675333 ]\n",
      "   [ -4.8425922   -4.2636166   -2.6981843    0.22403693   0.68739414\n",
      "     -3.1009803    2.71456     -1.489905  ]\n",
      "   [ -5.1876755    5.671311    -1.2768452   -2.9591484   -0.39675456\n",
      "     -3.3887212    1.0478692   -2.3474293 ]\n",
      "   [ -2.170307     3.4666238    1.0260042   -0.8940675    2.2694418\n",
      "     -2.5140228    1.5545614   -1.9796547 ]\n",
      "   [ -7.992303     0.4727184   -0.13311552  -3.517133    -2.5849926\n",
      "     -4.228556    -1.0881504   -5.6788545 ]\n",
      "   [ -3.6134825    0.19133087   0.298861    -4.0497766   -2.2160487\n",
      "     -3.5523865    0.53147453  -4.259141  ]\n",
      "   [ -2.6600354    4.010139     4.72034     -3.9068446   -1.4782856\n",
      "     -0.05746605  -5.8499537   -1.9302822 ]\n",
      "   [  2.192343    -2.0631888   -0.458438     0.3939276    0.19301154\n",
      "      3.2884934   -1.1612332    1.5978622 ]]\n",
      "\n",
      "  [[ -2.1686854   -0.68126124   1.8873476   -1.0016267    1.665862\n",
      "     -0.39142966  -2.5463562   -2.0510204 ]\n",
      "   [ -3.4723454   -2.521675     3.3836114   -0.29911357   3.7395344\n",
      "      2.2255218   -0.5534243   -3.5096362 ]\n",
      "   [ -1.241272    -1.0362755    0.8233798    0.13532306   0.49612203\n",
      "      2.9168866   -0.06637239   0.07041088]\n",
      "   [  0.16238996  -2.266713    -0.43468216  -1.9490578   -0.7804693\n",
      "     -2.2501192    0.14404878   0.669509  ]\n",
      "   [ -4.5097795    0.04322206   2.9916728    4.4943275    1.4705126\n",
      "     -0.8136589   -2.140882    -5.1061444 ]\n",
      "   [ -6.6359534    3.4645762    4.6767483   -2.6971583    5.242417\n",
      "      3.411301     1.1889111   -5.1261134 ]\n",
      "   [ -2.4793563    1.550663     3.5571485   -3.2087276    2.422179\n",
      "      0.22151406  -3.0109324   -3.599103  ]\n",
      "   [ -1.2139645   -1.8291416    2.1708345   -1.4788015    1.7007154\n",
      "     -1.5888573   -2.220674    -2.1623607 ]]\n",
      "\n",
      "  [[ -0.3049526    1.6942294   -0.7551851   -0.20310196  -1.9716048\n",
      "      0.50340194   0.2921721   -1.8446437 ]\n",
      "   [ -3.5450757    9.210564     3.1409702   -1.4201491   -8.377774\n",
      "     -2.9744062   -6.1630034   -6.277001  ]\n",
      "   [-10.783727     9.743        1.6049056   -7.786639    -9.65972\n",
      "     -3.0615754   -6.16684     -9.820524  ]\n",
      "   [-10.1322155    4.096334     1.2889549   -8.946105    -7.003514\n",
      "     -2.119969    -5.464064    -9.032952  ]\n",
      "   [ -6.2441616    5.4699683    0.8303648   -2.5861697   -6.0254755\n",
      "     -5.8178663   -3.9932399   -6.5110664 ]\n",
      "   [ -7.973193     7.724517     4.6691766   -4.006742    -8.032589\n",
      "     -1.6155707   -8.793758    -7.380326  ]\n",
      "   [ -5.8270884    2.8298204    4.595993    -2.1421301   -1.5096639\n",
      "     -4.502131    -9.880545    -6.981122  ]\n",
      "   [ -1.5424074   -0.03381129   0.8708008    0.84780496  -2.2531416\n",
      "     -0.00659109   0.6038286   -2.874726  ]]\n",
      "\n",
      "  [[ -2.8292158    1.3493172   -1.1630216   -2.1086867   -0.7928409\n",
      "     -1.6628647    0.936982     0.17500922]\n",
      "   [ -4.8446875    2.2061067    5.2808175   -7.194037     1.36124\n",
      "      0.13124909  -5.7810917   -5.9226546 ]\n",
      "   [ -5.9484262    1.2807072    4.3257394   -5.866291    -1.1549981\n",
      "      0.3883991   -5.670845    -4.975041  ]\n",
      "   [ -4.245631     5.4597836    3.617752    -7.2080035   -3.3983228\n",
      "     -4.2374063   -2.325605    -2.7597272 ]\n",
      "   [ -3.7233293    5.6208553    0.9434773   -6.0832963   -2.91809\n",
      "     -0.02632309  -1.9356793   -0.92453605]\n",
      "   [ -2.065824     5.411931     2.41236     -4.3067594    2.3385603\n",
      "     -1.8093777   -4.6885967   -1.7742286 ]\n",
      "   [ -3.8498166    5.2004757   -1.0518612   -7.7332063   -0.27363002\n",
      "      0.8425215   -2.1548126   -2.9958065 ]\n",
      "   [ -1.0712852    3.3348348    0.1982265   -2.701739     0.3845683\n",
      "     -1.0062916    2.1746593    0.6345082 ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.108165     2.77802      7.04615      2.6454995    5.4191833\n",
      "      5.2317686   -5.6294155   -9.130048  ]\n",
      "   [-10.735054     1.7547116    0.76175237  -0.42852682   4.004292\n",
      "      2.5349874   -3.68677    -13.101013  ]\n",
      "   [ -7.5400214    0.57103527  -0.5523173    0.4326064    1.1318936\n",
      "      0.61845857  -3.0292842   -9.038257  ]\n",
      "   [-13.195052     3.5323567    3.9254384    3.0674067    4.645694\n",
      "      4.6829467   -5.6105638  -17.256235  ]\n",
      "   [-10.567291     1.5221579    0.52586746   1.5666054    3.392625\n",
      "      3.7933207   -3.6424325  -13.40815   ]\n",
      "   [-11.348783     1.1702118    0.33376536   1.9208177    4.171567\n",
      "      3.2230349   -4.625526   -14.001499  ]\n",
      "   [-13.822072     4.6995225    6.882736     4.5583987    9.456506\n",
      "      8.984521    -7.40026    -19.019571  ]\n",
      "   [ -4.276782     2.2549555    6.41255      2.478643     5.0064325\n",
      "      5.4905148   -5.49556     -7.172459  ]]\n",
      "\n",
      "  [[ -2.6697712    0.18644784  -3.2964184   -9.0699005   -5.8869786\n",
      "     -8.670129    -1.7457955   -2.4067173 ]\n",
      "   [ -4.892798     5.1958284    0.86651105  -1.7851338   -0.5991809\n",
      "      1.0769178   -6.025225    -5.985986  ]\n",
      "   [ -9.587597     3.4813366   -1.9858788   -0.24309416   0.67243534\n",
      "      4.1456385   -5.7291985  -11.872933  ]\n",
      "   [-10.449494     4.90856     -2.7020588   -1.3338774    0.16304302\n",
      "      4.3895545   -7.198788   -13.19233   ]\n",
      "   [ -6.8310695    3.5543683   -0.808639     0.5658518    1.5122908\n",
      "      3.5409427   -5.93916     -8.560726  ]\n",
      "   [-10.439719     3.3256927   -2.8369732   -1.1681513    0.17411843\n",
      "      4.7924056   -7.1944017  -12.024714  ]\n",
      "   [ -4.1172786    3.8512855   -1.755627    -3.863435    -1.9191699\n",
      "     -2.5844417   -4.672864    -4.7063437 ]\n",
      "   [ -1.7964184   -0.00925941  -3.5144498   -9.731884    -6.624717\n",
      "    -10.108276    -0.76045424  -0.7916409 ]]\n",
      "\n",
      "  [[ -4.7987876    2.3515558   -2.4956558    3.970095     6.9596395\n",
      "      6.1769433    2.5710814   -5.3403516 ]\n",
      "   [ -6.3391857    1.296634    -1.2071677   -2.4126644   -0.8230457\n",
      "      1.5592674   -4.067092    -6.657909  ]\n",
      "   [ -9.1259985    8.161333     4.880271    -1.283182     4.1173687\n",
      "      4.9365716   -5.1432853  -11.959505  ]\n",
      "   [-10.686145     8.565781     5.08118      0.27082056   5.3818784\n",
      "      6.742503    -2.9075274  -13.400082  ]\n",
      "   [ -9.507849    10.487074     6.0747166   -1.3516918    4.202924\n",
      "      5.809277    -3.300832   -12.412424  ]\n",
      "   [ -9.160462    10.127481     7.924963     1.2205707    6.1613603\n",
      "      7.179847    -3.3297603  -12.610715  ]\n",
      "   [ -3.074413    11.459646     8.69444      8.782502    13.618378\n",
      "      9.073093     3.539803    -6.4021416 ]\n",
      "   [ -2.5498397    2.3677871   -2.3312814    5.376789     7.2629266\n",
      "      5.222065     4.6978946   -2.8941624 ]]\n",
      "\n",
      "  [[  5.7272882    2.8488104   -2.2674716    2.4710479    1.0492487\n",
      "     -1.1509322    1.7533069    4.0113854 ]\n",
      "   [ -6.0593033   -0.8574972   -0.14114377   2.036406     0.46856305\n",
      "      3.5360727    1.1927096   -6.1919413 ]\n",
      "   [ -3.0789824   -0.10847238   1.995033     3.6496744    3.8057036\n",
      "      6.084562     4.08223     -3.2766128 ]\n",
      "   [ -4.919659     1.601183     1.003934     3.2019112    0.6709702\n",
      "      4.7461705    1.0749629   -5.1771154 ]\n",
      "   [ -6.994366     0.38075814  -0.8496923    2.2005079    2.1127446\n",
      "      4.8004184    1.2863806   -7.113053  ]\n",
      "   [ -1.9637377    2.5683565    0.11696774   0.11398675  -0.02081025\n",
      "      1.7587206   -0.9755858   -2.719708  ]\n",
      "   [ -4.988607     7.0706754    1.6426951    7.2739515    7.1590147\n",
      "      9.629911     6.3608046   -7.190774  ]\n",
      "   [  7.1238737    5.775128    -1.2283382    3.9956207    3.3504624\n",
      "      0.41485792   3.2472675    4.7716117 ]]\n",
      "\n",
      "  [[ 15.227098    -7.6481614   -6.1704655   -2.9068828   -4.9539256\n",
      "     -6.9440393   -5.5164175   14.977688  ]\n",
      "   [ -9.1538725    2.5506845    3.8603075   -0.97211933  -0.7266477\n",
      "      1.0761876    4.012439    -8.621349  ]\n",
      "   [ -7.3554325    3.0492685    4.882066     1.9635559    1.5533912\n",
      "      2.5195072    5.0332184   -6.660119  ]\n",
      "   [ -7.502227     4.479796     3.8863442    0.8529868    2.5325358\n",
      "      2.6109834    3.7704332   -7.242996  ]\n",
      "   [-14.381295     7.1894965    6.8530087    0.8468516    1.6221398\n",
      "      3.4500875    6.026878   -14.084528  ]\n",
      "   [-12.931715     6.7301073    6.57782      0.15552036   0.9927032\n",
      "      0.9647971    3.3784504  -12.61685   ]\n",
      "   [  0.44247183  -0.69109786  -1.4392438   -1.959099    -3.2892675\n",
      "     -3.1996653   -2.5787928   -0.35673866]\n",
      "   [ 16.702045    -7.9659853   -7.5883594   -3.50606     -4.3180175\n",
      "     -7.998839    -8.3601055   15.943729  ]]\n",
      "\n",
      "  [[ 16.92651     -1.8878899    4.612371     8.297861     8.785893\n",
      "     10.494433    13.696043    18.756512  ]\n",
      "   [ -4.8187237    0.3554866   -0.10856649  -0.7664691   -0.8179527\n",
      "     -2.149825    -1.7334187   -4.167079  ]\n",
      "   [ -9.965244     1.1075305   -3.9921682   -5.4180512   -3.1439044\n",
      "     -9.256229    -8.250524   -10.748956  ]\n",
      "   [ -7.730639     0.702649    -3.5076466   -2.8937106   -1.1201787\n",
      "     -2.574378    -2.7975821   -7.5726776 ]\n",
      "   [-10.1452055   -1.0527366   -4.0010457   -2.8202963   -3.0494757\n",
      "     -5.4484124   -5.228744   -10.110497  ]\n",
      "   [-11.985994     3.1561482   -2.7560956   -3.3319776   -1.583422\n",
      "     -4.690591    -5.913287   -12.567582  ]\n",
      "   [ -5.3008895   -3.0260174   -2.1208181   -1.0755001   -2.3386164\n",
      "     -0.51985437  -1.1730318   -4.255245  ]\n",
      "   [ 17.86222     -1.6225508    5.1695466    7.7438717    8.076046\n",
      "     10.810771    13.726412    19.605438  ]]\n",
      "\n",
      "  [[ -5.9784994   -2.1169765    4.562182     0.5942515    3.0359569\n",
      "      1.3266702   -0.49234566  -3.304644  ]\n",
      "   [-12.7146845    2.0409193    2.1862695   -0.09457824   1.4637623\n",
      "      2.2060208   -4.997251   -13.3957815 ]\n",
      "   [-15.493677     3.6968644    4.4985094    1.5548421    1.8590727\n",
      "      4.2570806   -6.917543   -16.565073  ]\n",
      "   [ -9.458374    -0.38232473   3.5684469    0.3583388    1.3042445\n",
      "      1.8564001   -4.3985405   -9.246963  ]\n",
      "   [-12.0117655    2.1870446    3.3004608    0.608759     0.99599266\n",
      "      1.7472012   -5.940463   -13.297273  ]\n",
      "   [ -9.835355    -0.9845663   -0.7273823   -2.8138676   -0.3688764\n",
      "      0.66789573  -4.370575    -9.257524  ]\n",
      "   [ -3.3896937   -3.072007     1.3647641    1.7215154    1.1578857\n",
      "      2.4925263    3.2728026   -0.55832624]\n",
      "   [ -2.0235658   -2.5166588    4.0697465    1.5893646    3.5398774\n",
      "      1.8933309    2.816157     1.3247585 ]]\n",
      "\n",
      "  [[ 13.876768     5.289463     2.8875844    5.729185    -0.31207085\n",
      "      2.216713     0.970955    11.02701   ]\n",
      "   [-11.855931     7.044625     2.757176    -2.0767522    9.620936\n",
      "      3.4307175   -0.20549646 -11.727619  ]\n",
      "   [-17.164427     9.00703      2.9232194   -2.114338    10.674683\n",
      "      4.7908983    0.6977465  -17.203806  ]\n",
      "   [ -6.0810537    6.5570908    3.815859     1.4116415    7.24501\n",
      "      5.5307817    2.8692966   -6.8714347 ]\n",
      "   [ -9.309462     5.957898     3.5396616   -0.5519851    6.9079504\n",
      "      5.2796082    1.4829706   -9.929194  ]\n",
      "   [ -9.980593     6.659394     3.6414435   -1.2484376    6.108645\n",
      "      4.148278     2.2938857  -10.687042  ]\n",
      "   [-11.461593     8.152552     3.527002    -0.32889172   5.8540664\n",
      "      4.3797064    2.374406   -12.23368   ]\n",
      "   [ 16.641361     4.062904     2.288843     5.4982553   -3.302351\n",
      "      0.8199426    0.47605655  13.646871  ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.09299298  0.60130334  1.3376766   1.1430501 ]\n",
      "   [-0.18269673 -4.40855     0.07642998  2.0146773   0.21539456]\n",
      "   [-0.11618186  3.2863328  -3.025875    0.4890776   3.37132   ]\n",
      "   [ 1.0314696  -1.1489769  -4.595038   -7.4570365  -6.3443    ]\n",
      "   [ 0.47551402 -2.962047    0.34962878 -1.2729536   1.1051896 ]]\n",
      "\n",
      "  [[-0.1123142   0.56263393  0.21921782 -0.1352104  -0.02665874]\n",
      "   [-0.29892173 -1.5473536   1.286812    2.3739398   5.4393287 ]\n",
      "   [-0.25255403  2.1486242  -6.0938      3.577338    6.605463  ]\n",
      "   [ 0.24040057 -0.7833131  -1.2749462   2.9239094  -3.3049998 ]\n",
      "   [ 0.4180626  -2.545671   -0.2369275   3.2331958  -3.0744147 ]]\n",
      "\n",
      "  [[-0.10918517  0.11000926  0.21102953  0.4795525   0.34849948]\n",
      "   [ 0.07259987 -4.5046606  -4.6542673  -1.4462825  -0.4685369 ]\n",
      "   [-0.0773119  -0.45317498  3.3376467   3.8257363   5.9663725 ]\n",
      "   [ 0.6502231   2.718805    2.302118    3.527786    9.5337105 ]\n",
      "   [-0.32270402  1.7251928   2.3383963   5.20828     7.563763  ]]\n",
      "\n",
      "  [[-0.22114104 -0.48759893  0.03473653  1.0193444   0.8324323 ]\n",
      "   [ 0.67009145 -7.431193   -0.8688647  -3.4817095   0.6493854 ]\n",
      "   [-0.6334822   2.1947665  -1.3594295   3.5641603   2.0439782 ]\n",
      "   [ 0.43472287 -3.067627    0.94878364 -1.7823364   3.5336585 ]\n",
      "   [-0.21712674 -1.7631739  -0.9125511   6.9749928   8.10879   ]]\n",
      "\n",
      "  [[ 0.00076126  0.07451362  0.17366233 -0.0934004   0.05626196]\n",
      "   [-0.10380323 -2.2713957  -1.0344037  -0.31538382  2.3452058 ]\n",
      "   [ 0.00120817 -2.261606   -4.7580304   3.1489854  -0.6975868 ]\n",
      "   [ 0.92358315 -4.130864   -1.2742832   0.6084041  -2.5395427 ]\n",
      "   [ 0.62570703 -3.030466    1.548444    1.2197994   0.37899518]]\n",
      "\n",
      "  [[-0.00999826  0.21593446 -0.3890052   0.1919408   0.18444464]\n",
      "   [ 0.9415558  -2.968748   -1.1732452  -2.6510923  -2.8050153 ]\n",
      "   [-0.03670223 -0.73637694  3.4228125  -0.21042274  0.7946963 ]\n",
      "   [ 0.88830537 -2.3718863   3.6929164   0.17536415 -1.252368  ]\n",
      "   [ 0.72545135 -2.5605028  -0.0558291  -4.3498635  -4.4779325 ]]\n",
      "\n",
      "  [[-0.20863976 -0.27273425  0.42647386  0.07348019  0.62226117]\n",
      "   [ 0.59413576 -7.0776234   0.3937803   0.7662048  -2.2862425 ]\n",
      "   [ 0.96936405  1.54816    -5.38302    -2.581315   -3.2941782 ]\n",
      "   [ 0.38710782 -1.8460459   0.8744281  -2.146471   -1.8200386 ]\n",
      "   [ 0.8605784  -2.11317    -2.68866     0.36657253 -2.6509297 ]]\n",
      "\n",
      "  [[-0.15726982 -0.16104104  0.07374533  1.040318   -0.00393564]\n",
      "   [-0.19427125 -5.400611    2.8342748   0.9576878   2.3463671 ]\n",
      "   [ 0.9997108  -1.4077754   0.58703816 -2.564867    2.9698117 ]\n",
      "   [ 0.04265315  4.068154    0.62130535 -1.1917355  -1.5518103 ]\n",
      "   [ 0.3693564  -1.2260512  -0.9977996   0.86115277  3.8171647 ]]]], shape=(1, 8, 5, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 5)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.64339346  -3.427214    -1.9332778   -1.831178    -2.9725688\n",
      "     -3.3511758   -0.33726037   0.65105456]\n",
      "   [ -0.9194904    0.8968038    0.45222297   1.0594327    1.1557782\n",
      "     -0.8956241   -0.17587543  -0.8396095 ]\n",
      "   [ -0.04047573  -0.15472572  -0.69453156   0.27039978  -0.9049967\n",
      "      0.7182969   -0.13295098  -0.00661357]\n",
      "   [  0.40479198  -0.3806878   -0.4154958   -0.36258787  -1.1813488\n",
      "      0.5046691    0.05306904   0.42328048]\n",
      "   [  1.9060687   -5.0097375   -2.570788    -2.4334707   -5.5112567\n",
      "     -4.1143527   -0.6269515    1.843126  ]]\n",
      "\n",
      "  [[  1.1482323   -9.236041    -8.218754    -6.717816    -9.638861\n",
      "     -8.856656    -2.9334142    1.2082323 ]\n",
      "   [ -1.8857075    3.473417     2.5907042    3.5434728    3.9081209\n",
      "      5.844051     1.0954899   -1.8498287 ]\n",
      "   [  0.47446606  -0.28332782   0.63271886  -0.17188838   0.33722964\n",
      "      0.37442216   0.03080073   0.46110418]\n",
      "   [  1.8575145    0.71112      1.3011446    1.2523906   -0.17590559\n",
      "     -1.8068957    0.19210525   1.7457882 ]\n",
      "   [  3.592382    -6.1458387   -5.0730557   -4.1431313   -6.217651\n",
      "     -8.495154    -0.84755003   3.5301905 ]]\n",
      "\n",
      "  [[ -1.0731127   -1.8999621   -2.2802618   -2.3014586   -1.5626087\n",
      "     -1.5433154   -1.408609    -1.0031282 ]\n",
      "   [ -0.15004301  -3.7747118   -4.124579    -3.467151    -3.0495815\n",
      "     -2.7865295   -1.1569786   -0.1172325 ]\n",
      "   [  1.0226461    0.42154983   0.85200024   1.2949932   -0.23638739\n",
      "      1.2461828    0.683046     0.9681701 ]\n",
      "   [  2.5858383   -3.6268303   -4.0738564   -2.3015351   -5.114497\n",
      "     -3.4861133    0.7499384    2.550022  ]\n",
      "   [  3.4190211   -1.8762418   -2.089898    -0.12326887  -2.6943471\n",
      "     -1.6936332    2.811297     3.332227  ]]\n",
      "\n",
      "  [[ -0.4604545   -1.5868427   -0.91173816  -0.8565646   -1.1848444\n",
      "     -2.0918827   -0.78210574  -0.41651735]\n",
      "   [ -1.3551308    0.1230931    0.7364044   -0.6643719    0.8351439\n",
      "      0.06500757   0.78761655  -1.2645116 ]\n",
      "   [  0.68831766   0.8079782   -0.10353167   1.935387     0.18036631\n",
      "      2.0298638   -0.18135574   0.6273567 ]\n",
      "   [  1.9218479   -8.242031    -7.458511    -5.4300194   -7.086632\n",
      "     -7.069091    -2.0435832    1.9204775 ]\n",
      "   [  2.4111772   -7.0431905   -6.9708333   -4.6501546   -7.462153\n",
      "     -7.238265    -2.632736     2.3841848 ]]\n",
      "\n",
      "  [[ -1.5332501    4.991253     6.4686904    4.0847487    5.368916\n",
      "      4.911813     2.5922883   -1.523277  ]\n",
      "   [ -1.8101153    6.2314014    5.68846      4.518671     6.62946\n",
      "      6.433606     3.0988286   -1.8167404 ]\n",
      "   [  1.1217315   -4.2282114   -4.6459684   -2.7598808   -3.6334445\n",
      "     -3.9288173   -2.1453893    1.1145377 ]\n",
      "   [  3.112364    -8.154078    -6.9861646   -4.1005797   -7.100312\n",
      "     -6.5753813   -2.6710312    3.1095238 ]\n",
      "   [  4.4973893  -12.889609    -9.3284645   -5.5953784  -10.986841\n",
      "     -9.9575405   -4.63632      4.475674  ]]\n",
      "\n",
      "  [[  1.6350883   -6.317388    -5.912019    -3.864096    -6.084104\n",
      "     -5.8046737   -1.3961531    1.6473184 ]\n",
      "   [  0.24405284   1.462353     0.9224829    1.581335     1.9375173\n",
      "      2.096835     2.0256872    0.28618026]\n",
      "   [  1.4571862   -8.192412    -7.3626842   -4.9873695   -7.7452354\n",
      "     -6.7284403   -1.935825     1.4636596 ]\n",
      "   [  1.2628276   -5.2162313   -6.7320704   -3.7959094   -6.008326\n",
      "     -4.775027    -1.3642161    1.3028276 ]\n",
      "   [  1.667126    -5.457896    -5.923977    -3.5345056   -6.5892243\n",
      "     -5.3130007   -1.6549468    1.6527491 ]]\n",
      "\n",
      "  [[  0.12968476  -2.7599597   -3.3458984   -0.6776076   -1.3498564\n",
      "     -2.4725385   -0.26429352   0.143075  ]\n",
      "   [  0.16608208  -3.8964067   -3.9270244   -2.1397204   -2.7393942\n",
      "     -2.4727707   -0.7227451    0.23026752]\n",
      "   [  0.2666929    0.5002075    0.8165109    1.2050005    1.1429677\n",
      "      2.0832279    1.431956     0.24901326]\n",
      "   [  1.9745605   -5.4055204   -6.2844653   -4.0218945   -6.1748066\n",
      "     -4.6382313   -1.3935999    1.9711884 ]\n",
      "   [  2.574275    -6.499424    -7.990838    -5.2485123   -7.2974114\n",
      "     -6.4010496   -1.8392944    2.5551796 ]]\n",
      "\n",
      "  [[ -0.48440975  -0.30788445  -1.4893405   -0.5263086    0.2881688\n",
      "     -0.46770203  -0.27271533  -0.46877885]\n",
      "   [ -0.3127084   -1.174712    -0.8735866   -0.32615614  -0.25206855\n",
      "     -0.55387217   0.23482844  -0.2958877 ]\n",
      "   [  0.75498635   1.107361     1.7389252    1.5763456    1.1201576\n",
      "      1.3887876    0.36387968   0.70142406]\n",
      "   [  1.9517523   -3.5100362   -4.6243114   -2.928465    -5.088273\n",
      "     -4.245649    -1.9016756    1.927332  ]\n",
      "   [  1.7787431   -2.3995202   -1.9599189   -1.07149     -3.3473392\n",
      "     -2.1400583   -1.0688959    1.7242967 ]]]], shape=(1, 8, 5, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 8)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.8282402  -0.42404926 -0.8946604  -0.24211022  0.3486308 ]\n",
      "   [-1.2308896   2.5182536   0.9160354  -0.32361427 -5.502114  ]\n",
      "   [ 1.7116143   0.36283326 -0.38897648 -4.488289   -4.894553  ]\n",
      "   [ 1.2025449  -1.5277051  -0.21148735 -2.888625   -0.5083307 ]\n",
      "   [-2.268408   -6.8269224  -0.6192657   1.2520456  10.82034   ]]\n",
      "\n",
      "  [[ 0.05669579 -0.14652377  0.7225302  -0.12972002  0.762966  ]\n",
      "   [-1.3575498   1.1293744  -1.4357362   0.78531826  2.1577141 ]\n",
      "   [ 3.1011949   1.2757022  -1.3072673  -3.8735275  -1.1950318 ]\n",
      "   [ 0.23908208  0.42009813 -1.7638853   0.09815715  1.505165  ]\n",
      "   [-1.1174238  -1.1195521  -0.5070932   2.6752462   2.2329626 ]]\n",
      "\n",
      "  [[-3.0994341   3.915559    3.6157894   1.844174    1.2353401 ]\n",
      "   [-6.1072607  -1.5910499   4.307164    3.4624133   2.229136  ]\n",
      "   [-1.8124647   3.9855342   1.44573     0.20429029 -2.5603697 ]\n",
      "   [-0.36083025 -3.97918    -1.1556294   0.4108099   0.2031284 ]\n",
      "   [-2.4606414   0.03661868  3.556143    2.4729695   0.3735249 ]]\n",
      "\n",
      "  [[-2.9529114   2.299974    3.0738797   3.844925    1.4925915 ]\n",
      "   [-7.031095   -0.24240322  8.241785    9.95658     5.335687  ]\n",
      "   [-4.861674   -1.6940056   2.8614326   6.290135    5.342425  ]\n",
      "   [-1.8445894  -2.6421747   0.42038977  3.2298315   4.995947  ]\n",
      "   [ 1.6251752   6.5201087  -5.3137836  -7.2730436  -8.355499  ]]\n",
      "\n",
      "  [[-0.5371959   0.03175565  0.72881126  0.9492595  -0.60133326]\n",
      "   [-1.4198313  -6.4444065  -0.92142916  2.328551    0.99433136]\n",
      "   [ 2.8078191   2.1120253  -2.642926   -6.017806   -4.966147  ]\n",
      "   [ 1.2005723  -0.03687301  0.87963223 -2.9841392  -0.84754145]\n",
      "   [ 0.6997136  -1.0197824  -4.4537454  -2.2257385   0.37594527]]\n",
      "\n",
      "  [[-1.412344    1.717014   -0.30293384 -0.56581223 -0.25823367]\n",
      "   [-1.9951228  -3.2546551   1.0123291  -0.44657713  3.7238536 ]\n",
      "   [ 0.73561543  0.3251526  -3.0940294   0.9392657  -0.1000299 ]\n",
      "   [ 1.3327664  -0.05496131 -0.6325569  -0.44301042  1.7962822 ]\n",
      "   [-0.36015886 -3.758684    0.89078796  0.7360876   7.093279  ]]\n",
      "\n",
      "  [[-2.4360008   0.67301464 -0.9478727   1.727219    2.7742186 ]\n",
      "   [-0.27072212 -7.063433    1.4467108   3.1845582   1.4821701 ]\n",
      "   [ 3.4422889  -1.1130698  -3.4816942  -3.4157388  -4.736643  ]\n",
      "   [ 1.1087734   0.70760036 -1.7606698  -2.657325   -2.180934  ]\n",
      "   [ 1.2798668  -1.775176   -0.39098    -2.8660033  -4.073905  ]]\n",
      "\n",
      "  [[-1.929139    0.8935091   1.5377133   1.4501257  -1.6422881 ]\n",
      "   [-1.0276425  -4.9508924   3.0305479   1.7626755   0.71068406]\n",
      "   [ 0.29604688 -1.3716084  -0.20351243  1.1358527   1.8680996 ]\n",
      "   [ 0.7165042  -1.7467748  -0.3651124  -0.20605943  1.1415403 ]\n",
      "   [ 0.16737235 -3.0147772   0.42238253  0.69539714  0.11543402]]]], shape=(1, 8, 5, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 5)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.3973298    3.2900124    5.064679     2.9755645    4.015015\n",
      "      4.0069337   -0.21337411  -0.49866062]\n",
      "   [ -0.33577263   4.7211685    5.425065     3.9286716    3.8147593\n",
      "      3.5898767    0.9110762   -0.36344096]\n",
      "   [ -0.05790175   0.3653249   -0.63628477  -0.08784539  -0.41260353\n",
      "     -1.178433     0.6327939   -0.04360336]\n",
      "   [  0.6763596   -3.6069698   -5.242045    -2.8647668   -4.1328135\n",
      "     -4.5069427   -0.0389813    0.7387742 ]\n",
      "   [  1.0683483   -3.5492473   -5.8436317   -3.085351    -4.965308\n",
      "     -5.2194877   -0.71366364   1.1221966 ]]\n",
      "\n",
      "  [[ -0.6408605    3.2326546    3.591694     2.0563188    3.0459878\n",
      "      4.4082403   -0.3960128   -0.63534915]\n",
      "   [ -1.1932893    5.5270476    5.752577     4.0630784    4.8154626\n",
      "      5.298501     1.4706326   -1.2358047 ]\n",
      "   [ -0.52651423   0.23078535   0.45221442   1.5162919    0.8854996\n",
      "      0.6778835    1.459687    -0.5176164 ]\n",
      "   [  0.6784578   -3.6849337   -3.4605443   -1.5862267   -2.9143507\n",
      "     -4.225469     0.56281316   0.6827177 ]\n",
      "   [  0.88377523  -6.1549253   -4.9009666   -3.4213269   -4.780138\n",
      "     -5.463376    -1.2016573    0.9243931 ]]\n",
      "\n",
      "  [[ -0.6057757    1.7133442    2.8641703    0.5958629    2.7594314\n",
      "      1.3030896   -0.8051967   -0.6426489 ]\n",
      "   [ -0.22311665  -8.5990305  -10.39877     -8.644732   -10.273176\n",
      "     -8.879024    -2.734983    -0.12058785]\n",
      "   [  0.5510202   -5.272624    -5.8298817   -4.458143    -6.6167383\n",
      "     -5.437187    -0.91685784   0.5980707 ]\n",
      "   [  1.7031115   -3.4144928   -4.1628165   -2.5063388   -5.1732554\n",
      "     -4.233163    -0.27847725   1.7545633 ]\n",
      "   [  1.997956    -4.2129874   -4.553624    -2.6992178   -5.7995715\n",
      "     -4.857032     0.17333609   1.9844515 ]]\n",
      "\n",
      "  [[ -1.699932     4.244729     6.19011      3.755602     5.4355664\n",
      "      6.699038    -0.3490838   -1.7462924 ]\n",
      "   [  1.4002923    0.54548395  -1.1169968    0.37443233  -1.1300082\n",
      "     -1.9262465    2.2690759    1.3882834 ]\n",
      "   [  1.6556793   -4.4972763   -5.2453957   -3.274147    -4.7581916\n",
      "     -5.8438854    1.4371101    1.7192887 ]\n",
      "   [  2.8702378   -4.0238047   -4.879114    -2.2400463   -4.472778\n",
      "     -6.184055     1.650003     2.8772376 ]\n",
      "   [  4.2184954  -10.176835   -10.954167    -6.2966757  -10.3249445\n",
      "    -10.7954855    1.2715628    4.230346  ]]\n",
      "\n",
      "  [[ -1.2787998    2.4755638    3.703323     1.3851306    3.2478092\n",
      "      3.6466854    0.553544    -1.0989556 ]\n",
      "   [ -0.68938744   0.7849126    0.80624384  -0.06641847   0.84722805\n",
      "      1.4088807    0.84816337  -0.6531029 ]\n",
      "   [  0.3542869   -2.6495297   -3.9148326   -2.3332536   -4.0371146\n",
      "     -2.1969824   -0.3534847    0.32806468]\n",
      "   [  1.7841424   -3.3586555   -5.717826    -3.0791197   -6.833018\n",
      "     -4.7764482   -0.9208296    1.659839  ]\n",
      "   [  2.1771913   -5.3258214   -6.7333293   -4.437318    -7.995056\n",
      "     -7.0693007   -1.9548045    2.037586  ]]\n",
      "\n",
      "  [[ -0.6709214    1.5035548    1.64232     -0.33530334   0.9649067\n",
      "      0.04905168  -0.89175946  -0.71355253]\n",
      "   [  0.24583115   0.6569769   -0.621118     1.2669846    0.91026103\n",
      "      1.4697332    1.5297475    0.26090285]\n",
      "   [  0.9396693    1.0186071    0.9804496    3.2190533    1.4295805\n",
      "      2.6770215    2.6240253    0.95563656]\n",
      "   [  0.6117326   -0.6111772   -0.8288269    0.2871933   -0.67439234\n",
      "     -0.41122133   0.9504747    0.6410048 ]\n",
      "   [ -0.64481556   2.1225655    2.2653017    1.3190256    3.896422\n",
      "      2.724441     1.7529845   -0.6356979 ]]\n",
      "\n",
      "  [[ -0.5303791   -2.110146    -1.3960739   -2.3525817   -1.9407862\n",
      "     -0.8966946   -2.0528047   -0.50565517]\n",
      "   [ -0.09481095  -1.1549708    0.31408975  -0.03525389  -0.4600581\n",
      "     -1.6961929    0.7154125   -0.0880874 ]\n",
      "   [  0.5121612   -0.1614938    0.4048692    0.8203044    0.5385774\n",
      "     -0.67759335   1.620004     0.4480935 ]\n",
      "   [  0.47279775   0.89751595   0.56361896   1.2335601    0.97617674\n",
      "      0.72556263   0.7255807    0.45003852]\n",
      "   [  0.77628833   1.3479958    0.6695447    1.9411157    1.0075263\n",
      "      0.5872247    1.8186214    0.7564151 ]]\n",
      "\n",
      "  [[ -1.6133933    5.655559     8.456806     4.720569     6.469103\n",
      "      6.3866005   -0.77200097  -1.6862562 ]\n",
      "   [  2.6422656   -3.0630863   -6.9324884   -2.84115     -4.448012\n",
      "     -5.5646486    1.8054792    2.638062  ]\n",
      "   [  2.5069184   -6.87229    -10.355062    -4.8297977   -8.231734\n",
      "     -8.594013     0.42899096   2.5198808 ]\n",
      "   [  2.0609508   -6.442012    -9.504503    -4.5295987   -6.941006\n",
      "     -7.39375      0.4449742    2.1119938 ]\n",
      "   [  1.6776993   -6.645504    -9.133078    -4.668825    -7.531585\n",
      "     -7.6619554    0.03244972   1.7574762 ]]]], shape=(1, 8, 5, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 8)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724    3.119958     0.8104701   -3.7867832   -0.53479564\n",
      "     -0.68400717  -2.684907    -4.306619  ]\n",
      "   [ -2.3828275    6.9414687    0.72609663  -5.475966    -3.1419044\n",
      "     -4.861004    -1.5760975   -1.7812909 ]\n",
      "   [ -9.4575205    9.2924795   -0.14664039  -7.7748985    0.3141518\n",
      "     -4.7069764   -3.4037216   -7.8661847 ]\n",
      "   [-10.166894    14.483014     3.299315   -11.655867    -6.542652\n",
      "     -5.2036433   -7.0711117   -9.239806  ]\n",
      "   [ -7.9254494   12.0516       3.657774   -14.190444    -6.1069603\n",
      "     -6.00821     -5.337775    -6.7280397 ]\n",
      "   [ -9.514821    12.602872     2.7181993  -11.5606165   -3.807155\n",
      "     -4.5184774   -7.4839892   -9.5418    ]\n",
      "   [ -6.294987     8.895258     0.31260866  -7.041161    -2.2837403\n",
      "     -4.8525386   -4.4346924   -5.9981575 ]\n",
      "   [ -1.627529     0.2572786    0.33721286  -1.0157803   -0.02855057\n",
      "      0.4145152   -2.0903144   -3.1125855 ]]\n",
      "\n",
      "  [[ -5.830585     2.9278567    6.4656816    2.9012501    0.7074189\n",
      "      1.8678039   -0.98469496  -5.710069  ]\n",
      "   [ -0.888771    -1.5304633   -0.92677224   7.0835996   -2.1796093\n",
      "      3.6029007   -0.78117675  -0.31565633]\n",
      "   [ -5.1757755   -0.00269019   5.1771684    2.4006374    1.5459608\n",
      "      4.5166783   -1.3455517   -7.2344646 ]\n",
      "   [ -0.32639125   1.338475     6.20836     -2.123073     1.0238662\n",
      "      0.95025975  -0.588602    -0.29866517]\n",
      "   [ -3.7669215    0.96661407   6.3918867   -2.0059547   -1.667028\n",
      "      2.4628794   -0.9746783   -5.5741014 ]\n",
      "   [ -5.9424763    3.3478012    5.0460277    5.0597334    1.5781783\n",
      "      5.766434    -3.736297    -6.4208727 ]\n",
      "   [ -5.028427     2.1733994    8.720076    -5.247528     2.0949328\n",
      "      0.3581393   -1.2882515   -6.517891  ]\n",
      "   [ -4.8532395    3.40758      5.827717     0.8119974    1.7706672\n",
      "      1.3585875   -2.134433    -4.6770263 ]]\n",
      "\n",
      "  [[  0.21261294   2.1825705    2.6290977   -0.7738304   -0.92338926\n",
      "      1.3111119   -1.7676708   -1.2661753 ]\n",
      "   [ -7.103699    -0.08641089   0.6366692   -1.1594403    0.52412534\n",
      "     -1.2009193   -3.281907    -9.214137  ]\n",
      "   [ -7.0138736    4.6630197   -1.6835009    0.64012516  -0.3236557\n",
      "      6.017604    -2.2194111   -8.46463   ]\n",
      "   [ -5.658084     3.870231     1.6583294   -3.8960452   -0.90947014\n",
      "     -1.150247     0.02895866  -8.4907255 ]\n",
      "   [ -2.1916983    2.1456993   -0.696395     2.0237832    0.4654629\n",
      "      2.97788     -1.8672262   -3.7927125 ]\n",
      "   [ -4.237206     0.8983024   -1.8900397   -1.5124333   -1.4959668\n",
      "     -3.0116844   -4.190044    -5.1174064 ]\n",
      "   [  0.44389454   0.6015544    1.0924258   -0.6734957   -0.14239292\n",
      "     -0.31658053  -3.1918075   -2.5049903 ]\n",
      "   [ -0.5491213    1.3170428    2.416191    -0.72504455   0.17214571\n",
      "      0.6214916   -0.7466009   -2.0425894 ]]\n",
      "\n",
      "  [[ -0.62310183   1.8474774   -0.70651436  -1.8744931    1.9892807\n",
      "      1.1960789    0.93903846  -0.96611404]\n",
      "   [ -2.1139884   -0.40789947   1.2968414   -2.4691556   -0.6430844\n",
      "      2.9704242   -0.49229723  -3.2126625 ]\n",
      "   [ -4.681421     1.071365     7.057754     1.6925517    2.925461\n",
      "      1.2117237   -2.5487785   -7.727114  ]\n",
      "   [ -5.2494354    2.6016214    4.7093654   -2.9185028   11.68196\n",
      "      5.0173087    0.2649115   -6.465863  ]\n",
      "   [ -4.187672     2.2745495    2.1844132   -2.950158     0.4811062\n",
      "      2.5944564   -1.2815626   -5.458976  ]\n",
      "   [ -5.1851773    3.6045432    5.879984    -1.5479906    9.332985\n",
      "      0.7532006   -6.1630864  -10.190875  ]\n",
      "   [ -1.4530067    3.602885     1.8737535   -1.0183158    6.639346\n",
      "     -0.45745188  -3.0397322   -5.1571107 ]\n",
      "   [ -1.3701262    1.7800907    0.4440075   -2.7584684    0.16184288\n",
      "      1.0718383    0.74469614  -2.4832382 ]]\n",
      "\n",
      "  [[ -3.365542     1.7112432   -0.18042703  -0.56367624  -1.0182812\n",
      "      0.91838205   0.3576792   -1.8675333 ]\n",
      "   [ -4.8425922   -4.2636166   -2.6981843    0.22403693   0.68739414\n",
      "     -3.1009803    2.71456     -1.489905  ]\n",
      "   [ -5.1876755    5.671311    -1.2768452   -2.9591484   -0.39675456\n",
      "     -3.3887212    1.0478692   -2.3474293 ]\n",
      "   [ -2.170307     3.4666238    1.0260042   -0.8940675    2.2694418\n",
      "     -2.5140228    1.5545614   -1.9796547 ]\n",
      "   [ -7.992303     0.4727184   -0.13311552  -3.517133    -2.5849926\n",
      "     -4.228556    -1.0881504   -5.6788545 ]\n",
      "   [ -3.6134825    0.19133087   0.298861    -4.0497766   -2.2160487\n",
      "     -3.5523865    0.53147453  -4.259141  ]\n",
      "   [ -2.6600354    4.010139     4.72034     -3.9068446   -1.4782856\n",
      "     -0.05746605  -5.8499537   -1.9302822 ]\n",
      "   [  2.192343    -2.0631888   -0.458438     0.3939276    0.19301154\n",
      "      3.2884934   -1.1612332    1.5978622 ]]\n",
      "\n",
      "  [[ -2.1686854   -0.68126124   1.8873476   -1.0016267    1.665862\n",
      "     -0.39142966  -2.5463562   -2.0510204 ]\n",
      "   [ -3.4723454   -2.521675     3.3836114   -0.29911357   3.7395344\n",
      "      2.2255218   -0.5534243   -3.5096362 ]\n",
      "   [ -1.241272    -1.0362755    0.8233798    0.13532306   0.49612203\n",
      "      2.9168866   -0.06637239   0.07041088]\n",
      "   [  0.16238996  -2.266713    -0.43468216  -1.9490578   -0.7804693\n",
      "     -2.2501192    0.14404878   0.669509  ]\n",
      "   [ -4.5097795    0.04322206   2.9916728    4.4943275    1.4705126\n",
      "     -0.8136589   -2.140882    -5.1061444 ]\n",
      "   [ -6.6359534    3.4645762    4.6767483   -2.6971583    5.242417\n",
      "      3.411301     1.1889111   -5.1261134 ]\n",
      "   [ -2.4793563    1.550663     3.5571485   -3.2087276    2.422179\n",
      "      0.22151406  -3.0109324   -3.599103  ]\n",
      "   [ -1.2139645   -1.8291416    2.1708345   -1.4788015    1.7007154\n",
      "     -1.5888573   -2.220674    -2.1623607 ]]\n",
      "\n",
      "  [[ -0.3049526    1.6942294   -0.7551851   -0.20310196  -1.9716048\n",
      "      0.50340194   0.2921721   -1.8446437 ]\n",
      "   [ -3.5450757    9.210564     3.1409702   -1.4201491   -8.377774\n",
      "     -2.9744062   -6.1630034   -6.277001  ]\n",
      "   [-10.783727     9.743        1.6049056   -7.786639    -9.65972\n",
      "     -3.0615754   -6.16684     -9.820524  ]\n",
      "   [-10.1322155    4.096334     1.2889549   -8.946105    -7.003514\n",
      "     -2.119969    -5.464064    -9.032952  ]\n",
      "   [ -6.2441616    5.4699683    0.8303648   -2.5861697   -6.0254755\n",
      "     -5.8178663   -3.9932399   -6.5110664 ]\n",
      "   [ -7.973193     7.724517     4.6691766   -4.006742    -8.032589\n",
      "     -1.6155707   -8.793758    -7.380326  ]\n",
      "   [ -5.8270884    2.8298204    4.595993    -2.1421301   -1.5096639\n",
      "     -4.502131    -9.880545    -6.981122  ]\n",
      "   [ -1.5424074   -0.03381129   0.8708008    0.84780496  -2.2531416\n",
      "     -0.00659109   0.6038286   -2.874726  ]]\n",
      "\n",
      "  [[ -2.8292158    1.3493172   -1.1630216   -2.1086867   -0.7928409\n",
      "     -1.6628647    0.936982     0.17500922]\n",
      "   [ -4.8446875    2.2061067    5.2808175   -7.194037     1.36124\n",
      "      0.13124909  -5.7810917   -5.9226546 ]\n",
      "   [ -5.9484262    1.2807072    4.3257394   -5.866291    -1.1549981\n",
      "      0.3883991   -5.670845    -4.975041  ]\n",
      "   [ -4.245631     5.4597836    3.617752    -7.2080035   -3.3983228\n",
      "     -4.2374063   -2.325605    -2.7597272 ]\n",
      "   [ -3.7233293    5.6208553    0.9434773   -6.0832963   -2.91809\n",
      "     -0.02632309  -1.9356793   -0.92453605]\n",
      "   [ -2.065824     5.411931     2.41236     -4.3067594    2.3385603\n",
      "     -1.8093777   -4.6885967   -1.7742286 ]\n",
      "   [ -3.8498166    5.2004757   -1.0518612   -7.7332063   -0.27363002\n",
      "      0.8425215   -2.1548126   -2.9958065 ]\n",
      "   [ -1.0712852    3.3348348    0.1982265   -2.701739     0.3845683\n",
      "     -1.0062916    2.1746593    0.6345082 ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.108165     2.77802      7.04615      2.6454995    5.4191833\n",
      "      5.2317686   -5.6294155   -9.130048  ]\n",
      "   [-10.735054     1.7547116    0.76175237  -0.42852682   4.004292\n",
      "      2.5349874   -3.68677    -13.101013  ]\n",
      "   [ -7.5400214    0.57103527  -0.5523173    0.4326064    1.1318936\n",
      "      0.61845857  -3.0292842   -9.038257  ]\n",
      "   [-13.195052     3.5323567    3.9254384    3.0674067    4.645694\n",
      "      4.6829467   -5.6105638  -17.256235  ]\n",
      "   [-10.567291     1.5221579    0.52586746   1.5666054    3.392625\n",
      "      3.7933207   -3.6424325  -13.40815   ]\n",
      "   [-11.348783     1.1702118    0.33376536   1.9208177    4.171567\n",
      "      3.2230349   -4.625526   -14.001499  ]\n",
      "   [-13.822072     4.6995225    6.882736     4.5583987    9.456506\n",
      "      8.984521    -7.40026    -19.019571  ]\n",
      "   [ -4.276782     2.2549555    6.41255      2.478643     5.0064325\n",
      "      5.4905148   -5.49556     -7.172459  ]]\n",
      "\n",
      "  [[ -2.6697712    0.18644784  -3.2964184   -9.0699005   -5.8869786\n",
      "     -8.670129    -1.7457955   -2.4067173 ]\n",
      "   [ -4.892798     5.1958284    0.86651105  -1.7851338   -0.5991809\n",
      "      1.0769178   -6.025225    -5.985986  ]\n",
      "   [ -9.587597     3.4813366   -1.9858788   -0.24309416   0.67243534\n",
      "      4.1456385   -5.7291985  -11.872933  ]\n",
      "   [-10.449494     4.90856     -2.7020588   -1.3338774    0.16304302\n",
      "      4.3895545   -7.198788   -13.19233   ]\n",
      "   [ -6.8310695    3.5543683   -0.808639     0.5658518    1.5122908\n",
      "      3.5409427   -5.93916     -8.560726  ]\n",
      "   [-10.439719     3.3256927   -2.8369732   -1.1681513    0.17411843\n",
      "      4.7924056   -7.1944017  -12.024714  ]\n",
      "   [ -4.1172786    3.8512855   -1.755627    -3.863435    -1.9191699\n",
      "     -2.5844417   -4.672864    -4.7063437 ]\n",
      "   [ -1.7964184   -0.00925941  -3.5144498   -9.731884    -6.624717\n",
      "    -10.108276    -0.76045424  -0.7916409 ]]\n",
      "\n",
      "  [[ -4.7987876    2.3515558   -2.4956558    3.970095     6.9596395\n",
      "      6.1769433    2.5710814   -5.3403516 ]\n",
      "   [ -6.3391857    1.296634    -1.2071677   -2.4126644   -0.8230457\n",
      "      1.5592674   -4.067092    -6.657909  ]\n",
      "   [ -9.1259985    8.161333     4.880271    -1.283182     4.1173687\n",
      "      4.9365716   -5.1432853  -11.959505  ]\n",
      "   [-10.686145     8.565781     5.08118      0.27082056   5.3818784\n",
      "      6.742503    -2.9075274  -13.400082  ]\n",
      "   [ -9.507849    10.487074     6.0747166   -1.3516918    4.202924\n",
      "      5.809277    -3.300832   -12.412424  ]\n",
      "   [ -9.160462    10.127481     7.924963     1.2205707    6.1613603\n",
      "      7.179847    -3.3297603  -12.610715  ]\n",
      "   [ -3.074413    11.459646     8.69444      8.782502    13.618378\n",
      "      9.073093     3.539803    -6.4021416 ]\n",
      "   [ -2.5498397    2.3677871   -2.3312814    5.376789     7.2629266\n",
      "      5.222065     4.6978946   -2.8941624 ]]\n",
      "\n",
      "  [[  5.7272882    2.8488104   -2.2674716    2.4710479    1.0492487\n",
      "     -1.1509322    1.7533069    4.0113854 ]\n",
      "   [ -6.0593033   -0.8574972   -0.14114377   2.036406     0.46856305\n",
      "      3.5360727    1.1927096   -6.1919413 ]\n",
      "   [ -3.0789824   -0.10847238   1.995033     3.6496744    3.8057036\n",
      "      6.084562     4.08223     -3.2766128 ]\n",
      "   [ -4.919659     1.601183     1.003934     3.2019112    0.6709702\n",
      "      4.7461705    1.0749629   -5.1771154 ]\n",
      "   [ -6.994366     0.38075814  -0.8496923    2.2005079    2.1127446\n",
      "      4.8004184    1.2863806   -7.113053  ]\n",
      "   [ -1.9637377    2.5683565    0.11696774   0.11398675  -0.02081025\n",
      "      1.7587206   -0.9755858   -2.719708  ]\n",
      "   [ -4.988607     7.0706754    1.6426951    7.2739515    7.1590147\n",
      "      9.629911     6.3608046   -7.190774  ]\n",
      "   [  7.1238737    5.775128    -1.2283382    3.9956207    3.3504624\n",
      "      0.41485792   3.2472675    4.7716117 ]]\n",
      "\n",
      "  [[ 15.227098    -7.6481614   -6.1704655   -2.9068828   -4.9539256\n",
      "     -6.9440393   -5.5164175   14.977688  ]\n",
      "   [ -9.1538725    2.5506845    3.8603075   -0.97211933  -0.7266477\n",
      "      1.0761876    4.012439    -8.621349  ]\n",
      "   [ -7.3554325    3.0492685    4.882066     1.9635559    1.5533912\n",
      "      2.5195072    5.0332184   -6.660119  ]\n",
      "   [ -7.502227     4.479796     3.8863442    0.8529868    2.5325358\n",
      "      2.6109834    3.7704332   -7.242996  ]\n",
      "   [-14.381295     7.1894965    6.8530087    0.8468516    1.6221398\n",
      "      3.4500875    6.026878   -14.084528  ]\n",
      "   [-12.931715     6.7301073    6.57782      0.15552036   0.9927032\n",
      "      0.9647971    3.3784504  -12.61685   ]\n",
      "   [  0.44247183  -0.69109786  -1.4392438   -1.959099    -3.2892675\n",
      "     -3.1996653   -2.5787928   -0.35673866]\n",
      "   [ 16.702045    -7.9659853   -7.5883594   -3.50606     -4.3180175\n",
      "     -7.998839    -8.3601055   15.943729  ]]\n",
      "\n",
      "  [[ 16.92651     -1.8878899    4.612371     8.297861     8.785893\n",
      "     10.494433    13.696043    18.756512  ]\n",
      "   [ -4.8187237    0.3554866   -0.10856649  -0.7664691   -0.8179527\n",
      "     -2.149825    -1.7334187   -4.167079  ]\n",
      "   [ -9.965244     1.1075305   -3.9921682   -5.4180512   -3.1439044\n",
      "     -9.256229    -8.250524   -10.748956  ]\n",
      "   [ -7.730639     0.702649    -3.5076466   -2.8937106   -1.1201787\n",
      "     -2.574378    -2.7975821   -7.5726776 ]\n",
      "   [-10.1452055   -1.0527366   -4.0010457   -2.8202963   -3.0494757\n",
      "     -5.4484124   -5.228744   -10.110497  ]\n",
      "   [-11.985994     3.1561482   -2.7560956   -3.3319776   -1.583422\n",
      "     -4.690591    -5.913287   -12.567582  ]\n",
      "   [ -5.3008895   -3.0260174   -2.1208181   -1.0755001   -2.3386164\n",
      "     -0.51985437  -1.1730318   -4.255245  ]\n",
      "   [ 17.86222     -1.6225508    5.1695466    7.7438717    8.076046\n",
      "     10.810771    13.726412    19.605438  ]]\n",
      "\n",
      "  [[ -5.9784994   -2.1169765    4.562182     0.5942515    3.0359569\n",
      "      1.3266702   -0.49234566  -3.304644  ]\n",
      "   [-12.7146845    2.0409193    2.1862695   -0.09457824   1.4637623\n",
      "      2.2060208   -4.997251   -13.3957815 ]\n",
      "   [-15.493677     3.6968644    4.4985094    1.5548421    1.8590727\n",
      "      4.2570806   -6.917543   -16.565073  ]\n",
      "   [ -9.458374    -0.38232473   3.5684469    0.3583388    1.3042445\n",
      "      1.8564001   -4.3985405   -9.246963  ]\n",
      "   [-12.0117655    2.1870446    3.3004608    0.608759     0.99599266\n",
      "      1.7472012   -5.940463   -13.297273  ]\n",
      "   [ -9.835355    -0.9845663   -0.7273823   -2.8138676   -0.3688764\n",
      "      0.66789573  -4.370575    -9.257524  ]\n",
      "   [ -3.3896937   -3.072007     1.3647641    1.7215154    1.1578857\n",
      "      2.4925263    3.2728026   -0.55832624]\n",
      "   [ -2.0235658   -2.5166588    4.0697465    1.5893646    3.5398774\n",
      "      1.8933309    2.816157     1.3247585 ]]\n",
      "\n",
      "  [[ 13.876768     5.289463     2.8875844    5.729185    -0.31207085\n",
      "      2.216713     0.970955    11.02701   ]\n",
      "   [-11.855931     7.044625     2.757176    -2.0767522    9.620936\n",
      "      3.4307175   -0.20549646 -11.727619  ]\n",
      "   [-17.164427     9.00703      2.9232194   -2.114338    10.674683\n",
      "      4.7908983    0.6977465  -17.203806  ]\n",
      "   [ -6.0810537    6.5570908    3.815859     1.4116415    7.24501\n",
      "      5.5307817    2.8692966   -6.8714347 ]\n",
      "   [ -9.309462     5.957898     3.5396616   -0.5519851    6.9079504\n",
      "      5.2796082    1.4829706   -9.929194  ]\n",
      "   [ -9.980593     6.659394     3.6414435   -1.2484376    6.108645\n",
      "      4.148278     2.2938857  -10.687042  ]\n",
      "   [-11.461593     8.152552     3.527002    -0.32889172   5.8540664\n",
      "      4.3797064    2.374406   -12.23368   ]\n",
      "   [ 16.641361     4.062904     2.288843     5.4982553   -3.302351\n",
      "      0.8199426    0.47605655  13.646871  ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    0.09299298   0.60130334   1.3376766    1.1430501\n",
      "      1.8065927 ]\n",
      "   [ -0.18269673  -4.40855      0.07642998   2.0146773    0.21539456\n",
      "      6.6477065 ]\n",
      "   [ -0.11618186   3.2863328   -3.025875     0.4890776    3.37132\n",
      "     -4.0622787 ]\n",
      "   [  1.0314696   -1.1489769   -4.595038    -7.4570365   -6.3443\n",
      "    -12.493905  ]\n",
      "   [  0.47551402  -2.962047     0.34962878  -1.2729536    1.1051896\n",
      "     -2.1368446 ]\n",
      "   [  1.2587248  -10.239238    -2.7199278    7.3864627    5.0898485\n",
      "     28.240181  ]]\n",
      "\n",
      "  [[ -0.1123142    0.56263393   0.21921782  -0.1352104   -0.02665874\n",
      "     -0.8839603 ]\n",
      "   [ -0.29892173  -1.5473536    1.286812     2.3739398    5.4393287\n",
      "      6.9619155 ]\n",
      "   [ -0.25255403   2.1486242   -6.0938       3.577338     6.605463\n",
      "      4.887539  ]\n",
      "   [  0.24040057  -0.7833131   -1.2749462    2.9239094   -3.3049998\n",
      "     -7.017581  ]\n",
      "   [  0.4180626   -2.545671    -0.2369275    3.2331958   -3.0744147\n",
      "     -1.2852639 ]\n",
      "   [  0.02501542 -10.661579    -7.6573796   -1.5968752    0.33785945\n",
      "     47.253338  ]]\n",
      "\n",
      "  [[ -0.10918517   0.11000926   0.21102953   0.4795525    0.34849948\n",
      "      0.4386105 ]\n",
      "   [  0.07259987  -4.5046606   -4.6542673   -1.4462825   -0.4685369\n",
      "      8.264568  ]\n",
      "   [ -0.0773119   -0.45317498   3.3376467    3.8257363    5.9663725\n",
      "      3.8890088 ]\n",
      "   [  0.6502231    2.718805     2.302118     3.527786     9.5337105\n",
      "     10.864909  ]\n",
      "   [ -0.32270402   1.7251928    2.3383963    5.20828      7.563763\n",
      "     12.650614  ]\n",
      "   [ -0.14452453  -2.6593761    0.12439811   0.23121887   4.6088295\n",
      "      9.131668  ]]\n",
      "\n",
      "  [[ -0.22114104  -0.48759893   0.03473653   1.0193444    0.8324323\n",
      "      1.3644354 ]\n",
      "   [  0.67009145  -7.431193    -0.8688647   -3.4817095    0.6493854\n",
      "      0.1774551 ]\n",
      "   [ -0.6334822    2.1947665   -1.3594295    3.5641603    2.0439782\n",
      "     -2.5815616 ]\n",
      "   [  0.43472287  -3.067627     0.94878364  -1.7823364    3.5336585\n",
      "      0.65961915]\n",
      "   [ -0.21712674  -1.7631739   -0.9125511    6.9749928    8.10879\n",
      "     10.273918  ]\n",
      "   [ -1.55404      1.3847003   -4.8859715   13.564033    13.340582\n",
      "     35.303543  ]]\n",
      "\n",
      "  [[  0.00076126   0.07451362   0.17366233  -0.0934004    0.05626196\n",
      "      0.78244406]\n",
      "   [ -0.10380323  -2.2713957   -1.0344037   -0.31538382   2.3452058\n",
      "     -6.528854  ]\n",
      "   [  0.00120817  -2.261606    -4.7580304    3.1489854   -0.6975868\n",
      "     -4.4293337 ]\n",
      "   [  0.92358315  -4.130864    -1.2742832    0.6084041   -2.5395427\n",
      "    -10.802182  ]\n",
      "   [  0.62570703  -3.030466     1.548444     1.2197994    0.37899518\n",
      "      2.8239465 ]\n",
      "   [  0.7233959   -6.8197236   -3.4712522   -5.727396    -1.9201907\n",
      "    -11.494286  ]]\n",
      "\n",
      "  [[ -0.00999826   0.21593446  -0.3890052    0.1919408    0.18444464\n",
      "      0.5804339 ]\n",
      "   [  0.9415558   -2.968748    -1.1732452   -2.6510923   -2.8050153\n",
      "     -5.9246264 ]\n",
      "   [ -0.03670223  -0.73637694   3.4228125   -0.21042274   0.7946963\n",
      "      5.035904  ]\n",
      "   [  0.88830537  -2.3718863    3.6929164    0.17536415  -1.252368\n",
      "     -8.647176  ]\n",
      "   [  0.72545135  -2.5605028   -0.0558291   -4.3498635   -4.4779325\n",
      "     -3.3239167 ]\n",
      "   [ -0.5783319   -1.7032222   -2.6085572   -3.1776903   -5.7269692\n",
      "      9.592078  ]]\n",
      "\n",
      "  [[ -0.20863976  -0.27273425   0.42647386   0.07348019   0.62226117\n",
      "      0.7607086 ]\n",
      "   [  0.59413576  -7.0776234    0.3937803    0.7662048   -2.2862425\n",
      "     -0.74427146]\n",
      "   [  0.96936405   1.54816     -5.38302     -2.581315    -3.2941782\n",
      "     -6.429257  ]\n",
      "   [  0.38710782  -1.8460459    0.8744281   -2.146471    -1.8200386\n",
      "      2.8290641 ]\n",
      "   [  0.8605784   -2.11317     -2.68866      0.36657253  -2.6509297\n",
      "     -3.7553818 ]\n",
      "   [  1.0977722    1.4501293    0.10086851  -5.2778416   -4.4163\n",
      "     27.651638  ]]\n",
      "\n",
      "  [[ -0.15726982  -0.16104104   0.07374533   1.040318    -0.00393564\n",
      "      0.8906403 ]\n",
      "   [ -0.19427125  -5.400611     2.8342748    0.9576878    2.3463671\n",
      "     -0.5020499 ]\n",
      "   [  0.9997108   -1.4077754    0.58703816  -2.564867     2.9698117\n",
      "     -0.49522534]\n",
      "   [  0.04265315   4.068154     0.62130535  -1.1917355   -1.5518103\n",
      "    -11.859318  ]\n",
      "   [  0.3693564   -1.2260512   -0.9977996    0.86115277   3.8171647\n",
      "      7.756687  ]\n",
      "   [  0.48773396  -8.20396     -8.352431    -0.36510298  -5.8348966\n",
      "     34.07664   ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.64339346  -3.427214    -1.9332778   -1.831178    -2.9725688\n",
      "     -3.3511758   -0.33726037   0.65105456]\n",
      "   [ -0.9194904    0.8968038    0.45222297   1.0594327    1.1557782\n",
      "     -0.8956241   -0.17587543  -0.8396095 ]\n",
      "   [ -0.04047573  -0.15472572  -0.69453156   0.27039978  -0.9049967\n",
      "      0.7182969   -0.13295098  -0.00661357]\n",
      "   [  0.40479198  -0.3806878   -0.4154958   -0.36258787  -1.1813488\n",
      "      0.5046691    0.05306904   0.42328048]\n",
      "   [  1.9060687   -5.0097375   -2.570788    -2.4334707   -5.5112567\n",
      "     -4.1143527   -0.6269515    1.843126  ]\n",
      "   [  2.2318835   -5.2877703   -2.8237607   -3.6016464   -5.5664296\n",
      "     -5.238737    -1.1923052    2.1229136 ]]\n",
      "\n",
      "  [[  1.1482323   -9.236041    -8.218754    -6.717816    -9.638861\n",
      "     -8.856656    -2.9334142    1.2082323 ]\n",
      "   [ -1.8857075    3.473417     2.5907042    3.5434728    3.9081209\n",
      "      5.844051     1.0954899   -1.8498287 ]\n",
      "   [  0.47446606  -0.28332782   0.63271886  -0.17188838   0.33722964\n",
      "      0.37442216   0.03080073   0.46110418]\n",
      "   [  1.8575145    0.71112      1.3011446    1.2523906   -0.17590559\n",
      "     -1.8068957    0.19210525   1.7457882 ]\n",
      "   [  3.592382    -6.1458387   -5.0730557   -4.1431313   -6.217651\n",
      "     -8.495154    -0.84755003   3.5301905 ]\n",
      "   [  2.3554647   -5.5941854   -4.755025    -5.4442506   -6.5367403\n",
      "     -9.464186    -2.4507759    2.3181825 ]]\n",
      "\n",
      "  [[ -1.0731127   -1.8999621   -2.2802618   -2.3014586   -1.5626087\n",
      "     -1.5433154   -1.408609    -1.0031282 ]\n",
      "   [ -0.15004301  -3.7747118   -4.124579    -3.467151    -3.0495815\n",
      "     -2.7865295   -1.1569786   -0.1172325 ]\n",
      "   [  1.0226461    0.42154983   0.85200024   1.2949932   -0.23638739\n",
      "      1.2461828    0.683046     0.9681701 ]\n",
      "   [  2.5858383   -3.6268303   -4.0738564   -2.3015351   -5.114497\n",
      "     -3.4861133    0.7499384    2.550022  ]\n",
      "   [  3.4190211   -1.8762418   -2.089898    -0.12326887  -2.6943471\n",
      "     -1.6936332    2.811297     3.332227  ]\n",
      "   [  2.4225883   -6.606093    -7.7032475   -4.8267703   -7.686963\n",
      "     -7.1137204   -0.681313     2.421773  ]]\n",
      "\n",
      "  [[ -0.4604545   -1.5868427   -0.91173816  -0.8565646   -1.1848444\n",
      "     -2.0918827   -0.78210574  -0.41651735]\n",
      "   [ -1.3551308    0.1230931    0.7364044   -0.6643719    0.8351439\n",
      "      0.06500757   0.78761655  -1.2645116 ]\n",
      "   [  0.68831766   0.8079782   -0.10353167   1.935387     0.18036631\n",
      "      2.0298638   -0.18135574   0.6273567 ]\n",
      "   [  1.9218479   -8.242031    -7.458511    -5.4300194   -7.086632\n",
      "     -7.069091    -2.0435832    1.9204775 ]\n",
      "   [  2.4111772   -7.0431905   -6.9708333   -4.6501546   -7.462153\n",
      "     -7.238265    -2.632736     2.3841848 ]\n",
      "   [  1.5472274   -6.249078    -5.308054    -2.767143    -5.4733706\n",
      "     -5.3149114   -2.159049     1.534408  ]]\n",
      "\n",
      "  [[ -1.5332501    4.991253     6.4686904    4.0847487    5.368916\n",
      "      4.911813     2.5922883   -1.523277  ]\n",
      "   [ -1.8101153    6.2314014    5.68846      4.518671     6.62946\n",
      "      6.433606     3.0988286   -1.8167404 ]\n",
      "   [  1.1217315   -4.2282114   -4.6459684   -2.7598808   -3.6334445\n",
      "     -3.9288173   -2.1453893    1.1145377 ]\n",
      "   [  3.112364    -8.154078    -6.9861646   -4.1005797   -7.100312\n",
      "     -6.5753813   -2.6710312    3.1095238 ]\n",
      "   [  4.4973893  -12.889609    -9.3284645   -5.5953784  -10.986841\n",
      "     -9.9575405   -4.63632      4.475674  ]\n",
      "   [  2.9127285  -10.431667    -9.397811    -6.4185715   -9.596227\n",
      "     -9.31257     -3.2900326    2.9107234 ]]\n",
      "\n",
      "  [[  1.6350883   -6.317388    -5.912019    -3.864096    -6.084104\n",
      "     -5.8046737   -1.3961531    1.6473184 ]\n",
      "   [  0.24405284   1.462353     0.9224829    1.581335     1.9375173\n",
      "      2.096835     2.0256872    0.28618026]\n",
      "   [  1.4571862   -8.192412    -7.3626842   -4.9873695   -7.7452354\n",
      "     -6.7284403   -1.935825     1.4636596 ]\n",
      "   [  1.2628276   -5.2162313   -6.7320704   -3.7959094   -6.008326\n",
      "     -4.775027    -1.3642161    1.3028276 ]\n",
      "   [  1.667126    -5.457896    -5.923977    -3.5345056   -6.5892243\n",
      "     -5.3130007   -1.6549468    1.6527491 ]\n",
      "   [  2.4885507   -9.548669    -8.804837    -5.8452096  -10.26328\n",
      "     -8.904843    -2.756548     2.4845312 ]]\n",
      "\n",
      "  [[  0.12968476  -2.7599597   -3.3458984   -0.6776076   -1.3498564\n",
      "     -2.4725385   -0.26429352   0.143075  ]\n",
      "   [  0.16608208  -3.8964067   -3.9270244   -2.1397204   -2.7393942\n",
      "     -2.4727707   -0.7227451    0.23026752]\n",
      "   [  0.2666929    0.5002075    0.8165109    1.2050005    1.1429677\n",
      "      2.0832279    1.431956     0.24901326]\n",
      "   [  1.9745605   -5.4055204   -6.2844653   -4.0218945   -6.1748066\n",
      "     -4.6382313   -1.3935999    1.9711884 ]\n",
      "   [  2.574275    -6.499424    -7.990838    -5.2485123   -7.2974114\n",
      "     -6.4010496   -1.8392944    2.5551796 ]\n",
      "   [  2.0763962   -9.478544   -10.627647    -7.967728   -10.239419\n",
      "     -8.769783    -3.3906584    2.1408231 ]]\n",
      "\n",
      "  [[ -0.48440975  -0.30788445  -1.4893405   -0.5263086    0.2881688\n",
      "     -0.46770203  -0.27271533  -0.46877885]\n",
      "   [ -0.3127084   -1.174712    -0.8735866   -0.32615614  -0.25206855\n",
      "     -0.55387217   0.23482844  -0.2958877 ]\n",
      "   [  0.75498635   1.107361     1.7389252    1.5763456    1.1201576\n",
      "      1.3887876    0.36387968   0.70142406]\n",
      "   [  1.9517523   -3.5100362   -4.6243114   -2.928465    -5.088273\n",
      "     -4.245649    -1.9016756    1.927332  ]\n",
      "   [  1.7787431   -2.3995202   -1.9599189   -1.07149     -3.3473392\n",
      "     -2.1400583   -1.0688959    1.7242967 ]\n",
      "   [  2.2856932   -3.2953563   -3.1881175   -2.1589944   -4.356325\n",
      "     -3.7118404   -1.937447     2.2174952 ]]]], shape=(1, 8, 6, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 8)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.8282402  -0.42404926 -0.8946604  -0.24211022  0.3486308\n",
      "    -0.5410196 ]\n",
      "   [-1.2308896   2.5182536   0.9160354  -0.32361427 -5.502114\n",
      "    -0.52194935]\n",
      "   [ 1.7116143   0.36283326 -0.38897648 -4.488289   -4.894553\n",
      "    -0.84117657]\n",
      "   [ 1.2025449  -1.5277051  -0.21148735 -2.888625   -0.5083307\n",
      "     0.96249807]\n",
      "   [-2.268408   -6.8269224  -0.6192657   1.2520456  10.82034\n",
      "     5.504037  ]\n",
      "   [ 2.4727604   1.4575851   0.499351    1.6664139   1.5472767\n",
      "     1.2165604 ]]\n",
      "\n",
      "  [[ 0.05669579 -0.14652377  0.7225302  -0.12972002  0.762966\n",
      "     1.8914417 ]\n",
      "   [-1.3575498   1.1293744  -1.4357362   0.78531826  2.1577141\n",
      "     0.68724555]\n",
      "   [ 3.1011949   1.2757022  -1.3072673  -3.8735275  -1.1950318\n",
      "    -0.9596819 ]\n",
      "   [ 0.23908208  0.42009813 -1.7638853   0.09815715  1.505165\n",
      "     1.2786608 ]\n",
      "   [-1.1174238  -1.1195521  -0.5070932   2.6752462   2.2329626\n",
      "     1.5505058 ]\n",
      "   [ 0.46334186 -0.7222311   3.6205647  -0.19479612  0.07176016\n",
      "     2.7273045 ]]\n",
      "\n",
      "  [[-3.0994341   3.915559    3.6157894   1.844174    1.2353401\n",
      "     1.9127477 ]\n",
      "   [-6.1072607  -1.5910499   4.307164    3.4624133   2.229136\n",
      "     4.1504526 ]\n",
      "   [-1.8124647   3.9855342   1.44573     0.20429029 -2.5603697\n",
      "     2.187535  ]\n",
      "   [-0.36083025 -3.97918    -1.1556294   0.4108099   0.2031284\n",
      "     1.8829602 ]\n",
      "   [-2.4606414   0.03661868  3.556143    2.4729695   0.3735249\n",
      "     0.2762304 ]\n",
      "   [ 1.5901557  -0.6747893  -1.8762383  -1.5708107  -0.19510797\n",
      "     4.408436  ]]\n",
      "\n",
      "  [[-2.9529114   2.299974    3.0738797   3.844925    1.4925915\n",
      "     1.5814453 ]\n",
      "   [-7.031095   -0.24240322  8.241785    9.95658     5.335687\n",
      "     7.2491493 ]\n",
      "   [-4.861674   -1.6940056   2.8614326   6.290135    5.342425\n",
      "     1.8807219 ]\n",
      "   [-1.8445894  -2.6421747   0.42038977  3.2298315   4.995947\n",
      "    -0.5657021 ]\n",
      "   [ 1.6251752   6.5201087  -5.3137836  -7.2730436  -8.355499\n",
      "    -1.0261768 ]\n",
      "   [ 1.761756    3.721384   -2.9289532  -4.5786343  -1.7682326\n",
      "     2.10552   ]]\n",
      "\n",
      "  [[-0.5371959   0.03175565  0.72881126  0.9492595  -0.60133326\n",
      "     0.8410563 ]\n",
      "   [-1.4198313  -6.4444065  -0.92142916  2.328551    0.99433136\n",
      "     4.9305143 ]\n",
      "   [ 2.8078191   2.1120253  -2.642926   -6.017806   -4.966147\n",
      "     0.9855578 ]\n",
      "   [ 1.2005723  -0.03687301  0.87963223 -2.9841392  -0.84754145\n",
      "     1.7816778 ]\n",
      "   [ 0.6997136  -1.0197824  -4.4537454  -2.2257385   0.37594527\n",
      "    -2.3804784 ]\n",
      "   [ 1.2986425  -0.04222885 -2.943778   -1.5755681  -1.155994\n",
      "    -3.0385654 ]]\n",
      "\n",
      "  [[-1.412344    1.717014   -0.30293384 -0.56581223 -0.25823367\n",
      "    -0.06839997]\n",
      "   [-1.9951228  -3.2546551   1.0123291  -0.44657713  3.7238536\n",
      "     1.6955757 ]\n",
      "   [ 0.73561543  0.3251526  -3.0940294   0.9392657  -0.1000299\n",
      "     0.09806607]\n",
      "   [ 1.3327664  -0.05496131 -0.6325569  -0.44301042  1.7962822\n",
      "     0.2296008 ]\n",
      "   [-0.36015886 -3.758684    0.89078796  0.7360876   7.093279\n",
      "    -0.7114609 ]\n",
      "   [ 0.7831615  -3.8473794  -2.7056634  -1.9370512   0.820313\n",
      "    -2.6928487 ]]\n",
      "\n",
      "  [[-2.4360008   0.67301464 -0.9478727   1.727219    2.7742186\n",
      "     2.4426146 ]\n",
      "   [-0.27072212 -7.063433    1.4467108   3.1845582   1.4821701\n",
      "    -1.9088871 ]\n",
      "   [ 3.4422889  -1.1130698  -3.4816942  -3.4157388  -4.736643\n",
      "    -5.1762915 ]\n",
      "   [ 1.1087734   0.70760036 -1.7606698  -2.657325   -2.180934\n",
      "    -0.81913483]\n",
      "   [ 1.2798668  -1.775176   -0.39098    -2.8660033  -4.073905\n",
      "    -1.7023361 ]\n",
      "   [ 2.4068856  -3.7182271  -0.40062857 -3.4740262  -5.85092\n",
      "    -3.3386378 ]]\n",
      "\n",
      "  [[-1.929139    0.8935091   1.5377133   1.4501257  -1.6422881\n",
      "     0.9996356 ]\n",
      "   [-1.0276425  -4.9508924   3.0305479   1.7626755   0.71068406\n",
      "    -0.31119004]\n",
      "   [ 0.29604688 -1.3716084  -0.20351243  1.1358527   1.8680996\n",
      "     1.4605832 ]\n",
      "   [ 0.7165042  -1.7467748  -0.3651124  -0.20605943  1.1415403\n",
      "     0.74873084]\n",
      "   [ 0.16737235 -3.0147772   0.42238253  0.69539714  0.11543402\n",
      "     0.96405405]\n",
      "   [ 1.8527663  -0.9928748  -2.5378797  -2.639054    0.24549098\n",
      "     2.3318253 ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.3973298    3.2900124    5.064679     2.9755645    4.015015\n",
      "      4.0069337   -0.21337411  -0.49866062]\n",
      "   [ -0.33577263   4.7211685    5.425065     3.9286716    3.8147593\n",
      "      3.5898767    0.9110762   -0.36344096]\n",
      "   [ -0.05790175   0.3653249   -0.63628477  -0.08784539  -0.41260353\n",
      "     -1.178433     0.6327939   -0.04360336]\n",
      "   [  0.6763596   -3.6069698   -5.242045    -2.8647668   -4.1328135\n",
      "     -4.5069427   -0.0389813    0.7387742 ]\n",
      "   [  1.0683483   -3.5492473   -5.8436317   -3.085351    -4.965308\n",
      "     -5.2194877   -0.71366364   1.1221966 ]\n",
      "   [  1.5454725   -5.328811    -5.023481    -3.1240249   -4.853062\n",
      "     -5.6651163   -1.2183317    1.5260065 ]]\n",
      "\n",
      "  [[ -0.6408605    3.2326546    3.591694     2.0563188    3.0459878\n",
      "      4.4082403   -0.3960128   -0.63534915]\n",
      "   [ -1.1932893    5.5270476    5.752577     4.0630784    4.8154626\n",
      "      5.298501     1.4706326   -1.2358047 ]\n",
      "   [ -0.52651423   0.23078535   0.45221442   1.5162919    0.8854996\n",
      "      0.6778835    1.459687    -0.5176164 ]\n",
      "   [  0.6784578   -3.6849337   -3.4605443   -1.5862267   -2.9143507\n",
      "     -4.225469     0.56281316   0.6827177 ]\n",
      "   [  0.88377523  -6.1549253   -4.9009666   -3.4213269   -4.780138\n",
      "     -5.463376    -1.2016573    0.9243931 ]\n",
      "   [  1.2206075   -5.753873    -4.250112    -2.921483    -4.9512706\n",
      "     -5.764468    -1.1546768    1.1742201 ]]\n",
      "\n",
      "  [[ -0.6057757    1.7133442    2.8641703    0.5958629    2.7594314\n",
      "      1.3030896   -0.8051967   -0.6426489 ]\n",
      "   [ -0.22311665  -8.5990305  -10.39877     -8.644732   -10.273176\n",
      "     -8.879024    -2.734983    -0.12058785]\n",
      "   [  0.5510202   -5.272624    -5.8298817   -4.458143    -6.6167383\n",
      "     -5.437187    -0.91685784   0.5980707 ]\n",
      "   [  1.7031115   -3.4144928   -4.1628165   -2.5063388   -5.1732554\n",
      "     -4.233163    -0.27847725   1.7545633 ]\n",
      "   [  1.997956    -4.2129874   -4.553624    -2.6992178   -5.7995715\n",
      "     -4.857032     0.17333609   1.9844515 ]\n",
      "   [  1.9294199   -5.6965923   -6.2545986   -3.4763138   -7.2742057\n",
      "     -6.2437105   -0.5825363    1.9847751 ]]\n",
      "\n",
      "  [[ -1.699932     4.244729     6.19011      3.755602     5.4355664\n",
      "      6.699038    -0.3490838   -1.7462924 ]\n",
      "   [  1.4002923    0.54548395  -1.1169968    0.37443233  -1.1300082\n",
      "     -1.9262465    2.2690759    1.3882834 ]\n",
      "   [  1.6556793   -4.4972763   -5.2453957   -3.274147    -4.7581916\n",
      "     -5.8438854    1.4371101    1.7192887 ]\n",
      "   [  2.8702378   -4.0238047   -4.879114    -2.2400463   -4.472778\n",
      "     -6.184055     1.650003     2.8772376 ]\n",
      "   [  4.2184954  -10.176835   -10.954167    -6.2966757  -10.3249445\n",
      "    -10.7954855    1.2715628    4.230346  ]\n",
      "   [  3.207005    -2.7796998   -3.4356103   -0.40124664  -3.7303617\n",
      "     -4.1345506    2.37348      3.2100012 ]]\n",
      "\n",
      "  [[ -1.2787998    2.4755638    3.703323     1.3851306    3.2478092\n",
      "      3.6466854    0.553544    -1.0989556 ]\n",
      "   [ -0.68938744   0.7849126    0.80624384  -0.06641847   0.84722805\n",
      "      1.4088807    0.84816337  -0.6531029 ]\n",
      "   [  0.3542869   -2.6495297   -3.9148326   -2.3332536   -4.0371146\n",
      "     -2.1969824   -0.3534847    0.32806468]\n",
      "   [  1.7841424   -3.3586555   -5.717826    -3.0791197   -6.833018\n",
      "     -4.7764482   -0.9208296    1.659839  ]\n",
      "   [  2.1771913   -5.3258214   -6.7333293   -4.437318    -7.995056\n",
      "     -7.0693007   -1.9548045    2.037586  ]\n",
      "   [  1.3977351   -3.5937362   -4.3651676   -2.8519344   -4.788908\n",
      "     -3.8655014   -1.1588197    1.3249018 ]]\n",
      "\n",
      "  [[ -0.6709214    1.5035548    1.64232     -0.33530334   0.9649067\n",
      "      0.04905168  -0.89175946  -0.71355253]\n",
      "   [  0.24583115   0.6569769   -0.621118     1.2669846    0.91026103\n",
      "      1.4697332    1.5297475    0.26090285]\n",
      "   [  0.9396693    1.0186071    0.9804496    3.2190533    1.4295805\n",
      "      2.6770215    2.6240253    0.95563656]\n",
      "   [  0.6117326   -0.6111772   -0.8288269    0.2871933   -0.67439234\n",
      "     -0.41122133   0.9504747    0.6410048 ]\n",
      "   [ -0.64481556   2.1225655    2.2653017    1.3190256    3.896422\n",
      "      2.724441     1.7529845   -0.6356979 ]\n",
      "   [  1.0155843   -3.0330362   -2.686433    -0.8795187   -2.3274312\n",
      "     -2.6010084    0.20283478   1.0291578 ]]\n",
      "\n",
      "  [[ -0.5303791   -2.110146    -1.3960739   -2.3525817   -1.9407862\n",
      "     -0.8966946   -2.0528047   -0.50565517]\n",
      "   [ -0.09481095  -1.1549708    0.31408975  -0.03525389  -0.4600581\n",
      "     -1.6961929    0.7154125   -0.0880874 ]\n",
      "   [  0.5121612   -0.1614938    0.4048692    0.8203044    0.5385774\n",
      "     -0.67759335   1.620004     0.4480935 ]\n",
      "   [  0.47279775   0.89751595   0.56361896   1.2335601    0.97617674\n",
      "      0.72556263   0.7255807    0.45003852]\n",
      "   [  0.77628833   1.3479958    0.6695447    1.9411157    1.0075263\n",
      "      0.5872247    1.8186214    0.7564151 ]\n",
      "   [  0.71668166  -0.7621067   -0.19866621   0.5315981   -0.59144294\n",
      "     -0.32088113   1.7715988    0.7526218 ]]\n",
      "\n",
      "  [[ -1.6133933    5.655559     8.456806     4.720569     6.469103\n",
      "      6.3866005   -0.77200097  -1.6862562 ]\n",
      "   [  2.6422656   -3.0630863   -6.9324884   -2.84115     -4.448012\n",
      "     -5.5646486    1.8054792    2.638062  ]\n",
      "   [  2.5069184   -6.87229    -10.355062    -4.8297977   -8.231734\n",
      "     -8.594013     0.42899096   2.5198808 ]\n",
      "   [  2.0609508   -6.442012    -9.504503    -4.5295987   -6.941006\n",
      "     -7.39375      0.4449742    2.1119938 ]\n",
      "   [  1.6776993   -6.645504    -9.133078    -4.668825    -7.531585\n",
      "     -7.6619554    0.03244972   1.7574762 ]\n",
      "   [  2.6513484   -5.3852997   -7.2641253   -3.7382479   -5.618508\n",
      "     -5.7422824    0.72726774   2.6885734 ]]]], shape=(1, 8, 6, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 8)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "Input: 날씨가 좋넹.\n",
      "Output: 좋은 사람이 찾아오려나봐요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"날씨가 좋넹.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 1)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106 ]]\n",
      "\n",
      "  [[-0.1123142 ]]\n",
      "\n",
      "  [[-0.10918517]]\n",
      "\n",
      "  [[-0.22114104]]\n",
      "\n",
      "  [[ 0.00076126]]\n",
      "\n",
      "  [[-0.00999826]]\n",
      "\n",
      "  [[-0.20863976]]\n",
      "\n",
      "  [[-0.15726982]]]], shape=(1, 8, 1, 1), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 1)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.6510173  -2.8665452  -1.9706712   0.6258339 ]]\n",
      "\n",
      "  [[ 1.5699102  -4.8338065  -2.8935153   1.5974954 ]]\n",
      "\n",
      "  [[-0.9479617  -1.5584124  -2.3440182  -0.91537315]]\n",
      "\n",
      "  [[-0.53032047 -2.1487055  -3.3869808  -0.5465881 ]]\n",
      "\n",
      "  [[-1.8786137   3.8044016   1.7851309  -1.9308672 ]]\n",
      "\n",
      "  [[ 1.8587611  -2.3004422  -1.999603    1.8540798 ]]\n",
      "\n",
      "  [[ 0.263768    1.3027129   0.7912128   0.31364933]]\n",
      "\n",
      "  [[-0.50981736  4.1422954   3.6196024  -0.43590528]]]], shape=(1, 8, 1, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 4)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 1)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344 ]]\n",
      "\n",
      "  [[-0.16619189]]\n",
      "\n",
      "  [[-3.0174336 ]]\n",
      "\n",
      "  [[-3.0523906 ]]\n",
      "\n",
      "  [[-0.6331823 ]]\n",
      "\n",
      "  [[-1.2599558 ]]\n",
      "\n",
      "  [[-2.1587913 ]]\n",
      "\n",
      "  [[-2.0107505 ]]]], shape=(1, 8, 1, 1), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 1)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.5176607   1.9687198   2.0989769  -0.47190297]]\n",
      "\n",
      "  [[-0.5409756   2.8193953   2.0520196  -0.5236139 ]]\n",
      "\n",
      "  [[-0.42360795  3.4683292   3.2733347  -0.39263082]]\n",
      "\n",
      "  [[-1.6453142   4.41468     3.7185764  -1.6388824 ]]\n",
      "\n",
      "  [[-1.29121     2.8940606   1.9502285  -1.3604355 ]]\n",
      "\n",
      "  [[-0.65288585  3.4191878   2.6229937  -0.6571499 ]]\n",
      "\n",
      "  [[-0.13920632  1.920276    1.8218281  -0.1018336 ]]\n",
      "\n",
      "  [[-1.5968649   5.382058    4.5500836  -1.5562288 ]]]], shape=(1, 8, 1, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 4)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 2)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.34484416]\n",
      "   [-1.5524524   0.57553977]]\n",
      "\n",
      "  [[-0.1123142   0.09086403]\n",
      "   [-0.27778682 -7.685327  ]]\n",
      "\n",
      "  [[-0.10918517 -0.21059825]\n",
      "   [ 0.2860052  -5.063574  ]]\n",
      "\n",
      "  [[-0.22114104 -0.3915212 ]\n",
      "   [ 0.14206643  1.1752353 ]]\n",
      "\n",
      "  [[ 0.00076126 -0.1641996 ]\n",
      "   [ 0.78445697 -5.048575  ]]\n",
      "\n",
      "  [[-0.00999826 -0.5171405 ]\n",
      "   [ 0.3875343   2.307618  ]]\n",
      "\n",
      "  [[-0.20863976  0.0058533 ]\n",
      "   [-0.5011563  -5.83805   ]]\n",
      "\n",
      "  [[-0.15726982  0.34343007]\n",
      "   [ 0.42748785 -3.541775  ]]]], shape=(1, 8, 2, 2), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 2)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.6510173  -2.8665452  -1.9706712   0.6258339 ]\n",
      "   [-1.2761326  -0.6591419  -0.772127   -1.2929033 ]]\n",
      "\n",
      "  [[ 1.5699102  -4.8338065  -2.8935153   1.5974954 ]\n",
      "   [ 1.602976   -6.0585413  -4.3626466   1.6033887 ]]\n",
      "\n",
      "  [[-0.9479617  -1.5584124  -2.3440182  -0.91537315]\n",
      "   [ 1.1684959  -6.083402   -5.361126    1.1414931 ]]\n",
      "\n",
      "  [[-0.53032047 -2.1487055  -3.3869808  -0.5465881 ]\n",
      "   [-0.37673792  0.5297353   0.7943683  -0.41485333]]\n",
      "\n",
      "  [[-1.8786137   3.8044016   1.7851309  -1.9308672 ]\n",
      "   [-2.0427704   1.9088362   0.8611902  -2.10597   ]]\n",
      "\n",
      "  [[ 1.8587611  -2.3004422  -1.999603    1.8540798 ]\n",
      "   [ 0.62134135 -1.1685245  -1.0336286   0.63556284]]\n",
      "\n",
      "  [[ 0.263768    1.3027129   0.7912128   0.31364933]\n",
      "   [ 0.90733486 -1.2863759  -0.23643312  0.92466456]]\n",
      "\n",
      "  [[-0.50981736  4.1422954   3.6196024  -0.43590528]\n",
      "   [ 0.4603232  -1.0371166  -0.40937394  0.4394898 ]]]], shape=(1, 8, 2, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 4)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 2)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181]\n",
      "   [ 3.96568    -0.17723085]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489 ]\n",
      "   [ 0.22428446 -2.402899  ]]\n",
      "\n",
      "  [[-3.0174336   0.41961712]\n",
      "   [-1.3549727  -0.09280814]]\n",
      "\n",
      "  [[-3.0523906   1.8179893 ]\n",
      "   [-7.7598214  -1.2395704 ]]\n",
      "\n",
      "  [[-0.6331823  -1.2349904 ]\n",
      "   [ 2.119253   -4.246382  ]]\n",
      "\n",
      "  [[-1.2599558   1.0058122 ]\n",
      "   [ 1.9198732  -0.84344107]]\n",
      "\n",
      "  [[-2.1587913   1.7147038 ]\n",
      "   [-0.00582398 -2.3462925 ]]\n",
      "\n",
      "  [[-2.0107505   2.0400176 ]\n",
      "   [-1.2814682  -2.6879327 ]]]], shape=(1, 8, 2, 2), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 2)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.51766056  1.96872     2.098977   -0.47190288]\n",
      "   [-0.49124843  0.08534324  0.2991765  -0.48528743]]\n",
      "\n",
      "  [[-0.5409756   2.819395    2.0520191  -0.5236139 ]\n",
      "   [-1.0098997   4.226584    3.5640724  -0.9722929 ]]\n",
      "\n",
      "  [[-0.42360795  3.468329    3.2733347  -0.39263085]\n",
      "   [-0.47245097  0.7355631   1.3526613  -0.47366753]]\n",
      "\n",
      "  [[-1.6453142   4.41468     3.7185767  -1.6388824 ]\n",
      "   [ 0.1504475  -1.3770971  -0.84712774  0.10965424]]\n",
      "\n",
      "  [[-1.29121     2.8940609   1.9502283  -1.3604355 ]\n",
      "   [ 0.00294916 -1.2568096  -0.89668256 -0.00931877]]\n",
      "\n",
      "  [[-0.65288585  3.419188    2.6229937  -0.65715   ]\n",
      "   [ 0.72896236 -2.1471467  -1.9732635   0.77281165]]\n",
      "\n",
      "  [[-0.13920635  1.9202758   1.8218281  -0.10183363]\n",
      "   [ 0.19900118 -1.852921   -1.364047    0.17529728]]\n",
      "\n",
      "  [[-1.5968649   5.382058    4.5500836  -1.5562288 ]\n",
      "   [ 0.86753845 -4.5697775  -4.1758018   0.8370436 ]]]], shape=(1, 8, 2, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 4)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 3)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.34484416  0.3515555 ]\n",
      "   [-1.5524524   0.57553977  1.342203  ]\n",
      "   [ 0.17656448 -2.1603198  -3.1747456 ]]\n",
      "\n",
      "  [[-0.1123142   0.09086403 -0.2228786 ]\n",
      "   [-0.27778682 -7.685327   -0.20150177]\n",
      "   [ 0.30459622 -1.6710656  -3.094567  ]]\n",
      "\n",
      "  [[-0.10918517 -0.21059825  0.3513798 ]\n",
      "   [ 0.2860052  -5.063574   -2.1691    ]\n",
      "   [ 0.6062982   2.719351   -4.9581633 ]]\n",
      "\n",
      "  [[-0.22114104 -0.3915212   0.13786234]\n",
      "   [ 0.14206643  1.1752353   3.6785405 ]\n",
      "   [-0.07449952 -3.5981474  -0.31350046]]\n",
      "\n",
      "  [[ 0.00076126 -0.1641996  -0.14196222]\n",
      "   [ 0.78445697 -5.048575    3.2536209 ]\n",
      "   [ 0.2599421  -2.3611805   4.198083  ]]\n",
      "\n",
      "  [[-0.00999826 -0.5171405   0.3842402 ]\n",
      "   [ 0.3875343   2.307618   -2.0652568 ]\n",
      "   [ 0.5494998   0.9012554  -5.717646  ]]\n",
      "\n",
      "  [[-0.20863976  0.0058533   0.141982  ]\n",
      "   [-0.5011563  -5.83805     3.0883057 ]\n",
      "   [-0.08541125  2.1106565   0.02025323]]\n",
      "\n",
      "  [[-0.15726982  0.34343007 -0.0106815 ]\n",
      "   [ 0.42748785 -3.541775   -4.416529  ]\n",
      "   [ 0.09384713  3.2008502  -4.3988295 ]]]], shape=(1, 8, 3, 3), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 3)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.6510173  -2.8665452  -1.9706712   0.6258339 ]\n",
      "   [-1.2761326  -0.6591419  -0.772127   -1.2929033 ]\n",
      "   [ 1.0074586  -3.2963974  -3.106445    0.9872349 ]]\n",
      "\n",
      "  [[ 1.5699102  -4.8338065  -2.8935153   1.5974954 ]\n",
      "   [ 1.602976   -6.0585413  -4.3626466   1.6033887 ]\n",
      "   [ 0.84273714 -3.3321726  -3.019569    0.8300638 ]]\n",
      "\n",
      "  [[-0.9479617  -1.5584124  -2.3440182  -0.91537315]\n",
      "   [ 1.1684959  -6.083402   -5.361126    1.1414931 ]\n",
      "   [ 0.8458585  -8.91149    -7.7787905   0.78298134]]\n",
      "\n",
      "  [[-0.53032047 -2.1487055  -3.3869808  -0.5465881 ]\n",
      "   [-0.37673792  0.5297353   0.7943683  -0.41485333]\n",
      "   [ 1.2794939  -4.2864437  -4.2701783   1.2569122 ]]\n",
      "\n",
      "  [[-1.8786137   3.8044016   1.7851309  -1.9308672 ]\n",
      "   [-2.0427704   1.9088362   0.8611902  -2.10597   ]\n",
      "   [ 0.3713629  -6.5012736  -5.5806484   0.3149101 ]]\n",
      "\n",
      "  [[ 1.8587611  -2.3004422  -1.999603    1.8540798 ]\n",
      "   [ 0.62134135 -1.1685245  -1.0336286   0.63556284]\n",
      "   [ 0.93438053 -3.9921112  -3.1974504   0.91454566]]\n",
      "\n",
      "  [[ 0.263768    1.3027129   0.7912128   0.31364933]\n",
      "   [ 0.90733486 -1.2863759  -0.23643312  0.92466456]\n",
      "   [ 1.9193958  -5.6561565  -3.624307    1.9176353 ]]\n",
      "\n",
      "  [[-0.50981736  4.1422954   3.6196024  -0.43590528]\n",
      "   [ 0.4603232  -1.0371166  -0.40937394  0.4394898 ]\n",
      "   [ 1.2603666  -3.7127948  -2.6582818   1.2983314 ]]]], shape=(1, 8, 3, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 4)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "split_heads()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 3)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181 -0.6147039 ]\n",
      "   [ 3.96568    -0.17723085  1.5590783 ]\n",
      "   [ 3.9833748  -2.2539997   0.15140963]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489  -0.22415894]\n",
      "   [ 0.22428446 -2.402899   -1.7699208 ]\n",
      "   [-0.6712342   0.30168235 -3.0742364 ]]\n",
      "\n",
      "  [[-3.0174336   0.41961712  1.9252456 ]\n",
      "   [-1.3549727  -0.09280814  2.5851169 ]\n",
      "   [-1.7961591  -1.8925554  -0.41655225]]\n",
      "\n",
      "  [[-3.0523906   1.8179893   4.253165  ]\n",
      "   [-7.7598214  -1.2395704   4.424106  ]\n",
      "   [-5.33444    -2.9674113   1.9738437 ]]\n",
      "\n",
      "  [[-0.6331823  -1.2349904   0.17108005]\n",
      "   [ 2.119253   -4.246382    1.0599287 ]\n",
      "   [-1.1188248   0.14757451  1.0110303 ]]\n",
      "\n",
      "  [[-1.2599558   1.0058122   0.9403136 ]\n",
      "   [ 1.9198732  -0.84344107  2.0494642 ]\n",
      "   [ 2.8428354  -1.6724693  -1.4574461 ]]\n",
      "\n",
      "  [[-2.1587913   1.7147038   1.544837  ]\n",
      "   [-0.00582398 -2.3462925   1.799922  ]\n",
      "   [-2.1043587  -2.8178024   2.2148285 ]]\n",
      "\n",
      "  [[-2.0107505   2.0400176   1.5965837 ]\n",
      "   [-1.2814682  -2.6879327   1.4795719 ]\n",
      "   [ 0.3697977   0.11362538 -0.8682849 ]]]], shape=(1, 8, 3, 3), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 3)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.51766056  1.96872     2.098977   -0.47190288]\n",
      "   [-0.49124843  0.08534324  0.2991765  -0.48528743]\n",
      "   [ 0.24215932 -4.697081   -4.019181    0.1927161 ]]\n",
      "\n",
      "  [[-0.5409756   2.819395    2.0520191  -0.5236139 ]\n",
      "   [-1.0098997   4.226584    3.5640724  -0.9722929 ]\n",
      "   [-0.31559435 -2.7033126  -2.1731372  -0.34413084]]\n",
      "\n",
      "  [[-0.42360795  3.468329    3.2733347  -0.39263085]\n",
      "   [-0.47245097  0.7355631   1.3526613  -0.47366753]\n",
      "   [ 0.8764782  -5.054033   -4.6074905   0.832105  ]]\n",
      "\n",
      "  [[-1.6453142   4.41468     3.7185767  -1.6388824 ]\n",
      "   [ 0.1504475  -1.3770971  -0.84712774  0.10965424]\n",
      "   [ 1.5174129  -5.9414515  -4.9204936   1.4784044 ]]\n",
      "\n",
      "  [[-1.29121     2.8940609   1.9502283  -1.3604355 ]\n",
      "   [ 0.00294916 -1.2568096  -0.89668256 -0.00931877]\n",
      "   [ 0.7530518  -2.5598295  -1.9020394   0.7952453 ]]\n",
      "\n",
      "  [[-0.65288585  3.419188    2.6229937  -0.65715   ]\n",
      "   [ 0.72896236 -2.1471467  -1.9732635   0.77281165]\n",
      "   [ 0.32350188 -1.2035745  -1.7607827   0.34396917]]\n",
      "\n",
      "  [[-0.13920635  1.9202758   1.8218281  -0.10183363]\n",
      "   [ 0.19900118 -1.852921   -1.364047    0.17529728]\n",
      "   [ 0.9201308  -2.3837185  -1.2430844   0.8900646 ]]\n",
      "\n",
      "  [[-1.5968649   5.382058    4.5500836  -1.5562288 ]\n",
      "   [ 0.86753845 -4.5697775  -4.1758018   0.8370436 ]\n",
      "   [ 1.9240823  -9.450348   -8.618066    1.8469068 ]]]], shape=(1, 8, 3, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 4)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.34484416  0.3515555   0.8269838 ]\n",
      "   [-1.5524524   0.57553977  1.342203    0.7264702 ]\n",
      "   [ 0.17656448 -2.1603198  -3.1747456  -2.3878162 ]\n",
      "   [-0.27855322 -2.3594024   0.28350914 -6.3847156 ]]\n",
      "\n",
      "  [[-0.1123142   0.09086403 -0.2228786  -0.18202324]\n",
      "   [-0.27778682 -7.685327   -0.20150177  3.3823    ]\n",
      "   [ 0.30459622 -1.6710656  -3.094567   -1.6013079 ]\n",
      "   [-0.03799488 -4.1970086   2.696054   -2.7333486 ]]\n",
      "\n",
      "  [[-0.10918517 -0.21059825  0.3513798   0.34551156]\n",
      "   [ 0.2860052  -5.063574   -2.1691     -3.5214968 ]\n",
      "   [ 0.6062982   2.719351   -4.9581633  -1.0914348 ]\n",
      "   [ 0.13651355 -1.777794    3.1865156  -0.07830478]]\n",
      "\n",
      "  [[-0.22114104 -0.3915212   0.13786234  0.33271423]\n",
      "   [ 0.14206643  1.1752353   3.6785405   1.6119353 ]\n",
      "   [-0.07449952 -3.5981474  -0.31350046  1.4361463 ]\n",
      "   [-0.1049557   0.70436597  0.5221487  -0.21612936]]\n",
      "\n",
      "  [[ 0.00076126 -0.1641996  -0.14196222 -0.19410133]\n",
      "   [ 0.78445697 -5.048575    3.2536209  -0.97517645]\n",
      "   [ 0.2599421  -2.3611805   4.198083   -0.59922963]\n",
      "   [ 0.203725    3.9226558   1.8656771   0.77520037]]\n",
      "\n",
      "  [[-0.00999826 -0.5171405   0.3842402  -0.26599303]\n",
      "   [ 0.3875343   2.307618   -2.0652568   0.06119107]\n",
      "   [ 0.5494998   0.9012554  -5.717646   -0.5140254 ]\n",
      "   [-0.09110849  1.9358311  -2.1756437  -0.7149111 ]]\n",
      "\n",
      "  [[-0.20863976  0.0058533   0.141982    0.3954799 ]\n",
      "   [-0.5011563  -5.83805     3.0883057   2.4906335 ]\n",
      "   [-0.08541125  2.1106565   0.02025323  0.89557755]\n",
      "   [ 1.3930423   2.344757   -1.3677286  -6.293937  ]]\n",
      "\n",
      "  [[-0.15726982  0.34343007 -0.0106815   0.2812659 ]\n",
      "   [ 0.42748785 -3.541775   -4.416529   -1.5682534 ]\n",
      "   [ 0.09384713  3.2008502  -4.3988295  -1.0193433 ]\n",
      "   [ 0.00172196 -0.81685966  2.484785   -2.7307875 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.6510173  -2.8665452  -1.9706712   0.6258339 ]\n",
      "   [-1.2761326  -0.6591419  -0.772127   -1.2929033 ]\n",
      "   [ 1.0074586  -3.2963974  -3.106445    0.9872349 ]\n",
      "   [ 1.2579683  -4.881622   -3.767476    1.2469316 ]]\n",
      "\n",
      "  [[ 1.5699102  -4.8338065  -2.8935153   1.5974954 ]\n",
      "   [ 1.602976   -6.0585413  -4.3626466   1.6033887 ]\n",
      "   [ 0.84273714 -3.3321726  -3.019569    0.8300638 ]\n",
      "   [ 1.739568   -5.32123    -4.1527953   1.7057066 ]]\n",
      "\n",
      "  [[-0.9479617  -1.5584124  -2.3440182  -0.91537315]\n",
      "   [ 1.1684959  -6.083402   -5.361126    1.1414931 ]\n",
      "   [ 0.8458585  -8.91149    -7.7787905   0.78298134]\n",
      "   [ 2.5803494  -5.880795   -5.0921974   2.5411623 ]]\n",
      "\n",
      "  [[-0.53032047 -2.1487055  -3.3869808  -0.5465881 ]\n",
      "   [-0.37673792  0.5297353   0.7943683  -0.41485333]\n",
      "   [ 1.2794939  -4.2864437  -4.2701783   1.2569122 ]\n",
      "   [ 2.427947   -1.600872   -0.90655017  2.481265  ]]\n",
      "\n",
      "  [[-1.8786137   3.8044016   1.7851309  -1.9308672 ]\n",
      "   [-2.0427704   1.9088362   0.8611902  -2.10597   ]\n",
      "   [ 0.3713629  -6.5012736  -5.5806484   0.3149101 ]\n",
      "   [ 2.3904674  -0.8018907   0.54941446  2.4430196 ]]\n",
      "\n",
      "  [[ 1.8587611  -2.3004422  -1.999603    1.8540798 ]\n",
      "   [ 0.62134135 -1.1685245  -1.0336286   0.63556284]\n",
      "   [ 0.93438053 -3.9921112  -3.1974504   0.91454566]\n",
      "   [ 2.386284   -8.792911   -6.874719    2.3729997 ]]\n",
      "\n",
      "  [[ 0.263768    1.3027129   0.7912128   0.31364933]\n",
      "   [ 0.90733486 -1.2863759  -0.23643312  0.92466456]\n",
      "   [ 1.9193958  -5.6561565  -3.624307    1.9176353 ]\n",
      "   [ 2.590705   -7.7614427  -5.438415    2.6344485 ]]\n",
      "\n",
      "  [[-0.50981736  4.1422954   3.6196024  -0.43590528]\n",
      "   [ 0.4603232  -1.0371166  -0.40937394  0.4394898 ]\n",
      "   [ 1.2603666  -3.7127948  -2.6582818   1.2983314 ]\n",
      "   [ 1.9421246  -6.2256336  -3.6375484   1.9323288 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181 -0.6147039  -0.12000973]\n",
      "   [ 3.96568    -0.17723085  1.5590783  -1.8989185 ]\n",
      "   [ 3.9833748  -2.2539997   0.15140963 -2.319149  ]\n",
      "   [ 3.0466342  -1.2768042  -0.9543833  -2.4668362 ]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489  -0.22415894  0.44419977]\n",
      "   [ 0.22428446 -2.402899   -1.7699208  -0.35874206]\n",
      "   [-0.6712342   0.30168235 -3.0742364   0.19597065]\n",
      "   [ 1.2226754   2.9617927   2.3518116  -2.0428975 ]]\n",
      "\n",
      "  [[-3.0174336   0.41961712  1.9252456   0.7638777 ]\n",
      "   [-1.3549727  -0.09280814  2.5851169  -0.3922029 ]\n",
      "   [-1.7961591  -1.8925554  -0.41655225  2.8299882 ]\n",
      "   [-1.1154089  -1.1745822   1.2023396  -0.06076117]]\n",
      "\n",
      "  [[-3.0523906   1.8179893   4.253165    1.4998401 ]\n",
      "   [-7.7598214  -1.2395704   4.424106    5.041027  ]\n",
      "   [-5.33444    -2.9674113   1.9738437   4.328881  ]\n",
      "   [-1.8466234  -0.9457812   2.1802762   4.8156037 ]]\n",
      "\n",
      "  [[-0.6331823  -1.2349904   0.17108005 -1.7825468 ]\n",
      "   [ 2.119253   -4.246382    1.0599287  -6.814192  ]\n",
      "   [-1.1188248   0.14757451  1.0110303   1.6372967 ]\n",
      "   [ 0.95955515 -1.9993405   2.5704405  -1.7231832 ]]\n",
      "\n",
      "  [[-1.2599558   1.0058122   0.9403136  -2.169106  ]\n",
      "   [ 1.9198732  -0.84344107  2.0494642  -1.7659745 ]\n",
      "   [ 2.8428354  -1.6724693  -1.4574461  -0.75728846]\n",
      "   [ 1.2147404   1.865701   -1.4707285  -1.3224499 ]]\n",
      "\n",
      "  [[-2.1587913   1.7147038   1.544837    2.9021912 ]\n",
      "   [-0.00582398 -2.3462925   1.799922    0.32839322]\n",
      "   [-2.1043587  -2.8178024   2.2148285   3.891037  ]\n",
      "   [ 1.8959482  -3.3403587   2.0201633  -2.8513105 ]]\n",
      "\n",
      "  [[-2.0107505   2.0400176   1.5965837   0.41086924]\n",
      "   [-1.2814682  -2.6879327   1.4795719   0.70971787]\n",
      "   [ 0.3697977   0.11362538 -0.8682849   1.8676807 ]\n",
      "   [ 0.17680757 -0.59016186 -0.07212502  1.788976  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.5176607   1.9687198   2.0989769  -0.47190297]\n",
      "   [-0.49124843  0.08534324  0.2991765  -0.48528743]\n",
      "   [ 0.24215932 -4.697081   -4.019181    0.1927161 ]\n",
      "   [ 0.7396501  -3.0292149  -2.48159     0.7256521 ]]\n",
      "\n",
      "  [[-0.5409756   2.8193953   2.0520196  -0.5236139 ]\n",
      "   [-1.0098997   4.226584    3.5640724  -0.9722929 ]\n",
      "   [-0.31559435 -2.7033126  -2.1731372  -0.34413084]\n",
      "   [ 0.67385    -4.3054934  -2.5939085   0.6549963 ]]\n",
      "\n",
      "  [[-0.42360795  3.4683292   3.2733347  -0.39263082]\n",
      "   [-0.47245097  0.7355631   1.3526613  -0.47366753]\n",
      "   [ 0.8764782  -5.054033   -4.6074905   0.832105  ]\n",
      "   [ 1.5158468  -5.5036316  -5.460711    1.4665604 ]]\n",
      "\n",
      "  [[-1.6453142   4.41468     3.7185764  -1.6388824 ]\n",
      "   [ 0.1504475  -1.3770971  -0.84712774  0.10965424]\n",
      "   [ 1.5174129  -5.9414515  -4.9204936   1.4784044 ]\n",
      "   [ 1.9338356  -4.261786   -3.374979    1.932654  ]]\n",
      "\n",
      "  [[-1.29121     2.8940606   1.9502285  -1.3604355 ]\n",
      "   [ 0.00294916 -1.2568096  -0.89668256 -0.00931877]\n",
      "   [ 0.7530518  -2.5598295  -1.9020394   0.7952453 ]\n",
      "   [ 1.7065613  -5.794166   -4.5830455   1.7274754 ]]\n",
      "\n",
      "  [[-0.65288585  3.4191878   2.6229937  -0.6571499 ]\n",
      "   [ 0.72896236 -2.1471467  -1.9732635   0.77281165]\n",
      "   [ 0.32350188 -1.2035745  -1.7607827   0.34396917]\n",
      "   [ 1.0276341   1.058868    1.2416905   1.0519707 ]]\n",
      "\n",
      "  [[-0.13920632  1.920276    1.8218281  -0.1018336 ]\n",
      "   [ 0.19900118 -1.852921   -1.364047    0.17529728]\n",
      "   [ 0.9201308  -2.3837185  -1.2430844   0.8900646 ]\n",
      "   [ 1.0384122  -2.2614129  -0.6760908   1.014745  ]]\n",
      "\n",
      "  [[-1.5968649   5.382058    4.5500836  -1.5562288 ]\n",
      "   [ 0.86753845 -4.5697775  -4.1758018   0.8370436 ]\n",
      "   [ 1.9240823  -9.450348   -8.618066    1.8469068 ]\n",
      "   [ 1.4563193  -6.307143   -5.9952025   1.4192016 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.34484416  0.3515555   0.8269838  -0.1325054 ]\n",
      "   [-1.5524524   0.57553977  1.342203    0.7264702   0.67869383]\n",
      "   [ 0.17656448 -2.1603198  -3.1747456  -2.3878162  -0.21427149]\n",
      "   [-0.27855322 -2.3594024   0.28350914 -6.3847156   2.1768348 ]\n",
      "   [-0.5387738  -1.1847006  -2.5186715   3.639655   -2.1242177 ]]\n",
      "\n",
      "  [[-0.1123142   0.09086403 -0.2228786  -0.18202324  0.14455257]\n",
      "   [-0.27778682 -7.685327   -0.20150177  3.3823      3.8902612 ]\n",
      "   [ 0.30459622 -1.6710656  -3.094567   -1.6013079   0.79777056]\n",
      "   [-0.03799488 -4.1970086   2.696054   -2.7333486   1.6560552 ]\n",
      "   [ 0.6443627  -3.9118009   1.1981791  -4.1091504  -6.4226418 ]]\n",
      "\n",
      "  [[-0.10918517 -0.21059825  0.3513798   0.34551156  0.83573407]\n",
      "   [ 0.2860052  -5.063574   -2.1691     -3.5214968  -3.319986  ]\n",
      "   [ 0.6062982   2.719351   -4.9581633  -1.0914348  -1.6627623 ]\n",
      "   [ 0.13651355 -1.777794    3.1865156  -0.07830478 -1.239388  ]\n",
      "   [-0.21224323 -1.5428834   7.991749   -0.5730239   1.2063524 ]]\n",
      "\n",
      "  [[-0.22114104 -0.3915212   0.13786234  0.33271423  0.6211036 ]\n",
      "   [ 0.14206643  1.1752353   3.6785405   1.6119353  -0.9453702 ]\n",
      "   [-0.07449952 -3.5981474  -0.31350046  1.4361463   2.564271  ]\n",
      "   [-0.1049557   0.70436597  0.5221487  -0.21612936  0.9116111 ]\n",
      "   [-0.310017   -2.4498558   2.5756524   1.8039283  -0.97895104]]\n",
      "\n",
      "  [[ 0.00076126 -0.1641996  -0.14196222 -0.19410133  0.00823206]\n",
      "   [ 0.78445697 -5.048575    3.2536209  -0.97517645  0.36774525]\n",
      "   [ 0.2599421  -2.3611805   4.198083   -0.59922963 -2.4801793 ]\n",
      "   [ 0.203725    3.9226558   1.8656771   0.77520037 -3.1088407 ]\n",
      "   [ 0.6548166   0.10552962  1.3786814  -2.6986282  -3.3625326 ]]\n",
      "\n",
      "  [[-0.00999826 -0.5171405   0.3842402  -0.26599303 -0.17592163]\n",
      "   [ 0.3875343   2.307618   -2.0652568   0.06119107 -1.376965  ]\n",
      "   [ 0.5494998   0.9012554  -5.717646   -0.5140254  -1.9746116 ]\n",
      "   [-0.09110849  1.9358311  -2.1756437  -0.7149111  -1.2668245 ]\n",
      "   [-0.99071145  6.797044   -1.4787755   4.8537555  -0.8091358 ]]\n",
      "\n",
      "  [[-0.20863976  0.0058533   0.141982    0.3954799   0.70690477]\n",
      "   [-0.5011563  -5.83805     3.0883057   2.4906335   2.8664675 ]\n",
      "   [-0.08541125  2.1106565   0.02025323  0.89557755  1.3944312 ]\n",
      "   [ 1.3930423   2.344757   -1.3677286  -6.293937   -6.4745502 ]\n",
      "   [ 2.1056752  -0.87906635 -3.2462146  -5.5659313  -6.810574  ]]\n",
      "\n",
      "  [[-0.15726982  0.34343007 -0.0106815   0.2812659   0.26918355]\n",
      "   [ 0.42748785 -3.541775   -4.416529   -1.5682534  -0.30482742]\n",
      "   [ 0.09384713  3.2008502  -4.3988295  -1.0193433  -1.7561591 ]\n",
      "   [ 0.00172196 -0.81685966  2.484785   -2.7307875   4.4621997 ]\n",
      "   [-0.26568195  6.4090133   0.19427305  4.194164   -5.8646803 ]]]], shape=(1, 8, 5, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 5)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.6510173   -2.8665452   -1.9706712    0.6258339 ]\n",
      "   [ -1.2761326   -0.6591419   -0.772127    -1.2929033 ]\n",
      "   [  1.0074586   -3.2963974   -3.106445     0.9872349 ]\n",
      "   [  1.2579683   -4.881622    -3.767476     1.2469316 ]\n",
      "   [  1.3234984   -5.9344664   -5.2370906    1.2870476 ]]\n",
      "\n",
      "  [[  1.5699102   -4.8338065   -2.8935153    1.5974954 ]\n",
      "   [  1.602976    -6.0585413   -4.3626466    1.6033887 ]\n",
      "   [  0.84273714  -3.3321726   -3.019569     0.8300638 ]\n",
      "   [  1.739568    -5.32123     -4.1527953    1.7057066 ]\n",
      "   [  1.9442569   -4.8756948   -3.3264403    1.9580569 ]]\n",
      "\n",
      "  [[ -0.9479617   -1.5584124   -2.3440182   -0.91537315]\n",
      "   [  1.1684959   -6.083402    -5.361126     1.1414931 ]\n",
      "   [  0.8458585   -8.91149     -7.7787905    0.78298134]\n",
      "   [  2.5803494   -5.880795    -5.0921974    2.5411623 ]\n",
      "   [  3.3461082  -11.678779   -10.798813     3.272511  ]]\n",
      "\n",
      "  [[ -0.53032047  -2.1487055   -3.3869808   -0.5465881 ]\n",
      "   [ -0.37673792   0.5297353    0.7943683   -0.41485333]\n",
      "   [  1.2794939   -4.2864437   -4.2701783    1.2569122 ]\n",
      "   [  2.427947    -1.600872    -0.90655017   2.481265  ]\n",
      "   [  2.3424263   -7.2077713   -5.824986     2.335523  ]]\n",
      "\n",
      "  [[ -1.8786137    3.8044016    1.7851309   -1.9308672 ]\n",
      "   [ -2.0427704    1.9088362    0.8611902   -2.10597   ]\n",
      "   [  0.3713629   -6.5012736   -5.5806484    0.3149101 ]\n",
      "   [  2.3904674   -0.8018907    0.54941446   2.4430196 ]\n",
      "   [  3.2833803   -5.793328    -3.5530388    3.3556385 ]]\n",
      "\n",
      "  [[  1.8587611   -2.3004422   -1.999603     1.8540798 ]\n",
      "   [  0.62134135  -1.1685245   -1.0336286    0.63556284]\n",
      "   [  0.93438053  -3.9921112   -3.1974504    0.91454566]\n",
      "   [  2.386284    -8.792911    -6.874719     2.3729997 ]\n",
      "   [  2.8132436  -11.001211    -8.411129     2.7784264 ]]\n",
      "\n",
      "  [[  0.263768     1.3027129    0.7912128    0.31364933]\n",
      "   [  0.90733486  -1.2863759   -0.23643312   0.92466456]\n",
      "   [  1.9193958   -5.6561565   -3.624307     1.9176353 ]\n",
      "   [  2.590705    -7.7614427   -5.438415     2.6344485 ]\n",
      "   [  2.0141904   -6.001995    -3.7005918    2.02994   ]]\n",
      "\n",
      "  [[ -0.50981736   4.1422954    3.6196024   -0.43590528]\n",
      "   [  0.4603232   -1.0371166   -0.40937394   0.4394898 ]\n",
      "   [  1.2603666   -3.7127948   -2.6582818    1.2983314 ]\n",
      "   [  1.9421246   -6.2256336   -3.6375484    1.9323288 ]\n",
      "   [  1.3231326   -4.248828    -3.1260052    1.3073777 ]]]], shape=(1, 8, 5, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 4)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181 -0.6147039  -0.12000973 -0.26990378]\n",
      "   [ 3.96568    -0.17723085  1.5590783  -1.8989185  -1.7120768 ]\n",
      "   [ 3.9833748  -2.2539997   0.15140963 -2.319149    1.4055393 ]\n",
      "   [ 3.0466342  -1.2768042  -0.9543833  -2.4668362  -3.578708  ]\n",
      "   [ 4.7013025   1.1783674   1.1767476  -4.7330627  -2.6853023 ]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489  -0.22415894  0.44419977  0.86740375]\n",
      "   [ 0.22428446 -2.402899   -1.7699208  -0.35874206 -2.1430826 ]\n",
      "   [-0.6712342   0.30168235 -3.0742364   0.19597065 -3.3459918 ]\n",
      "   [ 1.2226754   2.9617927   2.3518116  -2.0428975   1.0853696 ]\n",
      "   [ 3.3925583   4.3809915   0.44789767 -3.760937   -1.4172314 ]]\n",
      "\n",
      "  [[-3.0174336   0.41961712  1.9252456   0.7638777   1.4724514 ]\n",
      "   [-1.3549727  -0.09280814  2.5851169  -0.3922029  -0.2696607 ]\n",
      "   [-1.7961591  -1.8925554  -0.41655225  2.8299882   0.4466289 ]\n",
      "   [-1.1154089  -1.1745822   1.2023396  -0.06076117 -1.5096283 ]\n",
      "   [-0.09934417  0.8952499  -0.6025727  -0.7601258  -4.950531  ]]\n",
      "\n",
      "  [[-3.0523906   1.8179893   4.253165    1.4998401   2.628079  ]\n",
      "   [-7.7598214  -1.2395704   4.424106    5.041027    4.0303817 ]\n",
      "   [-5.33444    -2.9674113   1.9738437   4.328881    4.872642  ]\n",
      "   [-1.8466234  -0.9457812   2.1802762   4.8156037   0.12329892]\n",
      "   [-3.3392012  -4.19972    -0.9612767   6.551047    3.2264879 ]]\n",
      "\n",
      "  [[-0.6331823  -1.2349904   0.17108005 -1.7825468   0.4058478 ]\n",
      "   [ 2.119253   -4.246382    1.0599287  -6.814192   -1.6555526 ]\n",
      "   [-1.1188248   0.14757451  1.0110303   1.6372967   0.67021793]\n",
      "   [ 0.95955515 -1.9993405   2.5704405  -1.7231832  -0.62369555]\n",
      "   [ 0.49821588 -1.729508    2.970458   -3.6262503  -2.1428034 ]]\n",
      "\n",
      "  [[-1.2599558   1.0058122   0.9403136  -2.169106    1.3649019 ]\n",
      "   [ 1.9198732  -0.84344107  2.0494642  -1.7659745   1.8153642 ]\n",
      "   [ 2.8428354  -1.6724693  -1.4574461  -0.75728846 -2.8203032 ]\n",
      "   [ 1.2147404   1.865701   -1.4707285  -1.3224499  -2.9885063 ]\n",
      "   [ 2.2408023   0.45182097  0.23735748 -3.630287   -4.9238787 ]]\n",
      "\n",
      "  [[-2.1587913   1.7147038   1.544837    2.9021912  -0.54687065]\n",
      "   [-0.00582398 -2.3462925   1.799922    0.32839322  0.3320445 ]\n",
      "   [-2.1043587  -2.8178024   2.2148285   3.891037    2.2759385 ]\n",
      "   [ 1.8959482  -3.3403587   2.0201633  -2.8513105   2.2833853 ]\n",
      "   [ 1.2664658  -1.2141082   0.86169004 -0.82817227  1.8247795 ]]\n",
      "\n",
      "  [[-2.0107505   2.0400176   1.5965837   0.41086924  2.151286  ]\n",
      "   [-1.2814682  -2.6879327   1.4795719   0.70971787  1.2221687 ]\n",
      "   [ 0.3697977   0.11362538 -0.8682849   1.8676807   1.5285181 ]\n",
      "   [ 0.17680757 -0.59016186 -0.07212502  1.788976    1.8290929 ]\n",
      "   [ 0.27183303 -2.8065841  -0.40462255 -2.3706849  -0.47989556]]]], shape=(1, 8, 5, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 5)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.5176607   1.9687198   2.0989769  -0.47190297]\n",
      "   [-0.49124843  0.08534324  0.2991765  -0.48528743]\n",
      "   [ 0.24215932 -4.697081   -4.019181    0.1927161 ]\n",
      "   [ 0.7396501  -3.0292149  -2.48159     0.7256521 ]\n",
      "   [ 0.7377838  -1.2575825  -1.0456651   0.7396505 ]]\n",
      "\n",
      "  [[-0.5409756   2.8193953   2.0520196  -0.5236139 ]\n",
      "   [-1.0098997   4.226584    3.5640724  -0.9722929 ]\n",
      "   [-0.31559435 -2.7033126  -2.1731372  -0.34413084]\n",
      "   [ 0.67385    -4.3054934  -2.5939085   0.6549963 ]\n",
      "   [ 0.7716512  -4.6774607  -3.4361272   0.73228556]]\n",
      "\n",
      "  [[-0.42360795  3.4683292   3.2733347  -0.39263082]\n",
      "   [-0.47245097  0.7355631   1.3526613  -0.47366753]\n",
      "   [ 0.8764782  -5.054033   -4.6074905   0.832105  ]\n",
      "   [ 1.5158468  -5.5036316  -5.460711    1.4665604 ]\n",
      "   [ 1.3198522  -4.7758727  -4.838473    1.2815847 ]]\n",
      "\n",
      "  [[-1.6453142   4.41468     3.7185764  -1.6388824 ]\n",
      "   [ 0.1504475  -1.3770971  -0.84712774  0.10965424]\n",
      "   [ 1.5174129  -5.9414515  -4.9204936   1.4784044 ]\n",
      "   [ 1.9338356  -4.261786   -3.374979    1.932654  ]\n",
      "   [ 1.5312709  -4.5473847  -3.4884474   1.480693  ]]\n",
      "\n",
      "  [[-1.29121     2.8940606   1.9502285  -1.3604355 ]\n",
      "   [ 0.00294916 -1.2568096  -0.89668256 -0.00931877]\n",
      "   [ 0.7530518  -2.5598295  -1.9020394   0.7952453 ]\n",
      "   [ 1.7065613  -5.794166   -4.5830455   1.7274754 ]\n",
      "   [ 1.828848   -7.046152   -5.713792    1.8388789 ]]\n",
      "\n",
      "  [[-0.65288585  3.4191878   2.6229937  -0.6571499 ]\n",
      "   [ 0.72896236 -2.1471467  -1.9732635   0.77281165]\n",
      "   [ 0.32350188 -1.2035745  -1.7607827   0.34396917]\n",
      "   [ 1.0276341   1.058868    1.2416905   1.0519707 ]\n",
      "   [ 1.6617185  -1.5399216  -1.5241555   1.7414258 ]]\n",
      "\n",
      "  [[-0.13920632  1.920276    1.8218281  -0.1018336 ]\n",
      "   [ 0.19900118 -1.852921   -1.364047    0.17529728]\n",
      "   [ 0.9201308  -2.3837185  -1.2430844   0.8900646 ]\n",
      "   [ 1.0384122  -2.2614129  -0.6760908   1.014745  ]\n",
      "   [ 1.3033148  -4.046018   -2.5787446   1.2661915 ]]\n",
      "\n",
      "  [[-1.5968649   5.382058    4.5500836  -1.5562288 ]\n",
      "   [ 0.86753845 -4.5697775  -4.1758018   0.8370436 ]\n",
      "   [ 1.9240823  -9.450348   -8.618066    1.8469068 ]\n",
      "   [ 1.4563193  -6.307143   -5.9952025   1.4192016 ]\n",
      "   [ 1.9546863  -5.530293   -5.110774    1.9280047 ]]]], shape=(1, 8, 5, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 4)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    0.34484416   0.3515555    0.8269838   -0.1325054\n",
      "      1.1541476 ]\n",
      "   [ -1.5524524    0.57553977   1.342203     0.7264702    0.67869383\n",
      "     -2.3639412 ]\n",
      "   [  0.17656448  -2.1603198   -3.1747456   -2.3878162   -0.21427149\n",
      "     -2.5600047 ]\n",
      "   [ -0.27855322  -2.3594024    0.28350914  -6.3847156    2.1768348\n",
      "      0.48724338]\n",
      "   [ -0.5387738   -1.1847006   -2.5186715    3.639655    -2.1242177\n",
      "      8.928502  ]\n",
      "   [ -0.30547413  -1.4707135    0.06941595   0.72953105   1.0991143\n",
      "      2.2755632 ]]\n",
      "\n",
      "  [[ -0.1123142    0.09086403  -0.2228786   -0.18202324   0.14455257\n",
      "     -0.04013486]\n",
      "   [ -0.27778682  -7.685327    -0.20150177   3.3823       3.8902612\n",
      "      4.337893  ]\n",
      "   [  0.30459622  -1.6710656   -3.094567    -1.6013079    0.79777056\n",
      "      0.13481723]\n",
      "   [ -0.03799488  -4.1970086    2.696054    -2.7333486    1.6560552\n",
      "      4.530645  ]\n",
      "   [  0.6443627   -3.9118009    1.1981791   -4.1091504   -6.4226418\n",
      "     -0.9858088 ]\n",
      "   [  0.82783276  -3.7707963   -2.1052582   -2.503423    -4.9312515\n",
      "     -4.1483335 ]]\n",
      "\n",
      "  [[ -0.10918517  -0.21059825   0.3513798    0.34551156   0.83573407\n",
      "      0.38043547]\n",
      "   [  0.2860052   -5.063574    -2.1691      -3.5214968   -3.319986\n",
      "      1.6807239 ]\n",
      "   [  0.6062982    2.719351    -4.9581633   -1.0914348   -1.6627623\n",
      "     -0.31062695]\n",
      "   [  0.13651355  -1.777794     3.1865156   -0.07830478  -1.239388\n",
      "      1.7914739 ]\n",
      "   [ -0.21224323  -1.5428834    7.991749    -0.5730239    1.2063524\n",
      "      3.6277044 ]\n",
      "   [ -0.85228556  -2.2769668    6.909945     3.543675     3.6550546\n",
      "      9.817229  ]]\n",
      "\n",
      "  [[ -0.22114104  -0.3915212    0.13786234   0.33271423   0.6211036\n",
      "      1.0825356 ]\n",
      "   [  0.14206643   1.1752353    3.6785405    1.6119353   -0.9453702\n",
      "     -3.2077594 ]\n",
      "   [ -0.07449952  -3.5981474   -0.31350046   1.4361463    2.564271\n",
      "      2.474412  ]\n",
      "   [ -0.1049557    0.70436597   0.5221487   -0.21612936   0.9116111\n",
      "     -0.48090836]\n",
      "   [ -0.310017    -2.4498558    2.5756524    1.8039283   -0.97895104\n",
      "      3.0719118 ]\n",
      "   [ -0.39245275  -5.124641     3.6571612    1.2725205    3.1442227\n",
      "      0.02843648]]\n",
      "\n",
      "  [[  0.00076126  -0.1641996   -0.14196222  -0.19410133   0.00823206\n",
      "      0.61542094]\n",
      "   [  0.78445697  -5.048575     3.2536209   -0.97517645   0.36774525\n",
      "     -3.194747  ]\n",
      "   [  0.2599421   -2.3611805    4.198083    -0.59922963  -2.4801793\n",
      "     -5.2311    ]\n",
      "   [  0.203725     3.9226558    1.8656771    0.77520037  -3.1088407\n",
      "     -2.945639  ]\n",
      "   [  0.6548166    0.10552962   1.3786814   -2.6986282   -3.3625326\n",
      "     -3.5189183 ]\n",
      "   [  0.85031134  -2.967739     2.643215    -0.4758638    1.1578159\n",
      "     -8.628843  ]]\n",
      "\n",
      "  [[ -0.00999826  -0.5171405    0.3842402   -0.26599303  -0.17592163\n",
      "      0.08763131]\n",
      "   [  0.3875343    2.307618    -2.0652568    0.06119107  -1.376965\n",
      "     -0.84213823]\n",
      "   [  0.5494998    0.9012554   -5.717646    -0.5140254   -1.9746116\n",
      "     -4.1329675 ]\n",
      "   [ -0.09110849   1.9358311   -2.1756437   -0.7149111   -1.2668245\n",
      "      0.23427002]\n",
      "   [ -0.99071145   6.797044    -1.4787755    4.8537555   -0.8091358\n",
      "      1.0001513 ]\n",
      "   [  0.1653099   -0.6100134    4.8626056    0.720647     0.9629525\n",
      "     -1.3203751 ]]\n",
      "\n",
      "  [[ -0.20863976   0.0058533    0.141982     0.3954799    0.70690477\n",
      "      0.8365328 ]\n",
      "   [ -0.5011563   -5.83805      3.0883057    2.4906335    2.8664675\n",
      "      3.39397   ]\n",
      "   [ -0.08541125   2.1106565    0.02025323   0.89557755   1.3944312\n",
      "      4.8944817 ]\n",
      "   [  1.3930423    2.344757    -1.3677286   -6.293937    -6.4745502\n",
      "     -5.8228016 ]\n",
      "   [  2.1056752   -0.87906635  -3.2462146   -5.5659313   -6.810574\n",
      "    -12.417996  ]\n",
      "   [  0.7307259   -0.62191457   2.2546957   -4.717381    -0.30207893\n",
      "     -5.2433743 ]]\n",
      "\n",
      "  [[ -0.15726982   0.34343007  -0.0106815    0.2812659    0.26918355\n",
      "      0.6895108 ]\n",
      "   [  0.42748785  -3.541775    -4.416529    -1.5682534   -0.30482742\n",
      "      0.77651656]\n",
      "   [  0.09384713   3.2008502   -4.3988295   -1.0193433   -1.7561591\n",
      "     -0.20635648]\n",
      "   [  0.00172196  -0.81685966   2.484785    -2.7307875    4.4621997\n",
      "      2.2167425 ]\n",
      "   [ -0.26568195   6.4090133    0.19427305   4.194164    -5.8646803\n",
      "      0.7492719 ]\n",
      "   [  0.15044671  -0.83800465   5.2889094    3.357029     2.2839093\n",
      "     -3.226359  ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.6510173   -2.8665452   -1.9706712    0.6258339 ]\n",
      "   [ -1.2761326   -0.6591419   -0.772127    -1.2929033 ]\n",
      "   [  1.0074586   -3.2963974   -3.106445     0.9872349 ]\n",
      "   [  1.2579683   -4.881622    -3.767476     1.2469316 ]\n",
      "   [  1.3234984   -5.9344664   -5.2370906    1.2870476 ]\n",
      "   [  1.7000594   -3.6593008   -2.971424     1.7176253 ]]\n",
      "\n",
      "  [[  1.5699102   -4.8338065   -2.8935153    1.5974954 ]\n",
      "   [  1.602976    -6.0585413   -4.3626466    1.6033887 ]\n",
      "   [  0.84273714  -3.3321726   -3.019569     0.8300638 ]\n",
      "   [  1.739568    -5.32123     -4.1527953    1.7057066 ]\n",
      "   [  1.9442569   -4.8756948   -3.3264403    1.9580569 ]\n",
      "   [  3.9294844  -11.821626    -9.099886     3.8940978 ]]\n",
      "\n",
      "  [[ -0.9479617   -1.5584124   -2.3440182   -0.91537315]\n",
      "   [  1.1684959   -6.083402    -5.361126     1.1414931 ]\n",
      "   [  0.8458585   -8.91149     -7.7787905    0.78298134]\n",
      "   [  2.5803494   -5.880795    -5.0921974    2.5411623 ]\n",
      "   [  3.3461082  -11.678779   -10.798813     3.272511  ]\n",
      "   [  3.3453295   -7.0702214   -5.903679     3.2953756 ]]\n",
      "\n",
      "  [[ -0.53032047  -2.1487055   -3.3869808   -0.5465881 ]\n",
      "   [ -0.37673792   0.5297353    0.7943683   -0.41485333]\n",
      "   [  1.2794939   -4.2864437   -4.2701783    1.2569122 ]\n",
      "   [  2.427947    -1.600872    -0.90655017   2.481265  ]\n",
      "   [  2.3424263   -7.2077713   -5.824986     2.335523  ]\n",
      "   [  1.7222499   -2.7512228   -2.3087296    1.7864217 ]]\n",
      "\n",
      "  [[ -1.8786137    3.8044016    1.7851309   -1.9308672 ]\n",
      "   [ -2.0427704    1.9088362    0.8611902   -2.10597   ]\n",
      "   [  0.3713629   -6.5012736   -5.5806484    0.3149101 ]\n",
      "   [  2.3904674   -0.8018907    0.54941446   2.4430196 ]\n",
      "   [  3.2833803   -5.793328    -3.5530388    3.3556385 ]\n",
      "   [  4.031575    -8.23709     -5.352422     4.1288157 ]]\n",
      "\n",
      "  [[  1.8587611   -2.3004422   -1.999603     1.8540798 ]\n",
      "   [  0.62134135  -1.1685245   -1.0336286    0.63556284]\n",
      "   [  0.93438053  -3.9921112   -3.1974504    0.91454566]\n",
      "   [  2.386284    -8.792911    -6.874719     2.3729997 ]\n",
      "   [  2.8132436  -11.001211    -8.411129     2.7784264 ]\n",
      "   [  2.7941167   -3.5710614   -2.4098222    2.8085887 ]]\n",
      "\n",
      "  [[  0.263768     1.3027129    0.7912128    0.31364933]\n",
      "   [  0.90733486  -1.2863759   -0.23643312   0.92466456]\n",
      "   [  1.9193958   -5.6561565   -3.624307     1.9176353 ]\n",
      "   [  2.590705    -7.7614427   -5.438415     2.6344485 ]\n",
      "   [  2.0141904   -6.001995    -3.7005918    2.02994   ]\n",
      "   [  2.3413384   -2.5982022   -0.645182     2.369621  ]]\n",
      "\n",
      "  [[ -0.50981736   4.1422954    3.6196024   -0.43590528]\n",
      "   [  0.4603232   -1.0371166   -0.40937394   0.4394898 ]\n",
      "   [  1.2603666   -3.7127948   -2.6582818    1.2983314 ]\n",
      "   [  1.9421246   -6.2256336   -3.6375484    1.9323288 ]\n",
      "   [  1.3231326   -4.248828    -3.1260052    1.3073777 ]\n",
      "   [  2.442132    -7.4952087   -5.2700887    2.4572377 ]]]], shape=(1, 8, 6, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 4)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181 -0.6147039  -0.12000973 -0.26990378\n",
      "    -0.12601104]\n",
      "   [ 3.96568    -0.17723085  1.5590783  -1.8989185  -1.7120768\n",
      "    -2.4874895 ]\n",
      "   [ 3.9833748  -2.2539997   0.15140963 -2.319149    1.4055393\n",
      "     1.8976921 ]\n",
      "   [ 3.0466342  -1.2768042  -0.9543833  -2.4668362  -3.578708\n",
      "    -2.1999793 ]\n",
      "   [ 4.7013025   1.1783674   1.1767476  -4.7330627  -2.6853023\n",
      "    -3.841537  ]\n",
      "   [ 5.391753   -1.2895486   1.732423   -3.715419   -2.8239722\n",
      "    -3.662196  ]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489  -0.22415894  0.44419977  0.86740375\n",
      "    -1.3531986 ]\n",
      "   [ 0.22428446 -2.402899   -1.7699208  -0.35874206 -2.1430826\n",
      "    -0.03102632]\n",
      "   [-0.6712342   0.30168235 -3.0742364   0.19597065 -3.3459918\n",
      "    -0.10913967]\n",
      "   [ 1.2226754   2.9617927   2.3518116  -2.0428975   1.0853696\n",
      "    -0.76767504]\n",
      "   [ 3.3925583   4.3809915   0.44789767 -3.760937   -1.4172314\n",
      "    -2.3714716 ]\n",
      "   [-2.3619554  -2.1506267  -0.96027523  3.8960817   1.8534632\n",
      "    -0.74298   ]]\n",
      "\n",
      "  [[-3.0174336   0.41961712  1.9252456   0.7638777   1.4724514\n",
      "     3.0431707 ]\n",
      "   [-1.3549727  -0.09280814  2.5851169  -0.3922029  -0.2696607\n",
      "     0.5922087 ]\n",
      "   [-1.7961591  -1.8925554  -0.41655225  2.8299882   0.4466289\n",
      "     3.4457626 ]\n",
      "   [-1.1154089  -1.1745822   1.2023396  -0.06076117 -1.5096283\n",
      "     0.7106908 ]\n",
      "   [-0.09934417  0.8952499  -0.6025727  -0.7601258  -4.950531\n",
      "    -0.91794896]\n",
      "   [-0.730198    0.4987517   0.10699637  0.04278546  0.54071033\n",
      "    -0.50309646]]\n",
      "\n",
      "  [[-3.0523906   1.8179893   4.253165    1.4998401   2.628079\n",
      "     2.1809475 ]\n",
      "   [-7.7598214  -1.2395704   4.424106    5.041027    4.0303817\n",
      "     7.204144  ]\n",
      "   [-5.33444    -2.9674113   1.9738437   4.328881    4.872642\n",
      "     8.156278  ]\n",
      "   [-1.8466234  -0.9457812   2.1802762   4.8156037   0.12329892\n",
      "     0.8021099 ]\n",
      "   [-3.3392012  -4.19972    -0.9612767   6.551047    3.2264879\n",
      "     5.7523003 ]\n",
      "   [-1.1705554  -4.4363074  -1.1318855   3.9469442   3.668861\n",
      "     7.9511037 ]]\n",
      "\n",
      "  [[-0.6331823  -1.2349904   0.17108005 -1.7825468   0.4058478\n",
      "    -1.3177248 ]\n",
      "   [ 2.119253   -4.246382    1.0599287  -6.814192   -1.6555526\n",
      "    -5.312355  ]\n",
      "   [-1.1188248   0.14757451  1.0110303   1.6372967   0.67021793\n",
      "     2.4267418 ]\n",
      "   [ 0.95955515 -1.9993405   2.5704405  -1.7231832  -0.62369555\n",
      "    -1.1314759 ]\n",
      "   [ 0.49821588 -1.729508    2.970458   -3.6262503  -2.1428034\n",
      "     0.15667306]\n",
      "   [ 2.4125378   1.1109612   3.4186935  -5.2272053  -2.9276714\n",
      "    -4.479757  ]]\n",
      "\n",
      "  [[-1.2599558   1.0058122   0.9403136  -2.169106    1.3649019\n",
      "     0.05476408]\n",
      "   [ 1.9198732  -0.84344107  2.0494642  -1.7659745   1.8153642\n",
      "    -1.3940618 ]\n",
      "   [ 2.8428354  -1.6724693  -1.4574461  -0.75728846 -2.8203032\n",
      "    -4.540737  ]\n",
      "   [ 1.2147404   1.865701   -1.4707285  -1.3224499  -2.9885063\n",
      "    -0.7098579 ]\n",
      "   [ 2.2408023   0.45182097  0.23735748 -3.630287   -4.9238787\n",
      "    -1.6159724 ]\n",
      "   [ 2.0514302   1.124743   -0.66148055 -3.6998794  -3.634573\n",
      "    -3.2806454 ]]\n",
      "\n",
      "  [[-2.1587913   1.7147038   1.544837    2.9021912  -0.54687065\n",
      "     0.69791436]\n",
      "   [-0.00582398 -2.3462925   1.799922    0.32839322  0.3320445\n",
      "    -2.016989  ]\n",
      "   [-2.1043587  -2.8178024   2.2148285   3.891037    2.2759385\n",
      "     2.491633  ]\n",
      "   [ 1.8959482  -3.3403587   2.0201633  -2.8513105   2.2833853\n",
      "    -1.5907921 ]\n",
      "   [ 1.2664658  -1.2141082   0.86169004 -0.82817227  1.8247795\n",
      "    -2.2689962 ]\n",
      "   [ 1.4200376  -0.8142122  -0.15309595 -2.757525   -0.03373783\n",
      "    -3.5767376 ]]\n",
      "\n",
      "  [[-2.0107505   2.0400176   1.5965837   0.41086924  2.151286\n",
      "     2.0080416 ]\n",
      "   [-1.2814682  -2.6879327   1.4795719   0.70971787  1.2221687\n",
      "     4.831139  ]\n",
      "   [ 0.3697977   0.11362538 -0.8682849   1.8676807   1.5285181\n",
      "    -2.5126774 ]\n",
      "   [ 0.17680757 -0.59016186 -0.07212502  1.788976    1.8290929\n",
      "     1.2414881 ]\n",
      "   [ 0.27183303 -2.8065841  -0.40462255 -2.3706849  -0.47989556\n",
      "    -0.17444041]\n",
      "   [ 0.8829943  -3.017673   -2.083954    2.1678572   2.4494154\n",
      "    -0.16720197]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.5176607   1.9687198   2.0989769  -0.47190297]\n",
      "   [-0.49124843  0.08534324  0.2991765  -0.48528743]\n",
      "   [ 0.24215932 -4.697081   -4.019181    0.1927161 ]\n",
      "   [ 0.7396501  -3.0292149  -2.48159     0.7256521 ]\n",
      "   [ 0.7377838  -1.2575825  -1.0456651   0.7396505 ]\n",
      "   [ 0.12743154 -2.1013763  -1.7714081   0.15186152]]\n",
      "\n",
      "  [[-0.5409756   2.8193953   2.0520196  -0.5236139 ]\n",
      "   [-1.0098997   4.226584    3.5640724  -0.9722929 ]\n",
      "   [-0.31559435 -2.7033126  -2.1731372  -0.34413084]\n",
      "   [ 0.67385    -4.3054934  -2.5939085   0.6549963 ]\n",
      "   [ 0.7716512  -4.6774607  -3.4361272   0.73228556]\n",
      "   [ 0.30699015 -2.0128524  -1.77724     0.3016845 ]]\n",
      "\n",
      "  [[-0.42360795  3.4683292   3.2733347  -0.39263082]\n",
      "   [-0.47245097  0.7355631   1.3526613  -0.47366753]\n",
      "   [ 0.8764782  -5.054033   -4.6074905   0.832105  ]\n",
      "   [ 1.5158468  -5.5036316  -5.460711    1.4665604 ]\n",
      "   [ 1.3198522  -4.7758727  -4.838473    1.2815847 ]\n",
      "   [ 0.6756115  -5.767558   -5.86507     0.60872716]]\n",
      "\n",
      "  [[-1.6453142   4.41468     3.7185764  -1.6388824 ]\n",
      "   [ 0.1504475  -1.3770971  -0.84712774  0.10965424]\n",
      "   [ 1.5174129  -5.9414515  -4.9204936   1.4784044 ]\n",
      "   [ 1.9338356  -4.261786   -3.374979    1.932654  ]\n",
      "   [ 1.5312709  -4.5473847  -3.4884474   1.480693  ]\n",
      "   [ 2.4686995  -5.566623   -4.674686    2.4672065 ]]\n",
      "\n",
      "  [[-1.29121     2.8940606   1.9502285  -1.3604355 ]\n",
      "   [ 0.00294916 -1.2568096  -0.89668256 -0.00931877]\n",
      "   [ 0.7530518  -2.5598295  -1.9020394   0.7952453 ]\n",
      "   [ 1.7065613  -5.794166   -4.5830455   1.7274754 ]\n",
      "   [ 1.828848   -7.046152   -5.713792    1.8388789 ]\n",
      "   [ 2.3361118  -9.356349   -7.713717    2.356966  ]]\n",
      "\n",
      "  [[-0.65288585  3.4191878   2.6229937  -0.6571499 ]\n",
      "   [ 0.72896236 -2.1471467  -1.9732635   0.77281165]\n",
      "   [ 0.32350188 -1.2035745  -1.7607827   0.34396917]\n",
      "   [ 1.0276341   1.058868    1.2416905   1.0519707 ]\n",
      "   [ 1.6617185  -1.5399216  -1.5241555   1.7414258 ]\n",
      "   [ 2.3202841  -3.4387836  -2.1360145   2.3670495 ]]\n",
      "\n",
      "  [[-0.13920632  1.920276    1.8218281  -0.1018336 ]\n",
      "   [ 0.19900118 -1.852921   -1.364047    0.17529728]\n",
      "   [ 0.9201308  -2.3837185  -1.2430844   0.8900646 ]\n",
      "   [ 1.0384122  -2.2614129  -0.6760908   1.014745  ]\n",
      "   [ 1.3033148  -4.046018   -2.5787446   1.2661915 ]\n",
      "   [ 1.1603479  -2.2507179  -1.0619235   1.1338618 ]]\n",
      "\n",
      "  [[-1.5968649   5.382058    4.5500836  -1.5562288 ]\n",
      "   [ 0.86753845 -4.5697775  -4.1758018   0.8370436 ]\n",
      "   [ 1.9240823  -9.450348   -8.618066    1.8469068 ]\n",
      "   [ 1.4563193  -6.307143   -5.9952025   1.4192016 ]\n",
      "   [ 1.9546863  -5.530293   -5.110774    1.9280047 ]\n",
      "   [ 1.7568917  -6.1661077  -5.8179903   1.6855166 ]]]], shape=(1, 8, 6, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 4)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 7, 256)\n",
      "(1, 7, 256)\n",
      "(1, 7, 256)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "(1, 8, 7, 32)\n",
      "(1, 8, 7, 32)\n",
      "(1, 8, 7, 32)\n",
      "matmul_qk.shape = (1, 8, 7, 7)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    0.34484416   0.3515555    0.8269838   -0.1325054\n",
      "      1.1541476    0.36659813]\n",
      "   [ -1.5524524    0.57553977   1.342203     0.7264702    0.67869383\n",
      "     -2.3639412    0.21526393]\n",
      "   [  0.17656448  -2.1603198   -3.1747456   -2.3878162   -0.21427149\n",
      "     -2.5600047   -1.3389277 ]\n",
      "   [ -0.27855322  -2.3594024    0.28350914  -6.3847156    2.1768348\n",
      "      0.48724338   4.0435467 ]\n",
      "   [ -0.5387738   -1.1847006   -2.5186715    3.639655    -2.1242177\n",
      "      8.928502     3.1669946 ]\n",
      "   [ -0.30547413  -1.4707135    0.06941595   0.72953105   1.0991143\n",
      "      2.2755632    3.7240849 ]\n",
      "   [ -0.0702149   -2.969282    -3.383008     1.3616961    0.42645463\n",
      "     16.02929      8.762581  ]]\n",
      "\n",
      "  [[ -0.1123142    0.09086403  -0.2228786   -0.18202324   0.14455257\n",
      "     -0.04013486  -0.48076916]\n",
      "   [ -0.27778682  -7.685327    -0.20150177   3.3823       3.8902612\n",
      "      4.337893     1.4037256 ]\n",
      "   [  0.30459622  -1.6710656   -3.094567    -1.6013079    0.79777056\n",
      "      0.13481723  -1.8365917 ]\n",
      "   [ -0.03799488  -4.1970086    2.696054    -2.7333486    1.6560552\n",
      "      4.530645     4.271422  ]\n",
      "   [  0.6443627   -3.9118009    1.1981791   -4.1091504   -6.4226418\n",
      "     -0.9858088   -1.1690059 ]\n",
      "   [  0.82783276  -3.7707963   -2.1052582   -2.503423    -4.9312515\n",
      "     -4.1483335   -5.133481  ]\n",
      "   [  0.796586    -0.30842566  -2.7917175   -4.5511785   -0.32458684\n",
      "     -0.10727115  -6.259451  ]]\n",
      "\n",
      "  [[ -0.10918517  -0.21059825   0.3513798    0.34551156   0.83573407\n",
      "      0.38043547   0.42571396]\n",
      "   [  0.2860052   -5.063574    -2.1691      -3.5214968   -3.319986\n",
      "      1.6807239    2.6336646 ]\n",
      "   [  0.6062982    2.719351    -4.9581633   -1.0914348   -1.6627623\n",
      "     -0.31062695  -2.5335972 ]\n",
      "   [  0.13651355  -1.777794     3.1865156   -0.07830478  -1.239388\n",
      "      1.7914739    2.9210606 ]\n",
      "   [ -0.21224323  -1.5428834    7.991749    -0.5730239    1.2063524\n",
      "      3.6277044    1.6937939 ]\n",
      "   [ -0.85228556  -2.2769668    6.909945     3.543675     3.6550546\n",
      "      9.817229     3.752735  ]\n",
      "   [  0.18337622  -3.708946     1.3114123   -0.97666943   0.6310309\n",
      "      0.9075615   -3.0031114 ]]\n",
      "\n",
      "  [[ -0.22114104  -0.3915212    0.13786234   0.33271423   0.6211036\n",
      "      1.0825356   -0.11799879]\n",
      "   [  0.14206643   1.1752353    3.6785405    1.6119353   -0.9453702\n",
      "     -3.2077594   -2.725474  ]\n",
      "   [ -0.07449952  -3.5981474   -0.31350046   1.4361463    2.564271\n",
      "      2.474412     0.16766883]\n",
      "   [ -0.1049557    0.70436597   0.5221487   -0.21612936   0.9116111\n",
      "     -0.48090836   0.18603255]\n",
      "   [ -0.310017    -2.4498558    2.5756524    1.8039283   -0.97895104\n",
      "      3.0719118    0.6960478 ]\n",
      "   [ -0.39245275  -5.124641     3.6571612    1.2725205    3.1442227\n",
      "      0.02843648  -1.5512888 ]\n",
      "   [ -0.5742471   -4.852531     1.5101644    2.5168405    3.3431916\n",
      "      9.560527     0.73367864]]\n",
      "\n",
      "  [[  0.00076126  -0.1641996   -0.14196222  -0.19410133   0.00823206\n",
      "      0.61542094  -0.08737361]\n",
      "   [  0.78445697  -5.048575     3.2536209   -0.97517645   0.36774525\n",
      "     -3.194747    -0.04737727]\n",
      "   [  0.2599421   -2.3611805    4.198083    -0.59922963  -2.4801793\n",
      "     -5.2311      -3.3485384 ]\n",
      "   [  0.203725     3.9226558    1.8656771    0.77520037  -3.1088407\n",
      "     -2.945639     1.2251574 ]\n",
      "   [  0.6548166    0.10552962   1.3786814   -2.6986282   -3.3625326\n",
      "     -3.5189183   -7.556854  ]\n",
      "   [  0.85031134  -2.967739     2.643215    -0.4758638    1.1578159\n",
      "     -8.628843    -5.5761604 ]\n",
      "   [  0.01436896   0.14466181  -2.1695178    2.4206777    8.998847\n",
      "     -1.5658671   -0.12156205]]\n",
      "\n",
      "  [[ -0.00999826  -0.5171405    0.3842402   -0.26599303  -0.17592163\n",
      "      0.08763131   0.29407236]\n",
      "   [  0.3875343    2.307618    -2.0652568    0.06119107  -1.376965\n",
      "     -0.84213823   1.6010706 ]\n",
      "   [  0.5494998    0.9012554   -5.717646    -0.5140254   -1.9746116\n",
      "     -4.1329675    0.38239804]\n",
      "   [ -0.09110849   1.9358311   -2.1756437   -0.7149111   -1.2668245\n",
      "      0.23427002  -0.5892781 ]\n",
      "   [ -0.99071145   6.797044    -1.4787755    4.8537555   -0.8091358\n",
      "      1.0001513   -0.56871563]\n",
      "   [  0.1653099   -0.6100134    4.8626056    0.720647     0.9629525\n",
      "     -1.3203751   -4.107916  ]\n",
      "   [ -0.4776169   -4.275887     1.1203661    0.41609427   4.5017853\n",
      "     -0.39336598  -2.3056731 ]]\n",
      "\n",
      "  [[ -0.20863976   0.0058533    0.141982     0.3954799    0.70690477\n",
      "      0.8365328    0.28462148]\n",
      "   [ -0.5011563   -5.83805      3.0883057    2.4906335    2.8664675\n",
      "      3.39397      0.8982007 ]\n",
      "   [ -0.08541125   2.1106565    0.02025323   0.89557755   1.3944312\n",
      "      4.8944817    3.2451954 ]\n",
      "   [  1.3930423    2.344757    -1.3677286   -6.293937    -6.4745502\n",
      "     -5.8228016   -2.4453523 ]\n",
      "   [  2.1056752   -0.87906635  -3.2462146   -5.5659313   -6.810574\n",
      "    -12.417996    -5.3918004 ]\n",
      "   [  0.7307259   -0.62191457   2.2546957   -4.717381    -0.30207893\n",
      "     -5.2433743   -4.031666  ]\n",
      "   [  2.2975056   -3.7142315   -2.9478807  -10.469339    -5.418616\n",
      "    -10.1225815   -7.3040056 ]]\n",
      "\n",
      "  [[ -0.15726982   0.34343007  -0.0106815    0.2812659    0.26918355\n",
      "      0.6895108    0.5193374 ]\n",
      "   [  0.42748785  -3.541775    -4.416529    -1.5682534   -0.30482742\n",
      "      0.77651656   1.8078052 ]\n",
      "   [  0.09384713   3.2008502   -4.3988295   -1.0193433   -1.7561591\n",
      "     -0.20635648   1.7356048 ]\n",
      "   [  0.00172196  -0.81685966   2.484785    -2.7307875    4.4621997\n",
      "      2.2167425    0.05664166]\n",
      "   [ -0.26568195   6.4090133    0.19427305   4.194164    -5.8646803\n",
      "      0.7492719    3.420403  ]\n",
      "   [  0.15044671  -0.83800465   5.2889094    3.357029     2.2839093\n",
      "     -3.226359     1.4485415 ]\n",
      "   [  0.5396642   -4.57709     -2.997078    -1.2818362   -2.02568\n",
      "     -0.3638186    4.7706485 ]]]], shape=(1, 8, 7, 7), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 7, 7)\n",
      "output.shape = (1, 8, 7, 32)\n",
      "scaled_attention.shape= (1, 7, 8, 32)\n",
      "concat_attention.shape= (1, 7, 256)\n",
      "outputs.shape= (1, 7, 256)\n",
      "(1, 7, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 7, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 7, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.6510173   -2.8665452   -1.9706712    0.6258339 ]\n",
      "   [ -1.2761326   -0.6591419   -0.772127    -1.2929033 ]\n",
      "   [  1.0074586   -3.2963974   -3.106445     0.9872349 ]\n",
      "   [  1.2579683   -4.881622    -3.767476     1.2469316 ]\n",
      "   [  1.3234984   -5.9344664   -5.2370906    1.2870476 ]\n",
      "   [  1.7000594   -3.6593008   -2.971424     1.7176253 ]\n",
      "   [  1.277635    -1.9751607   -1.5198905    1.2974895 ]]\n",
      "\n",
      "  [[  1.5699102   -4.8338065   -2.8935153    1.5974954 ]\n",
      "   [  1.602976    -6.0585413   -4.3626466    1.6033887 ]\n",
      "   [  0.84273714  -3.3321726   -3.019569     0.8300638 ]\n",
      "   [  1.739568    -5.32123     -4.1527953    1.7057066 ]\n",
      "   [  1.9442569   -4.8756948   -3.3264403    1.9580569 ]\n",
      "   [  3.9294844  -11.821626    -9.099886     3.8940978 ]\n",
      "   [  2.1841393   -5.056805    -3.41623      2.1828265 ]]\n",
      "\n",
      "  [[ -0.9479617   -1.5584124   -2.3440182   -0.91537315]\n",
      "   [  1.1684959   -6.083402    -5.361126     1.1414931 ]\n",
      "   [  0.8458585   -8.91149     -7.7787905    0.78298134]\n",
      "   [  2.5803494   -5.880795    -5.0921974    2.5411623 ]\n",
      "   [  3.3461082  -11.678779   -10.798813     3.272511  ]\n",
      "   [  3.3453295   -7.0702214   -5.903679     3.2953756 ]\n",
      "   [  1.6342535   -4.6710873   -4.3739305    1.604569  ]]\n",
      "\n",
      "  [[ -0.53032047  -2.1487055   -3.3869808   -0.5465881 ]\n",
      "   [ -0.37673792   0.5297353    0.7943683   -0.41485333]\n",
      "   [  1.2794939   -4.2864437   -4.2701783    1.2569122 ]\n",
      "   [  2.427947    -1.600872    -0.90655017   2.481265  ]\n",
      "   [  2.3424263   -7.2077713   -5.824986     2.335523  ]\n",
      "   [  1.7222499   -2.7512228   -2.3087296    1.7864217 ]\n",
      "   [  1.0004859   -7.141323    -6.550525     0.9541158 ]]\n",
      "\n",
      "  [[ -1.8786137    3.8044016    1.7851309   -1.9308672 ]\n",
      "   [ -2.0427704    1.9088362    0.8611902   -2.10597   ]\n",
      "   [  0.3713629   -6.5012736   -5.5806484    0.3149101 ]\n",
      "   [  2.3904674   -0.8018907    0.54941446   2.4430196 ]\n",
      "   [  3.2833803   -5.793328    -3.5530388    3.3556385 ]\n",
      "   [  4.031575    -8.23709     -5.352422     4.1288157 ]\n",
      "   [  1.7043644   -4.002987    -2.874395     1.7465335 ]]\n",
      "\n",
      "  [[  1.8587611   -2.3004422   -1.999603     1.8540798 ]\n",
      "   [  0.62134135  -1.1685245   -1.0336286    0.63556284]\n",
      "   [  0.93438053  -3.9921112   -3.1974504    0.91454566]\n",
      "   [  2.386284    -8.792911    -6.874719     2.3729997 ]\n",
      "   [  2.8132436  -11.001211    -8.411129     2.7784264 ]\n",
      "   [  2.7941167   -3.5710614   -2.4098222    2.8085887 ]\n",
      "   [  2.5608003   -5.1447315   -4.0192666    2.5128264 ]]\n",
      "\n",
      "  [[  0.263768     1.3027129    0.7912128    0.31364933]\n",
      "   [  0.90733486  -1.2863759   -0.23643312   0.92466456]\n",
      "   [  1.9193958   -5.6561565   -3.624307     1.9176353 ]\n",
      "   [  2.590705    -7.7614427   -5.438415     2.6344485 ]\n",
      "   [  2.0141904   -6.001995    -3.7005918    2.02994   ]\n",
      "   [  2.3413384   -2.5982022   -0.645182     2.369621  ]\n",
      "   [  1.1849179   -0.41510123   0.7534639    1.1835017 ]]\n",
      "\n",
      "  [[ -0.50981736   4.1422954    3.6196024   -0.43590528]\n",
      "   [  0.4603232   -1.0371166   -0.40937394   0.4394898 ]\n",
      "   [  1.2603666   -3.7127948   -2.6582818    1.2983314 ]\n",
      "   [  1.9421246   -6.2256336   -3.6375484    1.9323288 ]\n",
      "   [  1.3231326   -4.248828    -3.1260052    1.3073777 ]\n",
      "   [  2.442132    -7.4952087   -5.2700887    2.4572377 ]\n",
      "   [  2.4187791   -5.273016    -2.1958537    2.496527  ]]]], shape=(1, 8, 7, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 7, 4)\n",
      "output.shape = (1, 8, 7, 32)\n",
      "scaled_attention.shape= (1, 7, 8, 32)\n",
      "concat_attention.shape= (1, 7, 256)\n",
      "outputs.shape= (1, 7, 256)\n",
      "(1, 7, 256)\n",
      "(1, 7, 256)\n",
      "(1, 7, 256)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "(1, 8, 7, 32)\n",
      "(1, 8, 7, 32)\n",
      "(1, 8, 7, 32)\n",
      "matmul_qk.shape = (1, 8, 7, 7)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181 -0.6147039  -0.12000973 -0.26990378\n",
      "    -0.12601104  0.12373019]\n",
      "   [ 3.96568    -0.17723085  1.5590783  -1.8989185  -1.7120768\n",
      "    -2.4874895  -2.461858  ]\n",
      "   [ 3.9833748  -2.2539997   0.15140963 -2.319149    1.4055393\n",
      "     1.8976921  -0.35062248]\n",
      "   [ 3.0466342  -1.2768042  -0.9543833  -2.4668362  -3.578708\n",
      "    -2.1999793  -3.1076705 ]\n",
      "   [ 4.7013025   1.1783674   1.1767476  -4.7330627  -2.6853023\n",
      "    -3.841537   -1.8945122 ]\n",
      "   [ 5.391753   -1.2895486   1.732423   -3.715419   -2.8239722\n",
      "    -3.662196   -3.4676816 ]\n",
      "   [ 4.304044    0.21154301  0.16459453 -0.47825167 -3.806402\n",
      "    -3.5910673  -5.444761  ]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489  -0.22415894  0.44419977  0.86740375\n",
      "    -1.3531986  -0.15267114]\n",
      "   [ 0.22428446 -2.402899   -1.7699208  -0.35874206 -2.1430826\n",
      "    -0.03102632  0.5429379 ]\n",
      "   [-0.6712342   0.30168235 -3.0742364   0.19597065 -3.3459918\n",
      "    -0.10913967 -1.0004693 ]\n",
      "   [ 1.2226754   2.9617927   2.3518116  -2.0428975   1.0853696\n",
      "    -0.76767504 -0.31001   ]\n",
      "   [ 3.3925583   4.3809915   0.44789767 -3.760937   -1.4172314\n",
      "    -2.3714716   0.38087097]\n",
      "   [-2.3619554  -2.1506267  -0.96027523  3.8960817   1.8534632\n",
      "    -0.74298    -1.9999115 ]\n",
      "   [ 0.09021449  0.7948777   0.09161925  3.2192543   1.7999158\n",
      "    -1.8118176  -1.6655257 ]]\n",
      "\n",
      "  [[-3.0174336   0.41961712  1.9252456   0.7638777   1.4724514\n",
      "     3.0431707   2.857288  ]\n",
      "   [-1.3549727  -0.09280814  2.5851169  -0.3922029  -0.2696607\n",
      "     0.5922087   0.36080608]\n",
      "   [-1.7961591  -1.8925554  -0.41655225  2.8299882   0.4466289\n",
      "     3.4457626   0.64506155]\n",
      "   [-1.1154089  -1.1745822   1.2023396  -0.06076117 -1.5096283\n",
      "     0.7106908  -1.2447225 ]\n",
      "   [-0.09934417  0.8952499  -0.6025727  -0.7601258  -4.950531\n",
      "    -0.91794896 -1.0665505 ]\n",
      "   [-0.730198    0.4987517   0.10699637  0.04278546  0.54071033\n",
      "    -0.50309646 -3.3410394 ]\n",
      "   [ 0.23829934  2.064014   -2.8888557  -1.358422   -0.28164428\n",
      "     2.4049861   0.2265279 ]]\n",
      "\n",
      "  [[-3.0523906   1.8179893   4.253165    1.4998401   2.628079\n",
      "     2.1809475   1.6848648 ]\n",
      "   [-7.7598214  -1.2395704   4.424106    5.041027    4.0303817\n",
      "     7.204144   10.943421  ]\n",
      "   [-5.33444    -2.9674113   1.9738437   4.328881    4.872642\n",
      "     8.156278   11.386528  ]\n",
      "   [-1.8466234  -0.9457812   2.1802762   4.8156037   0.12329892\n",
      "     0.8021099  -0.5149028 ]\n",
      "   [-3.3392012  -4.19972    -0.9612767   6.551047    3.2264879\n",
      "     5.7523003   4.0562634 ]\n",
      "   [-1.1705554  -4.4363074  -1.1318855   3.9469442   3.668861\n",
      "     7.9511037   7.773055  ]\n",
      "   [-0.29540277 -1.5871546  -2.855876    4.95405     3.0536811\n",
      "     5.366177    5.9232087 ]]\n",
      "\n",
      "  [[-0.6331823  -1.2349904   0.17108005 -1.7825468   0.4058478\n",
      "    -1.3177248   1.3473992 ]\n",
      "   [ 2.119253   -4.246382    1.0599287  -6.814192   -1.6555526\n",
      "    -5.312355   -1.9066014 ]\n",
      "   [-1.1188248   0.14757451  1.0110303   1.6372967   0.67021793\n",
      "     2.4267418  -1.4578599 ]\n",
      "   [ 0.95955515 -1.9993405   2.5704405  -1.7231832  -0.62369555\n",
      "    -1.1314759  -2.2201543 ]\n",
      "   [ 0.49821588 -1.729508    2.970458   -3.6262503  -2.1428034\n",
      "     0.15667306 -1.0091923 ]\n",
      "   [ 2.4125378   1.1109612   3.4186935  -5.2272053  -2.9276714\n",
      "    -4.479757   -1.5783914 ]\n",
      "   [ 1.1474882   0.25640345  4.7612867  -4.441928    0.44976506\n",
      "    -3.3421116  -5.0208836 ]]\n",
      "\n",
      "  [[-1.2599558   1.0058122   0.9403136  -2.169106    1.3649019\n",
      "     0.05476408 -0.26069167]\n",
      "   [ 1.9198732  -0.84344107  2.0494642  -1.7659745   1.8153642\n",
      "    -1.3940618  -1.7131975 ]\n",
      "   [ 2.8428354  -1.6724693  -1.4574461  -0.75728846 -2.8203032\n",
      "    -4.540737   -4.3870435 ]\n",
      "   [ 1.2147404   1.865701   -1.4707285  -1.3224499  -2.9885063\n",
      "    -0.7098579  -2.4524608 ]\n",
      "   [ 2.2408023   0.45182097  0.23735748 -3.630287   -4.9238787\n",
      "    -1.6159724  -4.2167706 ]\n",
      "   [ 2.0514302   1.124743   -0.66148055 -3.6998794  -3.634573\n",
      "    -3.2806454  -3.5353765 ]\n",
      "   [ 1.871314    1.4599013   4.544816    0.6970564   0.36970255\n",
      "    -1.5995741  -3.4669428 ]]\n",
      "\n",
      "  [[-2.1587913   1.7147038   1.544837    2.9021912  -0.54687065\n",
      "     0.69791436  0.48697126]\n",
      "   [-0.00582398 -2.3462925   1.799922    0.32839322  0.3320445\n",
      "    -2.016989   -2.8965588 ]\n",
      "   [-2.1043587  -2.8178024   2.2148285   3.891037    2.2759385\n",
      "     2.491633    0.3426766 ]\n",
      "   [ 1.8959482  -3.3403587   2.0201633  -2.8513105   2.2833853\n",
      "    -1.5907921  -5.2675643 ]\n",
      "   [ 1.2664658  -1.2141082   0.86169004 -0.82817227  1.8247795\n",
      "    -2.2689962  -2.915465  ]\n",
      "   [ 1.4200376  -0.8142122  -0.15309595 -2.757525   -0.03373783\n",
      "    -3.5767376  -0.78452003]\n",
      "   [ 0.9523569   2.6001348  -0.7241572  -1.2373092  -0.06528202\n",
      "    -2.2806585  -0.30806044]]\n",
      "\n",
      "  [[-2.0107505   2.0400176   1.5965837   0.41086924  2.151286\n",
      "     2.0080416   3.8807015 ]\n",
      "   [-1.2814682  -2.6879327   1.4795719   0.70971787  1.2221687\n",
      "     4.831139    4.012641  ]\n",
      "   [ 0.3697977   0.11362538 -0.8682849   1.8676807   1.5285181\n",
      "    -2.5126774  -0.09528378]\n",
      "   [ 0.17680757 -0.59016186 -0.07212502  1.788976    1.8290929\n",
      "     1.2414881   2.5110254 ]\n",
      "   [ 0.27183303 -2.8065841  -0.40462255 -2.3706849  -0.47989556\n",
      "    -0.17444041  1.52519   ]\n",
      "   [ 0.8829943  -3.017673   -2.083954    2.1678572   2.4494154\n",
      "    -0.16720197  2.1018946 ]\n",
      "   [ 0.3609053  -0.05415381  0.63386804  1.9816571   1.4667697\n",
      "     2.7945373   0.77046496]]]], shape=(1, 8, 7, 7), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 7, 7)\n",
      "output.shape = (1, 8, 7, 32)\n",
      "scaled_attention.shape= (1, 7, 8, 32)\n",
      "concat_attention.shape= (1, 7, 256)\n",
      "outputs.shape= (1, 7, 256)\n",
      "(1, 7, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 7, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 7, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.5176607    1.9687198    2.0989769   -0.47190297]\n",
      "   [ -0.49124843   0.08534324   0.2991765   -0.48528743]\n",
      "   [  0.24215932  -4.697081    -4.019181     0.1927161 ]\n",
      "   [  0.7396501   -3.0292149   -2.48159      0.7256521 ]\n",
      "   [  0.7377838   -1.2575825   -1.0456651    0.7396505 ]\n",
      "   [  0.12743154  -2.1013763   -1.7714081    0.15186152]\n",
      "   [  0.5769268   -1.6947745   -1.6039816    0.5679224 ]]\n",
      "\n",
      "  [[ -0.5409756    2.8193953    2.0520196   -0.5236139 ]\n",
      "   [ -1.0098997    4.226584     3.5640724   -0.9722929 ]\n",
      "   [ -0.31559435  -2.7033126   -2.1731372   -0.34413084]\n",
      "   [  0.67385     -4.3054934   -2.5939085    0.6549963 ]\n",
      "   [  0.7716512   -4.6774607   -3.4361272    0.73228556]\n",
      "   [  0.30699015  -2.0128524   -1.77724      0.3016845 ]\n",
      "   [  0.9517128   -5.12329     -3.7025745    0.919322  ]]\n",
      "\n",
      "  [[ -0.42360795   3.4683292    3.2733347   -0.39263082]\n",
      "   [ -0.47245097   0.7355631    1.3526613   -0.47366753]\n",
      "   [  0.8764782   -5.054033    -4.6074905    0.832105  ]\n",
      "   [  1.5158468   -5.5036316   -5.460711     1.4665604 ]\n",
      "   [  1.3198522   -4.7758727   -4.838473     1.2815847 ]\n",
      "   [  0.6756115   -5.767558    -5.86507      0.60872716]\n",
      "   [  0.6049767   -6.190683    -6.4384503    0.5407869 ]]\n",
      "\n",
      "  [[ -1.6453142    4.41468      3.7185764   -1.6388824 ]\n",
      "   [  0.1504475   -1.3770971   -0.84712774   0.10965424]\n",
      "   [  1.5174129   -5.9414515   -4.9204936    1.4784044 ]\n",
      "   [  1.9338356   -4.261786    -3.374979     1.932654  ]\n",
      "   [  1.5312709   -4.5473847   -3.4884474    1.480693  ]\n",
      "   [  2.4686995   -5.566623    -4.674686     2.4672065 ]\n",
      "   [  2.017123    -5.156707    -4.0176287    1.9925607 ]]\n",
      "\n",
      "  [[ -1.29121      2.8940606    1.9502285   -1.3604355 ]\n",
      "   [  0.00294916  -1.2568096   -0.89668256  -0.00931877]\n",
      "   [  0.7530518   -2.5598295   -1.9020394    0.7952453 ]\n",
      "   [  1.7065613   -5.794166    -4.5830455    1.7274754 ]\n",
      "   [  1.828848    -7.046152    -5.713792     1.8388789 ]\n",
      "   [  2.3361118   -9.356349    -7.713717     2.356966  ]\n",
      "   [  2.4712949   -7.777404    -6.13042      2.5266523 ]]\n",
      "\n",
      "  [[ -0.65288585   3.4191878    2.6229937   -0.6571499 ]\n",
      "   [  0.72896236  -2.1471467   -1.9732635    0.77281165]\n",
      "   [  0.32350188  -1.2035745   -1.7607827    0.34396917]\n",
      "   [  1.0276341    1.058868     1.2416905    1.0519707 ]\n",
      "   [  1.6617185   -1.5399216   -1.5241555    1.7414258 ]\n",
      "   [  2.3202841   -3.4387836   -2.1360145    2.3670495 ]\n",
      "   [  2.256841    -5.1996007   -3.339242     2.3179893 ]]\n",
      "\n",
      "  [[ -0.13920632   1.920276     1.8218281   -0.1018336 ]\n",
      "   [  0.19900118  -1.852921    -1.364047     0.17529728]\n",
      "   [  0.9201308   -2.3837185   -1.2430844    0.8900646 ]\n",
      "   [  1.0384122   -2.2614129   -0.6760908    1.014745  ]\n",
      "   [  1.3033148   -4.046018    -2.5787446    1.2661915 ]\n",
      "   [  1.1603479   -2.2507179   -1.0619235    1.1338618 ]\n",
      "   [  1.0411183   -4.245855    -2.9054105    1.0157434 ]]\n",
      "\n",
      "  [[ -1.5968649    5.382058     4.5500836   -1.5562288 ]\n",
      "   [  0.86753845  -4.5697775   -4.1758018    0.8370436 ]\n",
      "   [  1.9240823   -9.450348    -8.618066     1.8469068 ]\n",
      "   [  1.4563193   -6.307143    -5.9952025    1.4192016 ]\n",
      "   [  1.9546863   -5.530293    -5.110774     1.9280047 ]\n",
      "   [  1.7568917   -6.1661077   -5.8179903    1.6855166 ]\n",
      "   [  3.4059408  -12.176527   -11.224673     3.3215594 ]]]], shape=(1, 8, 7, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 7, 4)\n",
      "output.shape = (1, 8, 7, 32)\n",
      "scaled_attention.shape= (1, 7, 8, 32)\n",
      "concat_attention.shape= (1, 7, 256)\n",
      "outputs.shape= (1, 7, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    0.34484416   0.3515555    0.8269838   -0.1325054\n",
      "      1.1541476    0.36659813   0.6267392 ]\n",
      "   [ -1.5524524    0.57553977   1.342203     0.7264702    0.67869383\n",
      "     -2.3639412    0.21526393  -1.575456  ]\n",
      "   [  0.17656448  -2.1603198   -3.1747456   -2.3878162   -0.21427149\n",
      "     -2.5600047   -1.3389277    0.00445623]\n",
      "   [ -0.27855322  -2.3594024    0.28350914  -6.3847156    2.1768348\n",
      "      0.48724338   4.0435467    0.8452997 ]\n",
      "   [ -0.5387738   -1.1847006   -2.5186715    3.639655    -2.1242177\n",
      "      8.928502     3.1669946    5.1829453 ]\n",
      "   [ -0.30547413  -1.4707135    0.06941595   0.72953105   1.0991143\n",
      "      2.2755632    3.7240849    3.374271  ]\n",
      "   [ -0.0702149   -2.969282    -3.383008     1.3616961    0.42645463\n",
      "     16.02929      8.762581     8.823401  ]\n",
      "   [ -0.20550568  -0.10201316  -2.2742054   -2.1785567   -1.7485399\n",
      "      2.4331734    1.2430574    4.5232897 ]]\n",
      "\n",
      "  [[ -0.1123142    0.09086403  -0.2228786   -0.18202324   0.14455257\n",
      "     -0.04013486  -0.48076916  -0.06074027]\n",
      "   [ -0.27778682  -7.685327    -0.20150177   3.3823       3.8902612\n",
      "      4.337893     1.4037256   -1.6104152 ]\n",
      "   [  0.30459622  -1.6710656   -3.094567    -1.6013079    0.79777056\n",
      "      0.13481723  -1.8365917   -0.71397376]\n",
      "   [ -0.03799488  -4.1970086    2.696054    -2.7333486    1.6560552\n",
      "      4.530645     4.271422    -1.2604878 ]\n",
      "   [  0.6443627   -3.9118009    1.1981791   -4.1091504   -6.4226418\n",
      "     -0.9858088   -1.1690059   -0.22695212]\n",
      "   [  0.82783276  -3.7707963   -2.1052582   -2.503423    -4.9312515\n",
      "     -4.1483335   -5.133481    -1.3080887 ]\n",
      "   [  0.796586    -0.30842566  -2.7917175   -4.5511785   -0.32458684\n",
      "     -0.10727115  -6.259451     1.659165  ]\n",
      "   [  0.7033243   -0.6943296    0.38815764   1.9749643   -4.1990952\n",
      "     -3.2755108   -5.034937    -3.387448  ]]\n",
      "\n",
      "  [[ -0.10918517  -0.21059825   0.3513798    0.34551156   0.83573407\n",
      "      0.38043547   0.42571396  -0.10994706]\n",
      "   [  0.2860052   -5.063574    -2.1691      -3.5214968   -3.319986\n",
      "      1.6807239    2.6336646   -3.6028974 ]\n",
      "   [  0.6062982    2.719351    -4.9581633   -1.0914348   -1.6627623\n",
      "     -0.31062695  -2.5335972    0.98369557]\n",
      "   [  0.13651355  -1.777794     3.1865156   -0.07830478  -1.239388\n",
      "      1.7914739    2.9210606   -1.4029268 ]\n",
      "   [ -0.21224323  -1.5428834    7.991749    -0.5730239    1.2063524\n",
      "      3.6277044    1.6937939    1.8189023 ]\n",
      "   [ -0.85228556  -2.2769668    6.909945     3.543675     3.6550546\n",
      "      9.817229     3.752735     0.8422704 ]\n",
      "   [  0.18337622  -3.708946     1.3114123   -0.97666943   0.6310309\n",
      "      0.9075615   -3.0031114   -0.5886862 ]\n",
      "   [ -0.1485042   -3.7088253   -2.2962284   -1.5428704    0.04910947\n",
      "      4.634492     2.4989789   -4.185793  ]]\n",
      "\n",
      "  [[ -0.22114104  -0.3915212    0.13786234   0.33271423   0.6211036\n",
      "      1.0825356   -0.11799879   0.72872204]\n",
      "   [  0.14206643   1.1752353    3.6785405    1.6119353   -0.9453702\n",
      "     -3.2077594   -2.725474     1.8995842 ]\n",
      "   [ -0.07449952  -3.5981474   -0.31350046   1.4361463    2.564271\n",
      "      2.474412     0.16766883  -0.5836961 ]\n",
      "   [ -0.1049557    0.70436597   0.5221487   -0.21612936   0.9116111\n",
      "     -0.48090836   0.18603255  -1.9520314 ]\n",
      "   [ -0.310017    -2.4498558    2.5756524    1.8039283   -0.97895104\n",
      "      3.0719118    0.6960478    3.8471358 ]\n",
      "   [ -0.39245275  -5.124641     3.6571612    1.2725205    3.1442227\n",
      "      0.02843648  -1.5512888    2.58859   ]\n",
      "   [ -0.5742471   -4.852531     1.5101644    2.5168405    3.3431916\n",
      "      9.560527     0.73367864   4.908349  ]\n",
      "   [ -0.97490954  -4.6069136   -1.7809725    1.4556999    4.3725533\n",
      "      9.8998165    0.52443993   0.8263054 ]]\n",
      "\n",
      "  [[  0.00076126  -0.1641996   -0.14196222  -0.19410133   0.00823206\n",
      "      0.61542094  -0.08737361  -0.10085457]\n",
      "   [  0.78445697  -5.048575     3.2536209   -0.97517645   0.36774525\n",
      "     -3.194747    -0.04737727  -0.09847949]\n",
      "   [  0.2599421   -2.3611805    4.198083    -0.59922963  -2.4801793\n",
      "     -5.2311      -3.3485384   -2.4956753 ]\n",
      "   [  0.203725     3.9226558    1.8656771    0.77520037  -3.1088407\n",
      "     -2.945639     1.2251574    1.5569918 ]\n",
      "   [  0.6548166    0.10552962   1.3786814   -2.6986282   -3.3625326\n",
      "     -3.5189183   -7.556854    -3.5280879 ]\n",
      "   [  0.85031134  -2.967739     2.643215    -0.4758638    1.1578159\n",
      "     -8.628843    -5.5761604   -4.848856  ]\n",
      "   [  0.01436896   0.14466181  -2.1695178    2.4206777    8.998847\n",
      "     -1.5658671   -0.12156205  -0.9806287 ]\n",
      "   [  0.56318194  -4.2370687    1.5977541    1.2324364    2.794963\n",
      "     -3.8853424   -3.6924665   -4.2410336 ]]\n",
      "\n",
      "  [[ -0.00999826  -0.5171405    0.3842402   -0.26599303  -0.17592163\n",
      "      0.08763131   0.29407236   0.11240291]\n",
      "   [  0.3875343    2.307618    -2.0652568    0.06119107  -1.376965\n",
      "     -0.84213823   1.6010706   -1.9992107 ]\n",
      "   [  0.5494998    0.9012554   -5.717646    -0.5140254   -1.9746116\n",
      "     -4.1329675    0.38239804  -3.9651098 ]\n",
      "   [ -0.09110849   1.9358311   -2.1756437   -0.7149111   -1.2668245\n",
      "      0.23427002  -0.5892781    1.0475453 ]\n",
      "   [ -0.99071145   6.797044    -1.4787755    4.8537555   -0.8091358\n",
      "      1.0001513   -0.56871563  -1.046842  ]\n",
      "   [  0.1653099   -0.6100134    4.8626056    0.720647     0.9629525\n",
      "     -1.3203751   -4.107916     2.463625  ]\n",
      "   [ -0.4776169   -4.275887     1.1203661    0.41609427   4.5017853\n",
      "     -0.39336598  -2.3056731   -1.7078637 ]\n",
      "   [  0.88401735  -3.8715084   -1.8880895   -3.6033385   -1.9594375\n",
      "      1.1121889   -1.3816634   -1.135376  ]]\n",
      "\n",
      "  [[ -0.20863976   0.0058533    0.141982     0.3954799    0.70690477\n",
      "      0.8365328    0.28462148  -0.24421918]\n",
      "   [ -0.5011563   -5.83805      3.0883057    2.4906335    2.8664675\n",
      "      3.39397      0.8982007   -2.3877478 ]\n",
      "   [ -0.08541125   2.1106565    0.02025323   0.89557755   1.3944312\n",
      "      4.8944817    3.2451954    4.2954817 ]\n",
      "   [  1.3930423    2.344757    -1.3677286   -6.293937    -6.4745502\n",
      "     -5.8228016   -2.4453523   -2.055828  ]\n",
      "   [  2.1056752   -0.87906635  -3.2462146   -5.5659313   -6.810574\n",
      "    -12.417996    -5.3918004   -2.5989642 ]\n",
      "   [  0.7307259   -0.62191457   2.2546957   -4.717381    -0.30207893\n",
      "     -5.2433743   -4.031666     0.92758286]\n",
      "   [  2.2975056   -3.7142315   -2.9478807  -10.469339    -5.418616\n",
      "    -10.1225815   -7.3040056    0.4335872 ]\n",
      "   [ -0.6196739   -1.8421621    1.7395886    2.5982852    5.356473\n",
      "      0.73179406   1.0359558   -3.0025475 ]]\n",
      "\n",
      "  [[ -0.15726982   0.34343007  -0.0106815    0.2812659    0.26918355\n",
      "      0.6895108    0.5193374    0.17895888]\n",
      "   [  0.42748785  -3.541775    -4.416529    -1.5682534   -0.30482742\n",
      "      0.77651656   1.8078052    1.8106172 ]\n",
      "   [  0.09384713   3.2008502   -4.3988295   -1.0193433   -1.7561591\n",
      "     -0.20635648   1.7356048   -0.08922087]\n",
      "   [  0.00172196  -0.81685966   2.484785    -2.7307875    4.4621997\n",
      "      2.2167425    0.05664166  -2.2540293 ]\n",
      "   [ -0.26568195   6.4090133    0.19427305   4.194164    -5.8646803\n",
      "      0.7492719    3.420403     4.213038  ]\n",
      "   [  0.15044671  -0.83800465   5.2889094    3.357029     2.2839093\n",
      "     -3.226359     1.4485415   -1.6294129 ]\n",
      "   [  0.5396642   -4.57709     -2.997078    -1.2818362   -2.02568\n",
      "     -0.3638186    4.7706485    3.2261612 ]\n",
      "   [ -0.00558216  -3.0874553   -2.459048     0.15172434   2.5665867\n",
      "      1.0365962    5.3165255   -1.6244347 ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.6510173   -2.8665452   -1.9706712    0.6258339 ]\n",
      "   [ -1.2761326   -0.6591419   -0.772127    -1.2929033 ]\n",
      "   [  1.0074586   -3.2963974   -3.106445     0.9872349 ]\n",
      "   [  1.2579683   -4.881622    -3.767476     1.2469316 ]\n",
      "   [  1.3234984   -5.9344664   -5.2370906    1.2870476 ]\n",
      "   [  1.7000594   -3.6593008   -2.971424     1.7176253 ]\n",
      "   [  1.2776281   -1.9751713   -1.519901     1.2974825 ]\n",
      "   [  0.58336854  -1.1657993   -0.539427     0.5778812 ]]\n",
      "\n",
      "  [[  1.5699102   -4.8338065   -2.8935153    1.5974954 ]\n",
      "   [  1.602976    -6.0585413   -4.3626466    1.6033887 ]\n",
      "   [  0.84273714  -3.3321726   -3.019569     0.8300638 ]\n",
      "   [  1.739568    -5.32123     -4.1527953    1.7057066 ]\n",
      "   [  1.9442569   -4.8756948   -3.3264403    1.9580569 ]\n",
      "   [  3.9294844  -11.821626    -9.099886     3.8940978 ]\n",
      "   [  2.184128    -5.056767    -3.4162076    2.1828146 ]\n",
      "   [  1.9612429   -4.841177    -3.8993757    1.9119158 ]]\n",
      "\n",
      "  [[ -0.9479617   -1.5584124   -2.3440182   -0.91537315]\n",
      "   [  1.1684959   -6.083402    -5.361126     1.1414931 ]\n",
      "   [  0.8458585   -8.91149     -7.7787905    0.78298134]\n",
      "   [  2.5803494   -5.880795    -5.0921974    2.5411623 ]\n",
      "   [  3.3461082  -11.678779   -10.798813     3.272511  ]\n",
      "   [  3.3453295   -7.0702214   -5.903679     3.2953756 ]\n",
      "   [  1.6342559   -4.6711273   -4.3739734    1.6045712 ]\n",
      "   [  1.7710527   -4.211759    -3.7379825    1.7677531 ]]\n",
      "\n",
      "  [[ -0.53032047  -2.1487055   -3.3869808   -0.5465881 ]\n",
      "   [ -0.37673792   0.5297353    0.7943683   -0.41485333]\n",
      "   [  1.2794939   -4.2864437   -4.2701783    1.2569122 ]\n",
      "   [  2.427947    -1.600872    -0.90655017   2.481265  ]\n",
      "   [  2.3424263   -7.2077713   -5.824986     2.335523  ]\n",
      "   [  1.7222499   -2.7512228   -2.3087296    1.7864217 ]\n",
      "   [  1.0004832   -7.1413507   -6.550554     0.95411247]\n",
      "   [  1.4302231   -3.8393989   -3.2885814    1.4499868 ]]\n",
      "\n",
      "  [[ -1.8786137    3.8044016    1.7851309   -1.9308672 ]\n",
      "   [ -2.0427704    1.9088362    0.8611902   -2.10597   ]\n",
      "   [  0.3713629   -6.5012736   -5.5806484    0.3149101 ]\n",
      "   [  2.3904674   -0.8018907    0.54941446   2.4430196 ]\n",
      "   [  3.2833803   -5.793328    -3.5530388    3.3556385 ]\n",
      "   [  4.031575    -8.23709     -5.352422     4.1288157 ]\n",
      "   [  1.704372    -4.0030203   -2.8744233    1.746541  ]\n",
      "   [  0.55620587  -0.81578505  -0.7211592    0.5843559 ]]\n",
      "\n",
      "  [[  1.8587611   -2.3004422   -1.999603     1.8540798 ]\n",
      "   [  0.62134135  -1.1685245   -1.0336286    0.63556284]\n",
      "   [  0.93438053  -3.9921112   -3.1974504    0.91454566]\n",
      "   [  2.386284    -8.792911    -6.874719     2.3729997 ]\n",
      "   [  2.8132436  -11.001211    -8.411129     2.7784264 ]\n",
      "   [  2.7941167   -3.5710614   -2.4098222    2.8085887 ]\n",
      "   [  2.5607827   -5.1447167   -4.0192566    2.5128086 ]\n",
      "   [  1.9819213   -4.7589607   -4.3256965    1.9169271 ]]\n",
      "\n",
      "  [[  0.263768     1.3027129    0.7912128    0.31364933]\n",
      "   [  0.90733486  -1.2863759   -0.23643312   0.92466456]\n",
      "   [  1.9193958   -5.6561565   -3.624307     1.9176353 ]\n",
      "   [  2.590705    -7.7614427   -5.438415     2.6344485 ]\n",
      "   [  2.0141904   -6.001995    -3.7005918    2.02994   ]\n",
      "   [  2.3413384   -2.5982022   -0.645182     2.369621  ]\n",
      "   [  1.1849254   -0.41514406   0.7534283    1.1835097 ]\n",
      "   [  1.6196365   -2.736624    -1.3294117    1.6487104 ]]\n",
      "\n",
      "  [[ -0.50981736   4.1422954    3.6196024   -0.43590528]\n",
      "   [  0.4603232   -1.0371166   -0.40937394   0.4394898 ]\n",
      "   [  1.2603666   -3.7127948   -2.6582818    1.2983314 ]\n",
      "   [  1.9421246   -6.2256336   -3.6375484    1.9323288 ]\n",
      "   [  1.3231326   -4.248828    -3.1260052    1.3073777 ]\n",
      "   [  2.442132    -7.4952087   -5.2700887    2.4572377 ]\n",
      "   [  2.4187922   -5.27305     -2.1958756    2.4965396 ]\n",
      "   [  1.0935532   -3.028773    -1.9892881    1.1174632 ]]]], shape=(1, 8, 8, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 4)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181 -0.6147039  -0.12000973 -0.26990378\n",
      "    -0.12601104  0.12369402  0.6703813 ]\n",
      "   [ 3.96568    -0.17723085  1.5590783  -1.8989185  -1.7120768\n",
      "    -2.4874895  -2.461606   -3.3995156 ]\n",
      "   [ 3.9833748  -2.2539997   0.15140963 -2.319149    1.4055393\n",
      "     1.8976921  -0.35040918 -0.84194183]\n",
      "   [ 3.0466342  -1.2768042  -0.9543833  -2.4668362  -3.578708\n",
      "    -2.1999793  -3.10732    -1.4634132 ]\n",
      "   [ 4.7013025   1.1783674   1.1767476  -4.7330627  -2.6853023\n",
      "    -3.841537   -1.8943468  -3.301475  ]\n",
      "   [ 5.391753   -1.2895486   1.732423   -3.715419   -2.8239722\n",
      "    -3.662196   -3.4674766  -3.3219385 ]\n",
      "   [ 4.304015    0.211817    0.16455683 -0.47811216 -3.8062627\n",
      "    -3.590859   -5.4443517  -5.1668825 ]\n",
      "   [ 4.4580784  -0.61394614  1.2823641   2.367739    2.35907\n",
      "     0.98633593 -3.9488287  -4.141637  ]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489  -0.22415894  0.44419977  0.86740375\n",
      "    -1.3531986  -0.1527121  -1.0399705 ]\n",
      "   [ 0.22428446 -2.402899   -1.7699208  -0.35874206 -2.1430826\n",
      "    -0.03102632  0.5429309  -1.0086042 ]\n",
      "   [-0.6712342   0.30168235 -3.0742364   0.19597065 -3.3459918\n",
      "    -0.10913967 -1.0004942   0.28558722]\n",
      "   [ 1.2226754   2.9617927   2.3518116  -2.0428975   1.0853696\n",
      "    -0.76767504 -0.3099975  -1.1994996 ]\n",
      "   [ 3.3925583   4.3809915   0.44789767 -3.760937   -1.4172314\n",
      "    -2.3714716   0.38091192  1.2854881 ]\n",
      "   [-2.3619554  -2.1506267  -0.96027523  3.8960817   1.8534632\n",
      "    -0.74298    -2.0000184   2.1462076 ]\n",
      "   [ 0.09026945  0.7947642   0.09152631  3.2191868   1.7997355\n",
      "    -1.8118993  -1.6655155   0.09941932]\n",
      "   [-0.677145   -1.8837119  -1.8422472   5.2752542   0.19712047\n",
      "     0.6287616  -2.4682517  -0.7707627 ]]\n",
      "\n",
      "  [[-3.0174336   0.41961712  1.9252456   0.7638777   1.4724514\n",
      "     3.0431707   2.8573203   3.2402046 ]\n",
      "   [-1.3549727  -0.09280814  2.5851169  -0.3922029  -0.2696607\n",
      "     0.5922087   0.36082545 -0.7003431 ]\n",
      "   [-1.7961591  -1.8925554  -0.41655225  2.8299882   0.4466289\n",
      "     3.4457626   0.6450706   0.9305902 ]\n",
      "   [-1.1154089  -1.1745822   1.2023396  -0.06076117 -1.5096283\n",
      "     0.7106908  -1.2446266  -3.0961888 ]\n",
      "   [-0.09934417  0.8952499  -0.6025727  -0.7601258  -4.950531\n",
      "    -0.91794896 -1.0664147  -1.4203914 ]\n",
      "   [-0.730198    0.4987517   0.10699637  0.04278546  0.54071033\n",
      "    -0.50309646 -3.3407922  -1.0143468 ]\n",
      "   [ 0.23826952  2.0639033  -2.888716   -1.358555   -0.28163734\n",
      "     2.404897    0.22699867  3.525614  ]\n",
      "   [ 1.2659289  -0.02524091 -2.9593556   0.11135699 -0.94128567\n",
      "    -2.829995   -3.2003918  -3.8595636 ]]\n",
      "\n",
      "  [[-3.0523906   1.8179893   4.253165    1.4998401   2.628079\n",
      "     2.1809475   1.6847974   2.1162713 ]\n",
      "   [-7.7598214  -1.2395704   4.424106    5.041027    4.0303817\n",
      "     7.204144   10.943072    3.6315343 ]\n",
      "   [-5.33444    -2.9674113   1.9738437   4.328881    4.872642\n",
      "     8.156278   11.38618     1.9597439 ]\n",
      "   [-1.8466234  -0.9457812   2.1802762   4.8156037   0.12329892\n",
      "     0.8021099  -0.51522094 -0.3930014 ]\n",
      "   [-3.3392012  -4.19972    -0.9612767   6.551047    3.2264879\n",
      "     5.7523003   4.0560446   1.2217602 ]\n",
      "   [-1.1705554  -4.4363074  -1.1318855   3.9469442   3.668861\n",
      "     7.9511037   7.7728662   3.1097941 ]\n",
      "   [-0.29538846 -1.5871849  -2.8558958   4.954087    3.0537887\n",
      "     5.3664327   5.922907    0.4448749 ]\n",
      "   [-1.4024625  -3.924694   -1.0780026   2.6823587   2.3963602\n",
      "     4.2347217   6.6275954   2.555661  ]]\n",
      "\n",
      "  [[-0.6331823  -1.2349904   0.17108005 -1.7825468   0.4058478\n",
      "    -1.3177248   1.3472757  -1.038774  ]\n",
      "   [ 2.119253   -4.246382    1.0599287  -6.814192   -1.6555526\n",
      "    -5.312355   -1.9065204  -3.2599387 ]\n",
      "   [-1.1188248   0.14757451  1.0110303   1.6372967   0.67021793\n",
      "     2.4267418  -1.4579339  -3.0462801 ]\n",
      "   [ 0.95955515 -1.9993405   2.5704405  -1.7231832  -0.62369555\n",
      "    -1.1314759  -2.2201748  -2.0597012 ]\n",
      "   [ 0.49821588 -1.729508    2.970458   -3.6262503  -2.1428034\n",
      "     0.15667306 -1.009277   -2.750008  ]\n",
      "   [ 2.4125378   1.1109612   3.4186935  -5.2272053  -2.9276714\n",
      "    -4.479757   -1.5780019   1.15176   ]\n",
      "   [ 1.147488    0.25655437  4.761012   -4.4415584   0.4498366\n",
      "    -3.3419423  -5.0206585  -2.8960762 ]\n",
      "   [ 3.2666113  -0.7482998  -2.4978724  -4.6047583  -0.4991162\n",
      "    -1.8868389  -4.1274176  -7.000914  ]]\n",
      "\n",
      "  [[-1.2599558   1.0058122   0.9403136  -2.169106    1.3649019\n",
      "     0.05476408 -0.2606994   0.6737088 ]\n",
      "   [ 1.9198732  -0.84344107  2.0494642  -1.7659745   1.8153642\n",
      "    -1.3940618  -1.7129728   0.6147069 ]\n",
      "   [ 2.8428354  -1.6724693  -1.4574461  -0.75728846 -2.8203032\n",
      "    -4.540737   -4.3868213  -0.7501175 ]\n",
      "   [ 1.2147404   1.865701   -1.4707285  -1.3224499  -2.9885063\n",
      "    -0.7098579  -2.4524012   0.32578066]\n",
      "   [ 2.2408023   0.45182097  0.23735748 -3.630287   -4.9238787\n",
      "    -1.6159724  -4.2166977  -0.00287914]\n",
      "   [ 2.0514302   1.124743   -0.66148055 -3.6998794  -3.634573\n",
      "    -3.2806454  -3.53534    -0.587686  ]\n",
      "   [ 1.8713237   1.459809    4.544932    0.6970329   0.3698051\n",
      "    -1.5995883  -3.4667106  -2.5332448 ]\n",
      "   [ 0.7016293   0.6996512  -0.24522036 -1.6627809   1.9750067\n",
      "    -0.02285164 -0.6037045  -1.2260524 ]]\n",
      "\n",
      "  [[-2.1587913   1.7147038   1.544837    2.9021912  -0.54687065\n",
      "     0.69791436  0.48692545  0.3874992 ]\n",
      "   [-0.00582398 -2.3462925   1.799922    0.32839322  0.3320445\n",
      "    -2.016989   -2.8966556  -2.9691656 ]\n",
      "   [-2.1043587  -2.8178024   2.2148285   3.891037    2.2759385\n",
      "     2.491633    0.34263963  3.354882  ]\n",
      "   [ 1.8959482  -3.3403587   2.0201633  -2.8513105   2.2833853\n",
      "    -1.5907921  -5.267708   -0.7608538 ]\n",
      "   [ 1.2664658  -1.2141082   0.86169004 -0.82817227  1.8247795\n",
      "    -2.2689962  -2.9155438  -0.7371273 ]\n",
      "   [ 1.4200376  -0.8142122  -0.15309595 -2.757525   -0.03373783\n",
      "    -3.5767376  -0.7845442   0.7012546 ]\n",
      "   [ 0.9523497   2.5999942  -0.72424364 -1.2373599  -0.06541023\n",
      "    -2.280726   -0.30826616 -1.5803338 ]\n",
      "   [-0.66845006 -1.2726071   0.88486147 -0.17885678  0.5529859\n",
      "     0.5443401   1.7109423   0.3173334 ]]\n",
      "\n",
      "  [[-2.0107505   2.0400176   1.5965837   0.41086924  2.151286\n",
      "     2.0080416   3.8806849   3.7532885 ]\n",
      "   [-1.2814682  -2.6879327   1.4795719   0.70971787  1.2221687\n",
      "     4.831139    4.012818    3.0739448 ]\n",
      "   [ 0.3697977   0.11362538 -0.8682849   1.8676807   1.5285181\n",
      "    -2.5126774  -0.09541667 -0.62111366]\n",
      "   [ 0.17680757 -0.59016186 -0.07212502  1.788976    1.8290929\n",
      "     1.2414881   2.5110507   0.9378911 ]\n",
      "   [ 0.27183303 -2.8065841  -0.40462255 -2.3706849  -0.47989556\n",
      "    -0.17444041  1.5252     -0.41247755]\n",
      "   [ 0.8829943  -3.017673   -2.083954    2.1678572   2.4494154\n",
      "    -0.16720197  2.1018574  -0.03369299]\n",
      "   [ 0.36083797 -0.05401262  0.63395387  1.9816741   1.4668022\n",
      "     2.7947261   0.7702297   0.40087664]\n",
      "   [ 0.28325427 -1.9323467   0.58271986  2.48945     0.9275244\n",
      "     2.8025713   1.751703    0.5133646 ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.5176607    1.9687198    2.0989769   -0.47190297]\n",
      "   [ -0.49124843   0.08534324   0.2991765   -0.48528743]\n",
      "   [  0.24215932  -4.697081    -4.019181     0.1927161 ]\n",
      "   [  0.7396501   -3.0292149   -2.48159      0.7256521 ]\n",
      "   [  0.7377838   -1.2575825   -1.0456651    0.7396505 ]\n",
      "   [  0.12743154  -2.1013763   -1.7714081    0.15186152]\n",
      "   [  0.57692707  -1.6948001   -1.6039941    0.56792045]\n",
      "   [ -0.0456275   -3.2555232   -2.8839726   -0.07279108]]\n",
      "\n",
      "  [[ -0.5409756    2.8193953    2.0520196   -0.5236139 ]\n",
      "   [ -1.0098997    4.226584     3.5640724   -0.9722929 ]\n",
      "   [ -0.31559435  -2.7033126   -2.1731372   -0.34413084]\n",
      "   [  0.67385     -4.3054934   -2.5939085    0.6549963 ]\n",
      "   [  0.7716512   -4.6774607   -3.4361272    0.73228556]\n",
      "   [  0.30699015  -2.0128524   -1.77724      0.3016845 ]\n",
      "   [  0.95172566  -5.123536    -3.7028008    0.9193312 ]\n",
      "   [  0.1625587   -2.8245268   -2.2594504    0.1333035 ]]\n",
      "\n",
      "  [[ -0.42360795   3.4683292    3.2733347   -0.39263082]\n",
      "   [ -0.47245097   0.7355631    1.3526613   -0.47366753]\n",
      "   [  0.8764782   -5.054033    -4.6074905    0.832105  ]\n",
      "   [  1.5158468   -5.5036316   -5.460711     1.4665604 ]\n",
      "   [  1.3198522   -4.7758727   -4.838473     1.2815847 ]\n",
      "   [  0.6756115   -5.767558    -5.86507      0.60872716]\n",
      "   [  0.6050416   -6.19143     -6.4390326    0.54084617]\n",
      "   [  0.44500238  -0.68225163  -0.9241052    0.42594016]]\n",
      "\n",
      "  [[ -1.6453142    4.41468      3.7185764   -1.6388824 ]\n",
      "   [  0.1504475   -1.3770971   -0.84712774   0.10965424]\n",
      "   [  1.5174129   -5.9414515   -4.9204936    1.4784044 ]\n",
      "   [  1.9338356   -4.261786    -3.374979     1.932654  ]\n",
      "   [  1.5312709   -4.5473847   -3.4884474    1.480693  ]\n",
      "   [  2.4686995   -5.566623    -4.674686     2.4672065 ]\n",
      "   [  2.0171244   -5.156834    -4.0178304    1.9925612 ]\n",
      "   [  1.4369041   -3.667318    -2.7229993    1.4106048 ]]\n",
      "\n",
      "  [[ -1.29121      2.8940606    1.9502285   -1.3604355 ]\n",
      "   [  0.00294916  -1.2568096   -0.89668256  -0.00931877]\n",
      "   [  0.7530518   -2.5598295   -1.9020394    0.7952453 ]\n",
      "   [  1.7065613   -5.794166    -4.5830455    1.7274754 ]\n",
      "   [  1.828848    -7.046152    -5.713792     1.8388789 ]\n",
      "   [  2.3361118   -9.356349    -7.713717     2.356966  ]\n",
      "   [  2.4712012   -7.777283    -6.1303487    2.5265574 ]\n",
      "   [  2.178688    -7.1766615   -5.927077     2.238128  ]]\n",
      "\n",
      "  [[ -0.65288585   3.4191878    2.6229937   -0.6571499 ]\n",
      "   [  0.72896236  -2.1471467   -1.9732635    0.77281165]\n",
      "   [  0.32350188  -1.2035745   -1.7607827    0.34396917]\n",
      "   [  1.0276341    1.058868     1.2416905    1.0519707 ]\n",
      "   [  1.6617185   -1.5399216   -1.5241555    1.7414258 ]\n",
      "   [  2.3202841   -3.4387836   -2.1360145    2.3670495 ]\n",
      "   [  2.2568145   -5.2002926   -3.3399537    2.3179612 ]\n",
      "   [  2.10796     -4.739115    -2.9632866    2.1519184 ]]\n",
      "\n",
      "  [[ -0.13920632   1.920276     1.8218281   -0.1018336 ]\n",
      "   [  0.19900118  -1.852921    -1.364047     0.17529728]\n",
      "   [  0.9201308   -2.3837185   -1.2430844    0.8900646 ]\n",
      "   [  1.0384122   -2.2614129   -0.6760908    1.014745  ]\n",
      "   [  1.3033148   -4.046018    -2.5787446    1.2661915 ]\n",
      "   [  1.1603479   -2.2507179   -1.0619235    1.1338618 ]\n",
      "   [  1.041217    -4.2461724   -2.9057727    1.0158387 ]\n",
      "   [  1.0148015   -1.7258983   -0.37147498   1.0063083 ]]\n",
      "\n",
      "  [[ -1.5968649    5.382058     4.5500836   -1.5562288 ]\n",
      "   [  0.86753845  -4.5697775   -4.1758018    0.8370436 ]\n",
      "   [  1.9240823   -9.450348    -8.618066     1.8469068 ]\n",
      "   [  1.4563193   -6.307143    -5.9952025    1.4192016 ]\n",
      "   [  1.9546863   -5.530293    -5.110774     1.9280047 ]\n",
      "   [  1.7568917   -6.1661077   -5.8179903    1.6855166 ]\n",
      "   [  3.4059234  -12.176341   -11.224475     3.3215392 ]\n",
      "   [  1.7747686   -8.078717    -7.356744     1.7220855 ]]]], shape=(1, 8, 8, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 4)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 9, 256)\n",
      "(1, 9, 256)\n",
      "(1, 9, 256)\n",
      "split_heads()\n",
      "(1, 9, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9, 8, 32)\n",
      "split_heads()\n",
      "(1, 9, 256)\n",
      "(1, 9, 8, 32)\n",
      "split_heads()\n",
      "(1, 9, 256)\n",
      "(1, 9, 8, 32)\n",
      "(1, 8, 9, 32)\n",
      "(1, 8, 9, 32)\n",
      "(1, 8, 9, 32)\n",
      "matmul_qk.shape = (1, 8, 9, 9)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    0.34484416   0.3515555    0.8269838   -0.1325054\n",
      "      1.1541476    0.36659813   0.6267392    0.7029531 ]\n",
      "   [ -1.5524524    0.57553977   1.342203     0.7264702    0.67869383\n",
      "     -2.3639412    0.21526393  -1.575456     4.0412474 ]\n",
      "   [  0.17656448  -2.1603198   -3.1747456   -2.3878162   -0.21427149\n",
      "     -2.5600047   -1.3389277    0.00445623  -1.037421  ]\n",
      "   [ -0.27855322  -2.3594024    0.28350914  -6.3847156    2.1768348\n",
      "      0.48724338   4.0435467    0.8452997    2.298873  ]\n",
      "   [ -0.5387738   -1.1847006   -2.5186715    3.639655    -2.1242177\n",
      "      8.928502     3.1669946    5.1829453    4.105474  ]\n",
      "   [ -0.30547413  -1.4707135    0.06941595   0.72953105   1.0991143\n",
      "      2.2755632    3.7240849    3.374271     3.1669166 ]\n",
      "   [ -0.0702149   -2.969282    -3.383008     1.3616961    0.42645463\n",
      "     16.02929      8.762581     8.823401     3.3795218 ]\n",
      "   [ -0.20550568  -0.10201316  -2.2742054   -2.1785567   -1.7485399\n",
      "      2.4331734    1.2430574    4.5232897    0.90502656]\n",
      "   [ -0.590271     1.0130959   -0.09686741   0.5674616    0.8100412\n",
      "      0.53022087   0.5689679    5.691497     0.52202976]]\n",
      "\n",
      "  [[ -0.1123142    0.09086403  -0.2228786   -0.18202324   0.14455257\n",
      "     -0.04013486  -0.48076916  -0.06074027  -0.04329262]\n",
      "   [ -0.27778682  -7.685327    -0.20150177   3.3823       3.8902612\n",
      "      4.337893     1.4037256   -1.6104152    0.8058254 ]\n",
      "   [  0.30459622  -1.6710656   -3.094567    -1.6013079    0.79777056\n",
      "      0.13481723  -1.8365917   -0.71397376  -0.33064502]\n",
      "   [ -0.03799488  -4.1970086    2.696054    -2.7333486    1.6560552\n",
      "      4.530645     4.271422    -1.2604878   -2.5451972 ]\n",
      "   [  0.6443627   -3.9118009    1.1981791   -4.1091504   -6.4226418\n",
      "     -0.9858088   -1.1690059   -0.22695212   1.5001154 ]\n",
      "   [  0.82783276  -3.7707963   -2.1052582   -2.503423    -4.9312515\n",
      "     -4.1483335   -5.133481    -1.3080887   -1.481337  ]\n",
      "   [  0.796586    -0.30842566  -2.7917175   -4.5511785   -0.32458684\n",
      "     -0.10727115  -6.259451     1.659165     1.4761349 ]\n",
      "   [  0.7033243   -0.6943296    0.38815764   1.9749643   -4.1990952\n",
      "     -3.2755108   -5.034937    -3.387448     3.127423  ]\n",
      "   [ -0.6092106   -2.0272765   -0.859915     2.4829376    4.411381\n",
      "      5.087132     1.9951113    2.134214    -1.0122576 ]]\n",
      "\n",
      "  [[ -0.10918517  -0.21059825   0.3513798    0.34551156   0.83573407\n",
      "      0.38043547   0.42571396  -0.10994706   0.73884016]\n",
      "   [  0.2860052   -5.063574    -2.1691      -3.5214968   -3.319986\n",
      "      1.6807239    2.6336646   -3.6028974    1.4836615 ]\n",
      "   [  0.6062982    2.719351    -4.9581633   -1.0914348   -1.6627623\n",
      "     -0.31062695  -2.5335972    0.98369557  -2.88995   ]\n",
      "   [  0.13651355  -1.777794     3.1865156   -0.07830478  -1.239388\n",
      "      1.7914739    2.9210606   -1.4029268    1.2590778 ]\n",
      "   [ -0.21224323  -1.5428834    7.991749    -0.5730239    1.2063524\n",
      "      3.6277044    1.6937939    1.8189023    3.786629  ]\n",
      "   [ -0.85228556  -2.2769668    6.909945     3.543675     3.6550546\n",
      "      9.817229     3.752735     0.8422704    5.880493  ]\n",
      "   [  0.18337622  -3.708946     1.3114123   -0.97666943   0.6310309\n",
      "      0.9075615   -3.0031114   -0.5886862   -0.67060435]\n",
      "   [ -0.1485042   -3.7088253   -2.2962284   -1.5428704    0.04910947\n",
      "      4.634492     2.4989789   -4.185793     3.9252877 ]\n",
      "   [  0.3454023   -4.214501    -4.5776362   -4.2041674   -5.6266522\n",
      "     -8.447932    -3.633056     1.5147972   -4.597967  ]]\n",
      "\n",
      "  [[ -0.22114104  -0.3915212    0.13786234   0.33271423   0.6211036\n",
      "      1.0825356   -0.11799879   0.72872204  -0.1314376 ]\n",
      "   [  0.14206643   1.1752353    3.6785405    1.6119353   -0.9453702\n",
      "     -3.2077594   -2.725474     1.8995842   -0.996005  ]\n",
      "   [ -0.07449952  -3.5981474   -0.31350046   1.4361463    2.564271\n",
      "      2.474412     0.16766883  -0.5836961    1.1178291 ]\n",
      "   [ -0.1049557    0.70436597   0.5221487   -0.21612936   0.9116111\n",
      "     -0.48090836   0.18603255  -1.9520314    3.4277797 ]\n",
      "   [ -0.310017    -2.4498558    2.5756524    1.8039283   -0.97895104\n",
      "      3.0719118    0.6960478    3.8471358    4.3071675 ]\n",
      "   [ -0.39245275  -5.124641     3.6571612    1.2725205    3.1442227\n",
      "      0.02843648  -1.5512888    2.58859     -0.89740974]\n",
      "   [ -0.5742471   -4.852531     1.5101644    2.5168405    3.3431916\n",
      "      9.560527     0.73367864   4.908349     4.149948  ]\n",
      "   [ -0.97490954  -4.6069136   -1.7809725    1.4556999    4.3725533\n",
      "      9.8998165    0.52443993   0.8263054    0.9018291 ]\n",
      "   [ -0.5735989   -4.0548587    0.27316695  -2.4717047    3.2703655\n",
      "      3.5540314    0.07714308   8.548014     3.0001826 ]]\n",
      "\n",
      "  [[  0.00076126  -0.1641996   -0.14196222  -0.19410133   0.00823206\n",
      "      0.61542094  -0.08737361  -0.10085457  -0.06919557]\n",
      "   [  0.78445697  -5.048575     3.2536209   -0.97517645   0.36774525\n",
      "     -3.194747    -0.04737727  -0.09847949   0.45832372]\n",
      "   [  0.2599421   -2.3611805    4.198083    -0.59922963  -2.4801793\n",
      "     -5.2311      -3.3485384   -2.4956753   -3.0119252 ]\n",
      "   [  0.203725     3.9226558    1.8656771    0.77520037  -3.1088407\n",
      "     -2.945639     1.2251574    1.5569918   -1.4664475 ]\n",
      "   [  0.6548166    0.10552962   1.3786814   -2.6986282   -3.3625326\n",
      "     -3.5189183   -7.556854    -3.5280879   -4.3273387 ]\n",
      "   [  0.85031134  -2.967739     2.643215    -0.4758638    1.1578159\n",
      "     -8.628843    -5.5761604   -4.848856    -2.0556912 ]\n",
      "   [  0.01436896   0.14466181  -2.1695178    2.4206777    8.998847\n",
      "     -1.5658671   -0.12156205  -0.9806287    3.9562185 ]\n",
      "   [  0.56318194  -4.2370687    1.5977541    1.2324364    2.794963\n",
      "     -3.8853424   -3.6924665   -4.2410336   -0.9847575 ]\n",
      "   [  0.30105153  -1.6933879   -2.020751    -1.2294377   -0.5758017\n",
      "     -4.3765674    0.27936047  -4.3569603   -5.729614  ]]\n",
      "\n",
      "  [[ -0.00999826  -0.5171405    0.3842402   -0.26599303  -0.17592163\n",
      "      0.08763131   0.29407236   0.11240291   0.06468003]\n",
      "   [  0.3875343    2.307618    -2.0652568    0.06119107  -1.376965\n",
      "     -0.84213823   1.6010706   -1.9992107    3.9152644 ]\n",
      "   [  0.5494998    0.9012554   -5.717646    -0.5140254   -1.9746116\n",
      "     -4.1329675    0.38239804  -3.9651098    0.4975535 ]\n",
      "   [ -0.09110849   1.9358311   -2.1756437   -0.7149111   -1.2668245\n",
      "      0.23427002  -0.5892781    1.0475453    4.2543564 ]\n",
      "   [ -0.99071145   6.797044    -1.4787755    4.8537555   -0.8091358\n",
      "      1.0001513   -0.56871563  -1.046842     7.9517646 ]\n",
      "   [  0.1653099   -0.6100134    4.8626056    0.720647     0.9629525\n",
      "     -1.3203751   -4.107916     2.463625    -1.0382884 ]\n",
      "   [ -0.4776169   -4.275887     1.1203661    0.41609427   4.5017853\n",
      "     -0.39336598  -2.3056731   -1.7078637   -0.52901644]\n",
      "   [  0.88401735  -3.8715084   -1.8880895   -3.6033385   -1.9594375\n",
      "      1.1121889   -1.3816634   -1.135376    -3.8049717 ]\n",
      "   [  1.1133462   -6.4942994   -0.6978988   -4.9526157   -5.715827\n",
      "     -5.9033365   -5.149199    -1.8298197   -7.6704    ]]\n",
      "\n",
      "  [[ -0.20863976   0.0058533    0.141982     0.3954799    0.70690477\n",
      "      0.8365328    0.28462148  -0.24421918   0.6515544 ]\n",
      "   [ -0.5011563   -5.83805      3.0883057    2.4906335    2.8664675\n",
      "      3.39397      0.8982007   -2.3877478    5.1261563 ]\n",
      "   [ -0.08541125   2.1106565    0.02025323   0.89557755   1.3944312\n",
      "      4.8944817    3.2451954    4.2954817    0.5471952 ]\n",
      "   [  1.3930423    2.344757    -1.3677286   -6.293937    -6.4745502\n",
      "     -5.8228016   -2.4453523   -2.055828    -3.1815403 ]\n",
      "   [  2.1056752   -0.87906635  -3.2462146   -5.5659313   -6.810574\n",
      "    -12.417996    -5.3918004   -2.5989642  -11.401493  ]\n",
      "   [  0.7307259   -0.62191457   2.2546957   -4.717381    -0.30207893\n",
      "     -5.2433743   -4.031666     0.92758286  -2.1127427 ]\n",
      "   [  2.2975056   -3.7142315   -2.9478807  -10.469339    -5.418616\n",
      "    -10.1225815   -7.3040056    0.4335872   -9.478246  ]\n",
      "   [ -0.6196739   -1.8421621    1.7395886    2.5982852    5.356473\n",
      "      0.73179406   1.0359558   -3.0025475    4.908996  ]\n",
      "   [  1.2433432    0.35926548  -1.4999105   -1.8502873   -3.6189969\n",
      "     -9.892432    -1.9395131   -0.6860688   -8.132981  ]]\n",
      "\n",
      "  [[ -0.15726982   0.34343007  -0.0106815    0.2812659    0.26918355\n",
      "      0.6895108    0.5193374    0.17895888  -0.26491943]\n",
      "   [  0.42748785  -3.541775    -4.416529    -1.5682534   -0.30482742\n",
      "      0.77651656   1.8078052    1.8106172    3.8573866 ]\n",
      "   [  0.09384713   3.2008502   -4.3988295   -1.0193433   -1.7561591\n",
      "     -0.20635648   1.7356048   -0.08922087   0.29839948]\n",
      "   [  0.00172196  -0.81685966   2.484785    -2.7307875    4.4621997\n",
      "      2.2167425    0.05664166  -2.2540293   -2.799476  ]\n",
      "   [ -0.26568195   6.4090133    0.19427305   4.194164    -5.8646803\n",
      "      0.7492719    3.420403     4.213038     3.715512  ]\n",
      "   [  0.15044671  -0.83800465   5.2889094    3.357029     2.2839093\n",
      "     -3.226359     1.4485415   -1.6294129    1.074201  ]\n",
      "   [  0.5396642   -4.57709     -2.997078    -1.2818362   -2.02568\n",
      "     -0.3638186    4.7706485    3.2261612    0.503986  ]\n",
      "   [ -0.00558216  -3.0874553   -2.459048     0.15172434   2.5665867\n",
      "      1.0365962    5.3165255   -1.6244347    1.7662504 ]\n",
      "   [ -0.08250386   0.13179336  -0.65814525  -2.291554    -0.36036602\n",
      "      1.4784904    0.8082328   -0.5854646   -3.5685499 ]]]], shape=(1, 8, 9, 9), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 9, 9)\n",
      "output.shape = (1, 8, 9, 32)\n",
      "scaled_attention.shape= (1, 9, 8, 32)\n",
      "concat_attention.shape= (1, 9, 256)\n",
      "outputs.shape= (1, 9, 256)\n",
      "(1, 9, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 9, 256)\n",
      "(1, 9, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 9, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 9, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.6510173   -2.8665452   -1.9706712    0.6258339 ]\n",
      "   [ -1.2761326   -0.6591419   -0.772127    -1.2929033 ]\n",
      "   [  1.0074586   -3.2963974   -3.106445     0.9872349 ]\n",
      "   [  1.2579683   -4.881622    -3.767476     1.2469316 ]\n",
      "   [  1.3234984   -5.9344664   -5.2370906    1.2870476 ]\n",
      "   [  1.7000594   -3.6593008   -2.971424     1.7176253 ]\n",
      "   [  1.2776281   -1.9751713   -1.519901     1.2974825 ]\n",
      "   [  0.58336854  -1.1657993   -0.539427     0.5778812 ]\n",
      "   [  0.7600763   -0.7146235    0.06742804   0.76477766]]\n",
      "\n",
      "  [[  1.5699102   -4.8338065   -2.8935153    1.5974954 ]\n",
      "   [  1.602976    -6.0585413   -4.3626466    1.6033887 ]\n",
      "   [  0.84273714  -3.3321726   -3.019569     0.8300638 ]\n",
      "   [  1.739568    -5.32123     -4.1527953    1.7057066 ]\n",
      "   [  1.9442569   -4.8756948   -3.3264403    1.9580569 ]\n",
      "   [  3.9294844  -11.821626    -9.099886     3.8940978 ]\n",
      "   [  2.184128    -5.056767    -3.4162076    2.1828146 ]\n",
      "   [  1.9612429   -4.841177    -3.8993757    1.9119158 ]\n",
      "   [  2.1313474   -8.619019    -7.0290985    2.1060083 ]]\n",
      "\n",
      "  [[ -0.9479617   -1.5584124   -2.3440182   -0.91537315]\n",
      "   [  1.1684959   -6.083402    -5.361126     1.1414931 ]\n",
      "   [  0.8458585   -8.91149     -7.7787905    0.78298134]\n",
      "   [  2.5803494   -5.880795    -5.0921974    2.5411623 ]\n",
      "   [  3.3461082  -11.678779   -10.798813     3.272511  ]\n",
      "   [  3.3453295   -7.0702214   -5.903679     3.2953756 ]\n",
      "   [  1.6342559   -4.6711273   -4.3739734    1.6045712 ]\n",
      "   [  1.7710527   -4.211759    -3.7379825    1.7677531 ]\n",
      "   [  2.3430088   -5.5874043   -4.7693295    2.3498065 ]]\n",
      "\n",
      "  [[ -0.53032047  -2.1487055   -3.3869808   -0.5465881 ]\n",
      "   [ -0.37673792   0.5297353    0.7943683   -0.41485333]\n",
      "   [  1.2794939   -4.2864437   -4.2701783    1.2569122 ]\n",
      "   [  2.427947    -1.600872    -0.90655017   2.481265  ]\n",
      "   [  2.3424263   -7.2077713   -5.824986     2.335523  ]\n",
      "   [  1.7222499   -2.7512228   -2.3087296    1.7864217 ]\n",
      "   [  1.0004832   -7.1413507   -6.550554     0.95411247]\n",
      "   [  1.4302231   -3.8393989   -3.2885814    1.4499868 ]\n",
      "   [  1.734392    -5.0363717   -4.562415     1.7265733 ]]\n",
      "\n",
      "  [[ -1.8786137    3.8044016    1.7851309   -1.9308672 ]\n",
      "   [ -2.0427704    1.9088362    0.8611902   -2.10597   ]\n",
      "   [  0.3713629   -6.5012736   -5.5806484    0.3149101 ]\n",
      "   [  2.3904674   -0.8018907    0.54941446   2.4430196 ]\n",
      "   [  3.2833803   -5.793328    -3.5530388    3.3556385 ]\n",
      "   [  4.031575    -8.23709     -5.352422     4.1288157 ]\n",
      "   [  1.704372    -4.0030203   -2.8744233    1.746541  ]\n",
      "   [  0.55620587  -0.81578505  -0.7211592    0.5843559 ]\n",
      "   [  1.4359853   -5.1196675   -3.4192846    1.4993091 ]]\n",
      "\n",
      "  [[  1.8587611   -2.3004422   -1.999603     1.8540798 ]\n",
      "   [  0.62134135  -1.1685245   -1.0336286    0.63556284]\n",
      "   [  0.93438053  -3.9921112   -3.1974504    0.91454566]\n",
      "   [  2.386284    -8.792911    -6.874719     2.3729997 ]\n",
      "   [  2.8132436  -11.001211    -8.411129     2.7784264 ]\n",
      "   [  2.7941167   -3.5710614   -2.4098222    2.8085887 ]\n",
      "   [  2.5607827   -5.1447167   -4.0192566    2.5128086 ]\n",
      "   [  1.9819213   -4.7589607   -4.3256965    1.9169271 ]\n",
      "   [  2.1862228   -3.8017366   -2.162978     2.196947  ]]\n",
      "\n",
      "  [[  0.263768     1.3027129    0.7912128    0.31364933]\n",
      "   [  0.90733486  -1.2863759   -0.23643312   0.92466456]\n",
      "   [  1.9193958   -5.6561565   -3.624307     1.9176353 ]\n",
      "   [  2.590705    -7.7614427   -5.438415     2.6344485 ]\n",
      "   [  2.0141904   -6.001995    -3.7005918    2.02994   ]\n",
      "   [  2.3413384   -2.5982022   -0.645182     2.369621  ]\n",
      "   [  1.1849254   -0.41514406   0.7534283    1.1835097 ]\n",
      "   [  1.6196365   -2.736624    -1.3294117    1.6487104 ]\n",
      "   [  1.8510432   -2.5615194   -0.7961973    1.8711935 ]]\n",
      "\n",
      "  [[ -0.50981736   4.1422954    3.6196024   -0.43590528]\n",
      "   [  0.4603232   -1.0371166   -0.40937394   0.4394898 ]\n",
      "   [  1.2603666   -3.7127948   -2.6582818    1.2983314 ]\n",
      "   [  1.9421246   -6.2256336   -3.6375484    1.9323288 ]\n",
      "   [  1.3231326   -4.248828    -3.1260052    1.3073777 ]\n",
      "   [  2.442132    -7.4952087   -5.2700887    2.4572377 ]\n",
      "   [  2.4187922   -5.27305     -2.1958756    2.4965396 ]\n",
      "   [  1.0935532   -3.028773    -1.9892881    1.1174632 ]\n",
      "   [  1.07857     -1.6685917   -0.5144004    1.1236442 ]]]], shape=(1, 8, 9, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 9, 4)\n",
      "output.shape = (1, 8, 9, 32)\n",
      "scaled_attention.shape= (1, 9, 8, 32)\n",
      "concat_attention.shape= (1, 9, 256)\n",
      "outputs.shape= (1, 9, 256)\n",
      "(1, 9, 256)\n",
      "(1, 9, 256)\n",
      "(1, 9, 256)\n",
      "split_heads()\n",
      "(1, 9, 256)\n",
      "(1, 9, 8, 32)\n",
      "split_heads()\n",
      "(1, 9, 256)\n",
      "(1, 9, 8, 32)\n",
      "split_heads()\n",
      "(1, 9, 256)\n",
      "(1, 9, 8, 32)\n",
      "(1, 8, 9, 32)\n",
      "(1, 8, 9, 32)\n",
      "(1, 8, 9, 32)\n",
      "matmul_qk.shape = (1, 8, 9, 9)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181 -0.6147039  -0.12000973 -0.26990378\n",
      "    -0.12601104  0.12369402  0.6703813  -0.01717847]\n",
      "   [ 3.96568    -0.17723085  1.5590783  -1.8989185  -1.7120768\n",
      "    -2.4874895  -2.461606   -3.3995156  -1.7362796 ]\n",
      "   [ 3.9833748  -2.2539997   0.15140963 -2.319149    1.4055393\n",
      "     1.8976921  -0.35040918 -0.84194183  2.7204072 ]\n",
      "   [ 3.0466342  -1.2768042  -0.9543833  -2.4668362  -3.578708\n",
      "    -2.1999793  -3.10732    -1.4634132  -0.6942425 ]\n",
      "   [ 4.7013025   1.1783674   1.1767476  -4.7330627  -2.6853023\n",
      "    -3.841537   -1.8943468  -3.301475   -0.50165665]\n",
      "   [ 5.391753   -1.2895486   1.732423   -3.715419   -2.8239722\n",
      "    -3.662196   -3.4674766  -3.3219385  -0.43135288]\n",
      "   [ 4.304015    0.211817    0.16455683 -0.47811216 -3.8062627\n",
      "    -3.590859   -5.4443517  -5.1668825   1.4912223 ]\n",
      "   [ 4.4580784  -0.61394614  1.2823641   2.367739    2.35907\n",
      "     0.98633593 -3.9488287  -4.141637    1.9198743 ]\n",
      "   [ 2.5623562  -1.0149673   0.28638697 -1.415009   -1.9392455\n",
      "    -0.7987015  -3.7834046  -2.8124275  -0.08811373]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489  -0.22415894  0.44419977  0.86740375\n",
      "    -1.3531986  -0.1527121  -1.0399705   1.146371  ]\n",
      "   [ 0.22428446 -2.402899   -1.7699208  -0.35874206 -2.1430826\n",
      "    -0.03102632  0.5429309  -1.0086042  -1.0677565 ]\n",
      "   [-0.6712342   0.30168235 -3.0742364   0.19597065 -3.3459918\n",
      "    -0.10913967 -1.0004942   0.28558722 -1.2108853 ]\n",
      "   [ 1.2226754   2.9617927   2.3518116  -2.0428975   1.0853696\n",
      "    -0.76767504 -0.3099975  -1.1994996   3.250949  ]\n",
      "   [ 3.3925583   4.3809915   0.44789767 -3.760937   -1.4172314\n",
      "    -2.3714716   0.38091192  1.2854881   1.0312135 ]\n",
      "   [-2.3619554  -2.1506267  -0.96027523  3.8960817   1.8534632\n",
      "    -0.74298    -2.0000184   2.1462076  -1.9178891 ]\n",
      "   [ 0.09026945  0.7947642   0.09152631  3.2191868   1.7997355\n",
      "    -1.8118993  -1.6655155   0.09941932  0.47267178]\n",
      "   [-0.677145   -1.8837119  -1.8422472   5.2752542   0.19712047\n",
      "     0.6287616  -2.4682517  -0.7707627  -1.2582183 ]\n",
      "   [ 0.18685156 -1.0029786  -0.48849955  0.7354018  -0.60926133\n",
      "    -2.234462   -2.1479487   2.2899785  -2.94474   ]]\n",
      "\n",
      "  [[-3.0174336   0.41961712  1.9252456   0.7638777   1.4724514\n",
      "     3.0431707   2.8573203   3.2402046   2.5931482 ]\n",
      "   [-1.3549727  -0.09280814  2.5851169  -0.3922029  -0.2696607\n",
      "     0.5922087   0.36082545 -0.7003431   0.8450714 ]\n",
      "   [-1.7961591  -1.8925554  -0.41655225  2.8299882   0.4466289\n",
      "     3.4457626   0.6450706   0.9305902   1.5253757 ]\n",
      "   [-1.1154089  -1.1745822   1.2023396  -0.06076117 -1.5096283\n",
      "     0.7106908  -1.2446266  -3.0961888   2.4934134 ]\n",
      "   [-0.09934417  0.8952499  -0.6025727  -0.7601258  -4.950531\n",
      "    -0.91794896 -1.0664147  -1.4203914   0.3035166 ]\n",
      "   [-0.730198    0.4987517   0.10699637  0.04278546  0.54071033\n",
      "    -0.50309646 -3.3407922  -1.0143468   0.2559061 ]\n",
      "   [ 0.23826952  2.0639033  -2.888716   -1.358555   -0.28163734\n",
      "     2.404897    0.22699867  3.525614    3.2236834 ]\n",
      "   [ 1.2659289  -0.02524091 -2.9593556   0.11135699 -0.94128567\n",
      "    -2.829995   -3.2003918  -3.8595636  -0.99238694]\n",
      "   [-0.5888609   0.8837037  -1.3720297  -0.50547075  1.1768699\n",
      "     1.318384    1.8200378   2.1513548   4.0589194 ]]\n",
      "\n",
      "  [[-3.0523906   1.8179893   4.253165    1.4998401   2.628079\n",
      "     2.1809475   1.6847974   2.1162713   1.0263407 ]\n",
      "   [-7.7598214  -1.2395704   4.424106    5.041027    4.0303817\n",
      "     7.204144   10.943072    3.6315343   0.10899801]\n",
      "   [-5.33444    -2.9674113   1.9738437   4.328881    4.872642\n",
      "     8.156278   11.38618     1.9597439  -2.7887135 ]\n",
      "   [-1.8466234  -0.9457812   2.1802762   4.8156037   0.12329892\n",
      "     0.8021099  -0.51522094 -0.3930014   0.88665265]\n",
      "   [-3.3392012  -4.19972    -0.9612767   6.551047    3.2264879\n",
      "     5.7523003   4.0560446   1.2217602  -3.9085174 ]\n",
      "   [-1.1705554  -4.4363074  -1.1318855   3.9469442   3.668861\n",
      "     7.9511037   7.7728662   3.1097941   3.9134045 ]\n",
      "   [-0.29538846 -1.5871849  -2.8558958   4.954087    3.0537887\n",
      "     5.3664327   5.922907    0.4448749   2.3723075 ]\n",
      "   [-1.4024625  -3.924694   -1.0780026   2.6823587   2.3963602\n",
      "     4.2347217   6.6275954   2.555661    1.4216021 ]\n",
      "   [ 0.11296152  1.4904397  -0.46121475  4.180045    1.8322803\n",
      "     3.166133    1.6744213   2.8632095   5.733159  ]]\n",
      "\n",
      "  [[-0.6331823  -1.2349904   0.17108005 -1.7825468   0.4058478\n",
      "    -1.3177248   1.3472757  -1.038774   -0.5425702 ]\n",
      "   [ 2.119253   -4.246382    1.0599287  -6.814192   -1.6555526\n",
      "    -5.312355   -1.9065204  -3.2599387   0.61110854]\n",
      "   [-1.1188248   0.14757451  1.0110303   1.6372967   0.67021793\n",
      "     2.4267418  -1.4579339  -3.0462801  -1.2310774 ]\n",
      "   [ 0.95955515 -1.9993405   2.5704405  -1.7231832  -0.62369555\n",
      "    -1.1314759  -2.2201748  -2.0597012   2.0095713 ]\n",
      "   [ 0.49821588 -1.729508    2.970458   -3.6262503  -2.1428034\n",
      "     0.15667306 -1.009277   -2.750008    2.0951803 ]\n",
      "   [ 2.4125378   1.1109612   3.4186935  -5.2272053  -2.9276714\n",
      "    -4.479757   -1.5780019   1.15176     4.194589  ]\n",
      "   [ 1.147488    0.25655437  4.761012   -4.4415584   0.4498366\n",
      "    -3.3419423  -5.0206585  -2.8960762   2.6126118 ]\n",
      "   [ 3.2666113  -0.7482998  -2.4978724  -4.6047583  -0.4991162\n",
      "    -1.8868389  -4.1274176  -7.000914   -0.26274645]\n",
      "   [ 2.625219    0.4134868  -0.2386017  -2.9531605   0.06102017\n",
      "    -2.7949638  -3.0890677  -2.0160155  -0.665806  ]]\n",
      "\n",
      "  [[-1.2599558   1.0058122   0.9403136  -2.169106    1.3649019\n",
      "     0.05476408 -0.2606994   0.6737088   0.974384  ]\n",
      "   [ 1.9198732  -0.84344107  2.0494642  -1.7659745   1.8153642\n",
      "    -1.3940618  -1.7129728   0.6147069  -0.501224  ]\n",
      "   [ 2.8428354  -1.6724693  -1.4574461  -0.75728846 -2.8203032\n",
      "    -4.540737   -4.3868213  -0.7501175  -0.68200713]\n",
      "   [ 1.2147404   1.865701   -1.4707285  -1.3224499  -2.9885063\n",
      "    -0.7098579  -2.4524012   0.32578066 -0.1086136 ]\n",
      "   [ 2.2408023   0.45182097  0.23735748 -3.630287   -4.9238787\n",
      "    -1.6159724  -4.2166977  -0.00287914  1.6390876 ]\n",
      "   [ 2.0514302   1.124743   -0.66148055 -3.6998794  -3.634573\n",
      "    -3.2806454  -3.53534    -0.587686    0.4574493 ]\n",
      "   [ 1.8713237   1.459809    4.544932    0.6970329   0.3698051\n",
      "    -1.5995883  -3.4667106  -2.5332448  -1.2041903 ]\n",
      "   [ 0.7016293   0.6996512  -0.24522036 -1.6627809   1.9750067\n",
      "    -0.02285164 -0.6037045  -1.2260524   0.3857271 ]\n",
      "   [-0.00567898 -1.5558052   0.70907253  0.3123517  -0.03035606\n",
      "    -0.63739645  2.4996793   0.65522164  0.10999125]]\n",
      "\n",
      "  [[-2.1587913   1.7147038   1.544837    2.9021912  -0.54687065\n",
      "     0.69791436  0.48692545  0.3874992   1.747807  ]\n",
      "   [-0.00582398 -2.3462925   1.799922    0.32839322  0.3320445\n",
      "    -2.016989   -2.8966556  -2.9691656   0.9186906 ]\n",
      "   [-2.1043587  -2.8178024   2.2148285   3.891037    2.2759385\n",
      "     2.491633    0.34263963  3.354882    2.579082  ]\n",
      "   [ 1.8959482  -3.3403587   2.0201633  -2.8513105   2.2833853\n",
      "    -1.5907921  -5.267708   -0.7608538  -1.667431  ]\n",
      "   [ 1.2664658  -1.2141082   0.86169004 -0.82817227  1.8247795\n",
      "    -2.2689962  -2.9155438  -0.7371273  -1.1245675 ]\n",
      "   [ 1.4200376  -0.8142122  -0.15309595 -2.757525   -0.03373783\n",
      "    -3.5767376  -0.7845442   0.7012546   0.22206096]\n",
      "   [ 0.9523497   2.5999942  -0.72424364 -1.2373599  -0.06541023\n",
      "    -2.280726   -0.30826616 -1.5803338  -0.43647492]\n",
      "   [-0.66845006 -1.2726071   0.88486147 -0.17885678  0.5529859\n",
      "     0.5443401   1.7109423   0.3173334   0.40885562]\n",
      "   [-1.6181766   2.127004   -3.3242197   2.8492422  -2.3754318\n",
      "    -1.4359603  -2.8194427   0.7514017  -1.035433  ]]\n",
      "\n",
      "  [[-2.0107505   2.0400176   1.5965837   0.41086924  2.151286\n",
      "     2.0080416   3.8806849   3.7532885   0.60808486]\n",
      "   [-1.2814682  -2.6879327   1.4795719   0.70971787  1.2221687\n",
      "     4.831139    4.012818    3.0739448  -0.3462172 ]\n",
      "   [ 0.3697977   0.11362538 -0.8682849   1.8676807   1.5285181\n",
      "    -2.5126774  -0.09541667 -0.62111366 -0.5378774 ]\n",
      "   [ 0.17680757 -0.59016186 -0.07212502  1.788976    1.8290929\n",
      "     1.2414881   2.5110507   0.9378911   1.4032513 ]\n",
      "   [ 0.27183303 -2.8065841  -0.40462255 -2.3706849  -0.47989556\n",
      "    -0.17444041  1.5252     -0.41247755  0.18653843]\n",
      "   [ 0.8829943  -3.017673   -2.083954    2.1678572   2.4494154\n",
      "    -0.16720197  2.1018574  -0.03369299 -0.08712415]\n",
      "   [ 0.36083797 -0.05401262  0.63395387  1.9816741   1.4668022\n",
      "     2.7947261   0.7702297   0.40087664 -1.3949666 ]\n",
      "   [ 0.28325427 -1.9323467   0.58271986  2.48945     0.9275244\n",
      "     2.8025713   1.751703    0.5133646  -2.6412737 ]\n",
      "   [-1.0369177  -0.8422439  -0.00286578 -1.2839406  -0.02112548\n",
      "     3.2767828   3.8468044   4.8574357  -0.255869  ]]]], shape=(1, 8, 9, 9), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 9, 9)\n",
      "output.shape = (1, 8, 9, 32)\n",
      "scaled_attention.shape= (1, 9, 8, 32)\n",
      "concat_attention.shape= (1, 9, 256)\n",
      "outputs.shape= (1, 9, 256)\n",
      "(1, 9, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 9, 256)\n",
      "(1, 9, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 9, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 9, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.5176607    1.9687198    2.0989769   -0.47190297]\n",
      "   [ -0.49124843   0.08534324   0.2991765   -0.48528743]\n",
      "   [  0.24215932  -4.697081    -4.019181     0.1927161 ]\n",
      "   [  0.7396501   -3.0292149   -2.48159      0.7256521 ]\n",
      "   [  0.7377838   -1.2575825   -1.0456651    0.7396505 ]\n",
      "   [  0.12743154  -2.1013763   -1.7714081    0.15186152]\n",
      "   [  0.57692707  -1.6948001   -1.6039941    0.56792045]\n",
      "   [ -0.0456275   -3.2555232   -2.8839726   -0.07279108]\n",
      "   [  0.541922    -4.2346587   -4.088871     0.5205673 ]]\n",
      "\n",
      "  [[ -0.5409756    2.8193953    2.0520196   -0.5236139 ]\n",
      "   [ -1.0098997    4.226584     3.5640724   -0.9722929 ]\n",
      "   [ -0.31559435  -2.7033126   -2.1731372   -0.34413084]\n",
      "   [  0.67385     -4.3054934   -2.5939085    0.6549963 ]\n",
      "   [  0.7716512   -4.6774607   -3.4361272    0.73228556]\n",
      "   [  0.30699015  -2.0128524   -1.77724      0.3016845 ]\n",
      "   [  0.95172566  -5.123536    -3.7028008    0.9193312 ]\n",
      "   [  0.1625587   -2.8245268   -2.2594504    0.1333035 ]\n",
      "   [  0.80569774  -2.9156864   -1.9677489    0.81221676]]\n",
      "\n",
      "  [[ -0.42360795   3.4683292    3.2733347   -0.39263082]\n",
      "   [ -0.47245097   0.7355631    1.3526613   -0.47366753]\n",
      "   [  0.8764782   -5.054033    -4.6074905    0.832105  ]\n",
      "   [  1.5158468   -5.5036316   -5.460711     1.4665604 ]\n",
      "   [  1.3198522   -4.7758727   -4.838473     1.2815847 ]\n",
      "   [  0.6756115   -5.767558    -5.86507      0.60872716]\n",
      "   [  0.6050416   -6.19143     -6.4390326    0.54084617]\n",
      "   [  0.44500238  -0.68225163  -0.9241052    0.42594016]\n",
      "   [  1.465985    -3.2105703   -2.976426     1.4414983 ]]\n",
      "\n",
      "  [[ -1.6453142    4.41468      3.7185764   -1.6388824 ]\n",
      "   [  0.1504475   -1.3770971   -0.84712774   0.10965424]\n",
      "   [  1.5174129   -5.9414515   -4.9204936    1.4784044 ]\n",
      "   [  1.9338356   -4.261786    -3.374979     1.932654  ]\n",
      "   [  1.5312709   -4.5473847   -3.4884474    1.480693  ]\n",
      "   [  2.4686995   -5.566623    -4.674686     2.4672065 ]\n",
      "   [  2.0171244   -5.156834    -4.0178304    1.9925612 ]\n",
      "   [  1.4369041   -3.667318    -2.7229993    1.4106048 ]\n",
      "   [  1.7216051   -4.4552937   -3.1465628    1.6921057 ]]\n",
      "\n",
      "  [[ -1.29121      2.8940606    1.9502285   -1.3604355 ]\n",
      "   [  0.00294916  -1.2568096   -0.89668256  -0.00931877]\n",
      "   [  0.7530518   -2.5598295   -1.9020394    0.7952453 ]\n",
      "   [  1.7065613   -5.794166    -4.5830455    1.7274754 ]\n",
      "   [  1.828848    -7.046152    -5.713792     1.8388789 ]\n",
      "   [  2.3361118   -9.356349    -7.713717     2.356966  ]\n",
      "   [  2.4712012   -7.777283    -6.1303487    2.5265574 ]\n",
      "   [  2.178688    -7.1766615   -5.927077     2.238128  ]\n",
      "   [  2.4477205   -5.1921487   -3.4574718    2.464835  ]]\n",
      "\n",
      "  [[ -0.65288585   3.4191878    2.6229937   -0.6571499 ]\n",
      "   [  0.72896236  -2.1471467   -1.9732635    0.77281165]\n",
      "   [  0.32350188  -1.2035745   -1.7607827    0.34396917]\n",
      "   [  1.0276341    1.058868     1.2416905    1.0519707 ]\n",
      "   [  1.6617185   -1.5399216   -1.5241555    1.7414258 ]\n",
      "   [  2.3202841   -3.4387836   -2.1360145    2.3670495 ]\n",
      "   [  2.2568145   -5.2002926   -3.3399537    2.3179612 ]\n",
      "   [  2.10796     -4.739115    -2.9632866    2.1519184 ]\n",
      "   [  1.2009804   -1.6315452   -1.1646751    1.2423136 ]]\n",
      "\n",
      "  [[ -0.13920632   1.920276     1.8218281   -0.1018336 ]\n",
      "   [  0.19900118  -1.852921    -1.364047     0.17529728]\n",
      "   [  0.9201308   -2.3837185   -1.2430844    0.8900646 ]\n",
      "   [  1.0384122   -2.2614129   -0.6760908    1.014745  ]\n",
      "   [  1.3033148   -4.046018    -2.5787446    1.2661915 ]\n",
      "   [  1.1603479   -2.2507179   -1.0619235    1.1338618 ]\n",
      "   [  1.041217    -4.2461724   -2.9057727    1.0158387 ]\n",
      "   [  1.0148015   -1.7258983   -0.37147498   1.0063083 ]\n",
      "   [  0.88598007  -2.206053    -0.84811103   0.8885484 ]]\n",
      "\n",
      "  [[ -1.5968649    5.382058     4.5500836   -1.5562288 ]\n",
      "   [  0.86753845  -4.5697775   -4.1758018    0.8370436 ]\n",
      "   [  1.9240823   -9.450348    -8.618066     1.8469068 ]\n",
      "   [  1.4563193   -6.307143    -5.9952025    1.4192016 ]\n",
      "   [  1.9546863   -5.530293    -5.110774     1.9280047 ]\n",
      "   [  1.7568917   -6.1661077   -5.8179903    1.6855166 ]\n",
      "   [  3.4059234  -12.176341   -11.224475     3.3215392 ]\n",
      "   [  1.7747686   -8.078717    -7.356744     1.7220855 ]\n",
      "   [  2.41111     -7.0833406   -6.4738836    2.3771992 ]]]], shape=(1, 8, 9, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 9, 4)\n",
      "output.shape = (1, 8, 9, 32)\n",
      "scaled_attention.shape= (1, 9, 8, 32)\n",
      "concat_attention.shape= (1, 9, 256)\n",
      "outputs.shape= (1, 9, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 10, 256)\n",
      "(1, 10, 256)\n",
      "(1, 10, 256)\n",
      "split_heads()\n",
      "(1, 10, 256)\n",
      "(1, 10, 8, 32)\n",
      "split_heads()\n",
      "(1, 10, 256)\n",
      "(1, 10, 8, 32)\n",
      "split_heads()\n",
      "(1, 10, 256)\n",
      "(1, 10, 8, 32)\n",
      "(1, 8, 10, 32)\n",
      "(1, 8, 10, 32)\n",
      "(1, 8, 10, 32)\n",
      "matmul_qk.shape = (1, 8, 10, 10)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    0.34484416   0.3515555    0.8269838   -0.1325054\n",
      "      1.1541476    0.36659813   0.6267392    0.7029531    1.932782  ]\n",
      "   [ -1.5524524    0.57553977   1.342203     0.7264702    0.67869383\n",
      "     -2.3639412    0.21526393  -1.575456     4.0412474    3.7598073 ]\n",
      "   [  0.17656448  -2.1603198   -3.1747456   -2.3878162   -0.21427149\n",
      "     -2.5600047   -1.3389277    0.00445623  -1.037421    -3.3319168 ]\n",
      "   [ -0.27855322  -2.3594024    0.28350914  -6.3847156    2.1768348\n",
      "      0.48724338   4.0435467    0.8452997    2.298873     4.9464703 ]\n",
      "   [ -0.5387738   -1.1847006   -2.5186715    3.639655    -2.1242177\n",
      "      8.928502     3.1669946    5.1829453    4.105474     9.427159  ]\n",
      "   [ -0.30547413  -1.4707135    0.06941595   0.72953105   1.0991143\n",
      "      2.2755632    3.7240849    3.374271     3.1669166    2.325922  ]\n",
      "   [ -0.0702149   -2.969282    -3.383008     1.3616961    0.42645463\n",
      "     16.02929      8.762581     8.823401     3.3795218    4.6411085 ]\n",
      "   [ -0.20550568  -0.10201316  -2.2742054   -2.1785567   -1.7485399\n",
      "      2.4331734    1.2430574    4.5232897    0.90502656  -2.2167747 ]\n",
      "   [ -0.590271     1.0130959   -0.09686741   0.5674616    0.8100412\n",
      "      0.53022087   0.5689679    5.691497     0.52202976   1.8942038 ]\n",
      "   [  0.25923175  -3.3190057   -3.0277622   -5.676654     1.3393734\n",
      "     -3.3025331    2.851563     4.2579527   -0.4028998   -8.078694  ]]\n",
      "\n",
      "  [[ -0.1123142    0.09086403  -0.2228786   -0.18202324   0.14455257\n",
      "     -0.04013486  -0.48076916  -0.06074027  -0.04329262   0.18260159]\n",
      "   [ -0.27778682  -7.685327    -0.20150177   3.3823       3.8902612\n",
      "      4.337893     1.4037256   -1.6104152    0.8058254   -0.02028271]\n",
      "   [  0.30459622  -1.6710656   -3.094567    -1.6013079    0.79777056\n",
      "      0.13481723  -1.8365917   -0.71397376  -0.33064502   2.8611202 ]\n",
      "   [ -0.03799488  -4.1970086    2.696054    -2.7333486    1.6560552\n",
      "      4.530645     4.271422    -1.2604878   -2.5451972    1.8992972 ]\n",
      "   [  0.6443627   -3.9118009    1.1981791   -4.1091504   -6.4226418\n",
      "     -0.9858088   -1.1690059   -0.22695212   1.5001154   -1.1231505 ]\n",
      "   [  0.82783276  -3.7707963   -2.1052582   -2.503423    -4.9312515\n",
      "     -4.1483335   -5.133481    -1.3080887   -1.481337    -5.980171  ]\n",
      "   [  0.796586    -0.30842566  -2.7917175   -4.5511785   -0.32458684\n",
      "     -0.10727115  -6.259451     1.659165     1.4761349   -1.4687202 ]\n",
      "   [  0.7033243   -0.6943296    0.38815764   1.9749643   -4.1990952\n",
      "     -3.2755108   -5.034937    -3.387448     3.127423    -5.54149   ]\n",
      "   [ -0.6092106   -2.0272765   -0.859915     2.4829376    4.411381\n",
      "      5.087132     1.9951113    2.134214    -1.0122576    3.2323248 ]\n",
      "   [  0.0895261   -3.0075777    2.4791887   -0.3216253    6.745585\n",
      "      5.4583244    4.826457    -0.94258535   4.3627224    2.697197  ]]\n",
      "\n",
      "  [[ -0.10918517  -0.21059825   0.3513798    0.34551156   0.83573407\n",
      "      0.38043547   0.42571396  -0.10994706   0.73884016   0.11337968]\n",
      "   [  0.2860052   -5.063574    -2.1691      -3.5214968   -3.319986\n",
      "      1.6807239    2.6336646   -3.6028974    1.4836615    0.80234605]\n",
      "   [  0.6062982    2.719351    -4.9581633   -1.0914348   -1.6627623\n",
      "     -0.31062695  -2.5335972    0.98369557  -2.88995     -1.7919333 ]\n",
      "   [  0.13651355  -1.777794     3.1865156   -0.07830478  -1.239388\n",
      "      1.7914739    2.9210606   -1.4029268    1.2590778   -1.0561415 ]\n",
      "   [ -0.21224323  -1.5428834    7.991749    -0.5730239    1.2063524\n",
      "      3.6277044    1.6937939    1.8189023    3.786629     0.13888861]\n",
      "   [ -0.85228556  -2.2769668    6.909945     3.543675     3.6550546\n",
      "      9.817229     3.752735     0.8422704    5.880493    -2.0402415 ]\n",
      "   [  0.18337622  -3.708946     1.3114123   -0.97666943   0.6310309\n",
      "      0.9075615   -3.0031114   -0.5886862   -0.67060435  -1.9287843 ]\n",
      "   [ -0.1485042   -3.7088253   -2.2962284   -1.5428704    0.04910947\n",
      "      4.634492     2.4989789   -4.185793     3.9252877   -1.8230985 ]\n",
      "   [  0.3454023   -4.214501    -4.5776362   -4.2041674   -5.6266522\n",
      "     -8.447932    -3.633056     1.5147972   -4.597967     3.3158236 ]\n",
      "   [  0.00192533   4.0501933   -0.5385378   -4.8801765   -4.5654726\n",
      "     -5.769101    -2.7084713   -1.0551419    0.06899593  -6.671878  ]]\n",
      "\n",
      "  [[ -0.22114104  -0.3915212    0.13786234   0.33271423   0.6211036\n",
      "      1.0825356   -0.11799879   0.72872204  -0.1314376   -0.34905735]\n",
      "   [  0.14206643   1.1752353    3.6785405    1.6119353   -0.9453702\n",
      "     -3.2077594   -2.725474     1.8995842   -0.996005     3.998312  ]\n",
      "   [ -0.07449952  -3.5981474   -0.31350046   1.4361463    2.564271\n",
      "      2.474412     0.16766883  -0.5836961    1.1178291   -3.7098937 ]\n",
      "   [ -0.1049557    0.70436597   0.5221487   -0.21612936   0.9116111\n",
      "     -0.48090836   0.18603255  -1.9520314    3.4277797   -0.04625581]\n",
      "   [ -0.310017    -2.4498558    2.5756524    1.8039283   -0.97895104\n",
      "      3.0719118    0.6960478    3.8471358    4.3071675    1.5473081 ]\n",
      "   [ -0.39245275  -5.124641     3.6571612    1.2725205    3.1442227\n",
      "      0.02843648  -1.5512888    2.58859     -0.89740974   3.4971538 ]\n",
      "   [ -0.5742471   -4.852531     1.5101644    2.5168405    3.3431916\n",
      "      9.560527     0.73367864   4.908349     4.149948     1.482831  ]\n",
      "   [ -0.97490954  -4.6069136   -1.7809725    1.4556999    4.3725533\n",
      "      9.8998165    0.52443993   0.8263054    0.9018291   -3.7931354 ]\n",
      "   [ -0.5735989   -4.0548587    0.27316695  -2.4717047    3.2703655\n",
      "      3.5540314    0.07714308   8.548014     3.0001826    1.4996339 ]\n",
      "   [  0.19105694  -1.2941034    1.8261616   -1.3133595    0.79539776\n",
      "      0.04804294  -0.99305815   3.3403172   -0.65854585  -1.1529343 ]]\n",
      "\n",
      "  [[  0.00076126  -0.1641996   -0.14196222  -0.19410133   0.00823206\n",
      "      0.61542094  -0.08737361  -0.10085457  -0.06919557  -0.05508299]\n",
      "   [  0.78445697  -5.048575     3.2536209   -0.97517645   0.36774525\n",
      "     -3.194747    -0.04737727  -0.09847949   0.45832372  -1.1919119 ]\n",
      "   [  0.2599421   -2.3611805    4.198083    -0.59922963  -2.4801793\n",
      "     -5.2311      -3.3485384   -2.4956753   -3.0119252   -0.40281573]\n",
      "   [  0.203725     3.9226558    1.8656771    0.77520037  -3.1088407\n",
      "     -2.945639     1.2251574    1.5569918   -1.4664475   -0.32573247]\n",
      "   [  0.6548166    0.10552962   1.3786814   -2.6986282   -3.3625326\n",
      "     -3.5189183   -7.556854    -3.5280879   -4.3273387    0.14598486]\n",
      "   [  0.85031134  -2.967739     2.643215    -0.4758638    1.1578159\n",
      "     -8.628843    -5.5761604   -4.848856    -2.0556912   -4.075636  ]\n",
      "   [  0.01436896   0.14466181  -2.1695178    2.4206777    8.998847\n",
      "     -1.5658671   -0.12156205  -0.9806287    3.9562185   -0.5530879 ]\n",
      "   [  0.56318194  -4.2370687    1.5977541    1.2324364    2.794963\n",
      "     -3.8853424   -3.6924665   -4.2410336   -0.9847575   -1.2085614 ]\n",
      "   [  0.30105153  -1.6933879   -2.020751    -1.2294377   -0.5758017\n",
      "     -4.3765674    0.27936047  -4.3569603   -5.729614    -6.6753893 ]\n",
      "   [  0.76540506   0.15341733   2.4403636   -2.3432696   -2.6396062\n",
      "     -1.03931      2.470319     4.2786593   -1.5197265   -1.8448883 ]]\n",
      "\n",
      "  [[ -0.00999826  -0.5171405    0.3842402   -0.26599303  -0.17592163\n",
      "      0.08763131   0.29407236   0.11240291   0.06468003   0.20075996]\n",
      "   [  0.3875343    2.307618    -2.0652568    0.06119107  -1.376965\n",
      "     -0.84213823   1.6010706   -1.9992107    3.9152644    2.9078257 ]\n",
      "   [  0.5494998    0.9012554   -5.717646    -0.5140254   -1.9746116\n",
      "     -4.1329675    0.38239804  -3.9651098    0.4975535    1.9827068 ]\n",
      "   [ -0.09110849   1.9358311   -2.1756437   -0.7149111   -1.2668245\n",
      "      0.23427002  -0.5892781    1.0475453    4.2543564    0.953046  ]\n",
      "   [ -0.99071145   6.797044    -1.4787755    4.8537555   -0.8091358\n",
      "      1.0001513   -0.56871563  -1.046842     7.9517646    4.4362655 ]\n",
      "   [  0.1653099   -0.6100134    4.8626056    0.720647     0.9629525\n",
      "     -1.3203751   -4.107916     2.463625    -1.0382884   -0.11817995]\n",
      "   [ -0.4776169   -4.275887     1.1203661    0.41609427   4.5017853\n",
      "     -0.39336598  -2.3056731   -1.7078637   -0.52901644   1.9856396 ]\n",
      "   [  0.88401735  -3.8715084   -1.8880895   -3.6033385   -1.9594375\n",
      "      1.1121889   -1.3816634   -1.135376    -3.8049717   -7.2075057 ]\n",
      "   [  1.1133462   -6.4942994   -0.6978988   -4.9526157   -5.715827\n",
      "     -5.9033365   -5.149199    -1.8298197   -7.6704     -10.312497  ]\n",
      "   [  0.48386398  -3.646351    -3.5835953   -2.0325422   -5.247802\n",
      "     -1.5264621   -1.6238784   -2.1048608   -0.827515    -7.4019194 ]]\n",
      "\n",
      "  [[ -0.20863976   0.0058533    0.141982     0.3954799    0.70690477\n",
      "      0.8365328    0.28462148  -0.24421918   0.6515544    0.92187274]\n",
      "   [ -0.5011563   -5.83805      3.0883057    2.4906335    2.8664675\n",
      "      3.39397      0.8982007   -2.3877478    5.1261563    3.4072495 ]\n",
      "   [ -0.08541125   2.1106565    0.02025323   0.89557755   1.3944312\n",
      "      4.8944817    3.2451954    4.2954817    0.5471952   -0.92864716]\n",
      "   [  1.3930423    2.344757    -1.3677286   -6.293937    -6.4745502\n",
      "     -5.8228016   -2.4453523   -2.055828    -3.1815403   -8.232062  ]\n",
      "   [  2.1056752   -0.87906635  -3.2462146   -5.5659313   -6.810574\n",
      "    -12.417996    -5.3918004   -2.5989642  -11.401493    -7.451914  ]\n",
      "   [  0.7307259   -0.62191457   2.2546957   -4.717381    -0.30207893\n",
      "     -5.2433743   -4.031666     0.92758286  -2.1127427   -1.417971  ]\n",
      "   [  2.2975056   -3.7142315   -2.9478807  -10.469339    -5.418616\n",
      "    -10.1225815   -7.3040056    0.4335872   -9.478246    -4.2933764 ]\n",
      "   [ -0.6196739   -1.8421621    1.7395886    2.5982852    5.356473\n",
      "      0.73179406   1.0359558   -3.0025475    4.908996     3.7934012 ]\n",
      "   [  1.2433432    0.35926548  -1.4999105   -1.8502873   -3.6189969\n",
      "     -9.892432    -1.9395131   -0.6860688   -8.132981    -3.592384  ]\n",
      "   [  1.4510003    1.0021973   -2.8439877   -6.589478    -4.991755\n",
      "    -11.815946    -5.369118    -0.26158586  -7.02527    -10.053152  ]]\n",
      "\n",
      "  [[ -0.15726982   0.34343007  -0.0106815    0.2812659    0.26918355\n",
      "      0.6895108    0.5193374    0.17895888  -0.26491943   0.13644844]\n",
      "   [  0.42748785  -3.541775    -4.416529    -1.5682534   -0.30482742\n",
      "      0.77651656   1.8078052    1.8106172    3.8573866    1.6207062 ]\n",
      "   [  0.09384713   3.2008502   -4.3988295   -1.0193433   -1.7561591\n",
      "     -0.20635648   1.7356048   -0.08922087   0.29839948  -2.692548  ]\n",
      "   [  0.00172196  -0.81685966   2.484785    -2.7307875    4.4621997\n",
      "      2.2167425    0.05664166  -2.2540293   -2.799476     2.9115417 ]\n",
      "   [ -0.26568195   6.4090133    0.19427305   4.194164    -5.8646803\n",
      "      0.7492719    3.420403     4.213038     3.715512     2.1433942 ]\n",
      "   [  0.15044671  -0.83800465   5.2889094    3.357029     2.2839093\n",
      "     -3.226359     1.4485415   -1.6294129    1.074201    -2.3407726 ]\n",
      "   [  0.5396642   -4.57709     -2.997078    -1.2818362   -2.02568\n",
      "     -0.3638186    4.7706485    3.2261612    0.503986    -0.5691261 ]\n",
      "   [ -0.00558216  -3.0874553   -2.459048     0.15172434   2.5665867\n",
      "      1.0365962    5.3165255   -1.6244347    1.7662504   -0.5312396 ]\n",
      "   [ -0.08250386   0.13179336  -0.65814525  -2.291554    -0.36036602\n",
      "      1.4784904    0.8082328   -0.5854646   -3.5685499    2.199129  ]\n",
      "   [  0.4018605   -0.47903845  -3.2845047   -4.582873    -2.3509977\n",
      "     -1.8646562    4.6285706   -1.8439622    2.0653298   -3.029793  ]]]], shape=(1, 8, 10, 10), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 10, 10)\n",
      "output.shape = (1, 8, 10, 32)\n",
      "scaled_attention.shape= (1, 10, 8, 32)\n",
      "concat_attention.shape= (1, 10, 256)\n",
      "outputs.shape= (1, 10, 256)\n",
      "(1, 10, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 10, 256)\n",
      "(1, 10, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 10, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 10, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.6510173   -2.8665452   -1.9706712    0.6258339 ]\n",
      "   [ -1.2761326   -0.6591419   -0.772127    -1.2929033 ]\n",
      "   [  1.0074586   -3.2963974   -3.106445     0.9872349 ]\n",
      "   [  1.2579683   -4.881622    -3.767476     1.2469316 ]\n",
      "   [  1.3234984   -5.9344664   -5.2370906    1.2870476 ]\n",
      "   [  1.7000594   -3.6593008   -2.971424     1.7176253 ]\n",
      "   [  1.2776281   -1.9751713   -1.519901     1.2974825 ]\n",
      "   [  0.58336854  -1.1657993   -0.539427     0.5778812 ]\n",
      "   [  0.7600763   -0.7146235    0.06742804   0.76477766]\n",
      "   [  0.68825406   0.40578374   1.1206802    0.7197399 ]]\n",
      "\n",
      "  [[  1.5699102   -4.8338065   -2.8935153    1.5974954 ]\n",
      "   [  1.602976    -6.0585413   -4.3626466    1.6033887 ]\n",
      "   [  0.84273714  -3.3321726   -3.019569     0.8300638 ]\n",
      "   [  1.739568    -5.32123     -4.1527953    1.7057066 ]\n",
      "   [  1.9442569   -4.8756948   -3.3264403    1.9580569 ]\n",
      "   [  3.9294844  -11.821626    -9.099886     3.8940978 ]\n",
      "   [  2.184128    -5.056767    -3.4162076    2.1828146 ]\n",
      "   [  1.9612429   -4.841177    -3.8993757    1.9119158 ]\n",
      "   [  2.1313474   -8.619019    -7.0290985    2.1060083 ]\n",
      "   [  1.7264353   -4.697804    -4.539785     1.657041  ]]\n",
      "\n",
      "  [[ -0.9479617   -1.5584124   -2.3440182   -0.91537315]\n",
      "   [  1.1684959   -6.083402    -5.361126     1.1414931 ]\n",
      "   [  0.8458585   -8.91149     -7.7787905    0.78298134]\n",
      "   [  2.5803494   -5.880795    -5.0921974    2.5411623 ]\n",
      "   [  3.3461082  -11.678779   -10.798813     3.272511  ]\n",
      "   [  3.3453295   -7.0702214   -5.903679     3.2953756 ]\n",
      "   [  1.6342559   -4.6711273   -4.3739734    1.6045712 ]\n",
      "   [  1.7710527   -4.211759    -3.7379825    1.7677531 ]\n",
      "   [  2.3430088   -5.5874043   -4.7693295    2.3498065 ]\n",
      "   [  1.7600528   -4.8194313   -3.658041     1.7406274 ]]\n",
      "\n",
      "  [[ -0.53032047  -2.1487055   -3.3869808   -0.5465881 ]\n",
      "   [ -0.37673792   0.5297353    0.7943683   -0.41485333]\n",
      "   [  1.2794939   -4.2864437   -4.2701783    1.2569122 ]\n",
      "   [  2.427947    -1.600872    -0.90655017   2.481265  ]\n",
      "   [  2.3424263   -7.2077713   -5.824986     2.335523  ]\n",
      "   [  1.7222499   -2.7512228   -2.3087296    1.7864217 ]\n",
      "   [  1.0004832   -7.1413507   -6.550554     0.95411247]\n",
      "   [  1.4302231   -3.8393989   -3.2885814    1.4499868 ]\n",
      "   [  1.734392    -5.0363717   -4.562415     1.7265733 ]\n",
      "   [  1.7437402   -4.61876     -3.9143934    1.71687   ]]\n",
      "\n",
      "  [[ -1.8786137    3.8044016    1.7851309   -1.9308672 ]\n",
      "   [ -2.0427704    1.9088362    0.8611902   -2.10597   ]\n",
      "   [  0.3713629   -6.5012736   -5.5806484    0.3149101 ]\n",
      "   [  2.3904674   -0.8018907    0.54941446   2.4430196 ]\n",
      "   [  3.2833803   -5.793328    -3.5530388    3.3556385 ]\n",
      "   [  4.031575    -8.23709     -5.352422     4.1288157 ]\n",
      "   [  1.704372    -4.0030203   -2.8744233    1.746541  ]\n",
      "   [  0.55620587  -0.81578505  -0.7211592    0.5843559 ]\n",
      "   [  1.4359853   -5.1196675   -3.4192846    1.4993091 ]\n",
      "   [  3.066259    -8.724568    -6.3771715    3.138459  ]]\n",
      "\n",
      "  [[  1.8587611   -2.3004422   -1.999603     1.8540798 ]\n",
      "   [  0.62134135  -1.1685245   -1.0336286    0.63556284]\n",
      "   [  0.93438053  -3.9921112   -3.1974504    0.91454566]\n",
      "   [  2.386284    -8.792911    -6.874719     2.3729997 ]\n",
      "   [  2.8132436  -11.001211    -8.411129     2.7784264 ]\n",
      "   [  2.7941167   -3.5710614   -2.4098222    2.8085887 ]\n",
      "   [  2.5607827   -5.1447167   -4.0192566    2.5128086 ]\n",
      "   [  1.9819213   -4.7589607   -4.3256965    1.9169271 ]\n",
      "   [  2.1862228   -3.8017366   -2.162978     2.196947  ]\n",
      "   [  2.025622    -0.50651217  -0.09534571   2.0341551 ]]\n",
      "\n",
      "  [[  0.263768     1.3027129    0.7912128    0.31364933]\n",
      "   [  0.90733486  -1.2863759   -0.23643312   0.92466456]\n",
      "   [  1.9193958   -5.6561565   -3.624307     1.9176353 ]\n",
      "   [  2.590705    -7.7614427   -5.438415     2.6344485 ]\n",
      "   [  2.0141904   -6.001995    -3.7005918    2.02994   ]\n",
      "   [  2.3413384   -2.5982022   -0.645182     2.369621  ]\n",
      "   [  1.1849254   -0.41514406   0.7534283    1.1835097 ]\n",
      "   [  1.6196365   -2.736624    -1.3294117    1.6487104 ]\n",
      "   [  1.8510432   -2.5615194   -0.7961973    1.8711935 ]\n",
      "   [  2.0705955   -4.3488426   -2.0247169    2.1132114 ]]\n",
      "\n",
      "  [[ -0.50981736   4.1422954    3.6196024   -0.43590528]\n",
      "   [  0.4603232   -1.0371166   -0.40937394   0.4394898 ]\n",
      "   [  1.2603666   -3.7127948   -2.6582818    1.2983314 ]\n",
      "   [  1.9421246   -6.2256336   -3.6375484    1.9323288 ]\n",
      "   [  1.3231326   -4.248828    -3.1260052    1.3073777 ]\n",
      "   [  2.442132    -7.4952087   -5.2700887    2.4572377 ]\n",
      "   [  2.4187922   -5.27305     -2.1958756    2.4965396 ]\n",
      "   [  1.0935532   -3.028773    -1.9892881    1.1174632 ]\n",
      "   [  1.07857     -1.6685917   -0.5144004    1.1236442 ]\n",
      "   [  1.9439204   -4.8428583   -2.7191334    2.0016615 ]]]], shape=(1, 8, 10, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 10, 4)\n",
      "output.shape = (1, 8, 10, 32)\n",
      "scaled_attention.shape= (1, 10, 8, 32)\n",
      "concat_attention.shape= (1, 10, 256)\n",
      "outputs.shape= (1, 10, 256)\n",
      "(1, 10, 256)\n",
      "(1, 10, 256)\n",
      "(1, 10, 256)\n",
      "split_heads()\n",
      "(1, 10, 256)\n",
      "(1, 10, 8, 32)\n",
      "split_heads()\n",
      "(1, 10, 256)\n",
      "(1, 10, 8, 32)\n",
      "split_heads()\n",
      "(1, 10, 256)\n",
      "(1, 10, 8, 32)\n",
      "(1, 8, 10, 32)\n",
      "(1, 8, 10, 32)\n",
      "(1, 8, 10, 32)\n",
      "matmul_qk.shape = (1, 8, 10, 10)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181 -0.6147039  -0.12000973 -0.26990378\n",
      "    -0.12601104  0.12369402  0.6703813  -0.01717847  1.4070063 ]\n",
      "   [ 3.96568    -0.17723085  1.5590783  -1.8989185  -1.7120768\n",
      "    -2.4874895  -2.461606   -3.3995156  -1.7362796   0.44090855]\n",
      "   [ 3.9833748  -2.2539997   0.15140963 -2.319149    1.4055393\n",
      "     1.8976921  -0.35040918 -0.84194183  2.7204072  -1.7445444 ]\n",
      "   [ 3.0466342  -1.2768042  -0.9543833  -2.4668362  -3.578708\n",
      "    -2.1999793  -3.10732    -1.4634132  -0.6942425  -2.7284784 ]\n",
      "   [ 4.7013025   1.1783674   1.1767476  -4.7330627  -2.6853023\n",
      "    -3.841537   -1.8943468  -3.301475   -0.50165665 -3.9194427 ]\n",
      "   [ 5.391753   -1.2895486   1.732423   -3.715419   -2.8239722\n",
      "    -3.662196   -3.4674766  -3.3219385  -0.43135288 -2.239058  ]\n",
      "   [ 4.304015    0.211817    0.16455683 -0.47811216 -3.8062627\n",
      "    -3.590859   -5.4443517  -5.1668825   1.4912223   0.24744101]\n",
      "   [ 4.4580784  -0.61394614  1.2823641   2.367739    2.35907\n",
      "     0.98633593 -3.9488287  -4.141637    1.9198743  -2.5438845 ]\n",
      "   [ 2.5623562  -1.0149673   0.28638697 -1.415009   -1.9392455\n",
      "    -0.7987015  -3.7834046  -2.8124275  -0.08811373 -0.63176244]\n",
      "   [ 4.277896   -0.13539058  3.4492354  -2.2103589  -0.02510412\n",
      "    -0.8326119  -2.8408864  -7.60088     0.3245765  -1.5360774 ]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489  -0.22415894  0.44419977  0.86740375\n",
      "    -1.3531986  -0.1527121  -1.0399705   1.146371   -0.35023493]\n",
      "   [ 0.22428446 -2.402899   -1.7699208  -0.35874206 -2.1430826\n",
      "    -0.03102632  0.5429309  -1.0086042  -1.0677565  -1.0865968 ]\n",
      "   [-0.6712342   0.30168235 -3.0742364   0.19597065 -3.3459918\n",
      "    -0.10913967 -1.0004942   0.28558722 -1.2108853  -3.0090327 ]\n",
      "   [ 1.2226754   2.9617927   2.3518116  -2.0428975   1.0853696\n",
      "    -0.76767504 -0.3099975  -1.1994996   3.250949   -0.02864727]\n",
      "   [ 3.3925583   4.3809915   0.44789767 -3.760937   -1.4172314\n",
      "    -2.3714716   0.38091192  1.2854881   1.0312135  -1.4300249 ]\n",
      "   [-2.3619554  -2.1506267  -0.96027523  3.8960817   1.8534632\n",
      "    -0.74298    -2.0000184   2.1462076  -1.9178891   2.6341922 ]\n",
      "   [ 0.09026945  0.7947642   0.09152631  3.2191868   1.7997355\n",
      "    -1.8118993  -1.6655155   0.09941932  0.47267178  1.5012183 ]\n",
      "   [-0.677145   -1.8837119  -1.8422472   5.2752542   0.19712047\n",
      "     0.6287616  -2.4682517  -0.7707627  -1.2582183   0.4480308 ]\n",
      "   [ 0.18685156 -1.0029786  -0.48849955  0.7354018  -0.60926133\n",
      "    -2.234462   -2.1479487   2.2899785  -2.94474    -1.5366384 ]\n",
      "   [ 0.5181513   1.1900467   2.245966    3.1080089   1.0276145\n",
      "     0.06970618 -2.1040907   1.4412831  -1.7990694  -0.21974309]]\n",
      "\n",
      "  [[-3.0174336   0.41961712  1.9252456   0.7638777   1.4724514\n",
      "     3.0431707   2.8573203   3.2402046   2.5931482   2.2301    ]\n",
      "   [-1.3549727  -0.09280814  2.5851169  -0.3922029  -0.2696607\n",
      "     0.5922087   0.36082545 -0.7003431   0.8450714   0.36341256]\n",
      "   [-1.7961591  -1.8925554  -0.41655225  2.8299882   0.4466289\n",
      "     3.4457626   0.6450706   0.9305902   1.5253757   0.8443167 ]\n",
      "   [-1.1154089  -1.1745822   1.2023396  -0.06076117 -1.5096283\n",
      "     0.7106908  -1.2446266  -3.0961888   2.4934134  -0.72988504]\n",
      "   [-0.09934417  0.8952499  -0.6025727  -0.7601258  -4.950531\n",
      "    -0.91794896 -1.0664147  -1.4203914   0.3035166  -0.86303115]\n",
      "   [-0.730198    0.4987517   0.10699637  0.04278546  0.54071033\n",
      "    -0.50309646 -3.3407922  -1.0143468   0.2559061   2.2120178 ]\n",
      "   [ 0.23826952  2.0639033  -2.888716   -1.358555   -0.28163734\n",
      "     2.404897    0.22699867  3.525614    3.2236834   6.1108885 ]\n",
      "   [ 1.2659289  -0.02524091 -2.9593556   0.11135699 -0.94128567\n",
      "    -2.829995   -3.2003918  -3.8595636  -0.99238694 -0.92329174]\n",
      "   [-0.5888609   0.8837037  -1.3720297  -0.50547075  1.1768699\n",
      "     1.318384    1.8200378   2.1513548   4.0589194   3.615378  ]\n",
      "   [ 0.4514389  -2.6888735  -1.3093098  -0.7302314  -2.76163\n",
      "     1.3936642  -1.402236    0.21021515  3.8791788   2.6653714 ]]\n",
      "\n",
      "  [[-3.0523906   1.8179893   4.253165    1.4998401   2.628079\n",
      "     2.1809475   1.6847974   2.1162713   1.0263407   2.2014513 ]\n",
      "   [-7.7598214  -1.2395704   4.424106    5.041027    4.0303817\n",
      "     7.204144   10.943072    3.6315343   0.10899801  3.0159957 ]\n",
      "   [-5.33444    -2.9674113   1.9738437   4.328881    4.872642\n",
      "     8.156278   11.38618     1.9597439  -2.7887135   0.01012839]\n",
      "   [-1.8466234  -0.9457812   2.1802762   4.8156037   0.12329892\n",
      "     0.8021099  -0.51522094 -0.3930014   0.88665265 -0.0200246 ]\n",
      "   [-3.3392012  -4.19972    -0.9612767   6.551047    3.2264879\n",
      "     5.7523003   4.0560446   1.2217602  -3.9085174  -0.95752114]\n",
      "   [-1.1705554  -4.4363074  -1.1318855   3.9469442   3.668861\n",
      "     7.9511037   7.7728662   3.1097941   3.9134045   1.9645027 ]\n",
      "   [-0.29538846 -1.5871849  -2.8558958   4.954087    3.0537887\n",
      "     5.3664327   5.922907    0.4448749   2.3723075   0.9585795 ]\n",
      "   [-1.4024625  -3.924694   -1.0780026   2.6823587   2.3963602\n",
      "     4.2347217   6.6275954   2.555661    1.4216021   1.4860326 ]\n",
      "   [ 0.11296152  1.4904397  -0.46121475  4.180045    1.8322803\n",
      "     3.166133    1.6744213   2.8632095   5.733159    1.3540206 ]\n",
      "   [-1.2402344  -1.0717087  -0.8928821   5.5589666   3.5899298\n",
      "     4.291726    6.806953    0.87921894  1.1001962   0.6661416 ]]\n",
      "\n",
      "  [[-0.6331823  -1.2349904   0.17108005 -1.7825468   0.4058478\n",
      "    -1.3177248   1.3472757  -1.038774   -0.5425702   0.61429256]\n",
      "   [ 2.119253   -4.246382    1.0599287  -6.814192   -1.6555526\n",
      "    -5.312355   -1.9065204  -3.2599387   0.61110854  1.5822006 ]\n",
      "   [-1.1188248   0.14757451  1.0110303   1.6372967   0.67021793\n",
      "     2.4267418  -1.4579339  -3.0462801  -1.2310774   2.1712596 ]\n",
      "   [ 0.95955515 -1.9993405   2.5704405  -1.7231832  -0.62369555\n",
      "    -1.1314759  -2.2201748  -2.0597012   2.0095713   2.728148  ]\n",
      "   [ 0.49821588 -1.729508    2.970458   -3.6262503  -2.1428034\n",
      "     0.15667306 -1.009277   -2.750008    2.0951803   3.5743556 ]\n",
      "   [ 2.4125378   1.1109612   3.4186935  -5.2272053  -2.9276714\n",
      "    -4.479757   -1.5780019   1.15176     4.194589   -0.8862791 ]\n",
      "   [ 1.147488    0.25655437  4.761012   -4.4415584   0.4498366\n",
      "    -3.3419423  -5.0206585  -2.8960762   2.6126118   2.0494146 ]\n",
      "   [ 3.2666113  -0.7482998  -2.4978724  -4.6047583  -0.4991162\n",
      "    -1.8868389  -4.1274176  -7.000914   -0.26274645 -0.9070944 ]\n",
      "   [ 2.625219    0.4134868  -0.2386017  -2.9531605   0.06102017\n",
      "    -2.7949638  -3.0890677  -2.0160155  -0.665806   -1.5237277 ]\n",
      "   [ 2.5969262   0.8712801   0.59294933 -2.1528423   1.917804\n",
      "    -3.1793027  -3.1771533  -2.1307032   1.078736   -1.3194213 ]]\n",
      "\n",
      "  [[-1.2599558   1.0058122   0.9403136  -2.169106    1.3649019\n",
      "     0.05476408 -0.2606994   0.6737088   0.974384   -0.18888953]\n",
      "   [ 1.9198732  -0.84344107  2.0494642  -1.7659745   1.8153642\n",
      "    -1.3940618  -1.7129728   0.6147069  -0.501224    1.0274923 ]\n",
      "   [ 2.8428354  -1.6724693  -1.4574461  -0.75728846 -2.8203032\n",
      "    -4.540737   -4.3868213  -0.7501175  -0.68200713 -5.1211104 ]\n",
      "   [ 1.2147404   1.865701   -1.4707285  -1.3224499  -2.9885063\n",
      "    -0.7098579  -2.4524012   0.32578066 -0.1086136   1.557107  ]\n",
      "   [ 2.2408023   0.45182097  0.23735748 -3.630287   -4.9238787\n",
      "    -1.6159724  -4.2166977  -0.00287914  1.6390876  -1.0723724 ]\n",
      "   [ 2.0514302   1.124743   -0.66148055 -3.6998794  -3.634573\n",
      "    -3.2806454  -3.53534    -0.587686    0.4574493  -1.4516882 ]\n",
      "   [ 1.8713237   1.459809    4.544932    0.6970329   0.3698051\n",
      "    -1.5995883  -3.4667106  -2.5332448  -1.2041903  -0.62293327]\n",
      "   [ 0.7016293   0.6996512  -0.24522036 -1.6627809   1.9750067\n",
      "    -0.02285164 -0.6037045  -1.2260524   0.3857271  -0.24273007]\n",
      "   [-0.00567898 -1.5558052   0.70907253  0.3123517  -0.03035606\n",
      "    -0.63739645  2.4996793   0.65522164  0.10999125  3.8119931 ]\n",
      "   [ 1.6093675  -0.25890806  1.8090376   1.2600975   1.3659852\n",
      "    -0.02949921 -1.4901761  -2.0621624  -1.4694445  -1.7734735 ]]\n",
      "\n",
      "  [[-2.1587913   1.7147038   1.544837    2.9021912  -0.54687065\n",
      "     0.69791436  0.48692545  0.3874992   1.747807    0.6198854 ]\n",
      "   [-0.00582398 -2.3462925   1.799922    0.32839322  0.3320445\n",
      "    -2.016989   -2.8966556  -2.9691656   0.9186906  -3.0179012 ]\n",
      "   [-2.1043587  -2.8178024   2.2148285   3.891037    2.2759385\n",
      "     2.491633    0.34263963  3.354882    2.579082    2.9529788 ]\n",
      "   [ 1.8959482  -3.3403587   2.0201633  -2.8513105   2.2833853\n",
      "    -1.5907921  -5.267708   -0.7608538  -1.667431   -4.55585   ]\n",
      "   [ 1.2664658  -1.2141082   0.86169004 -0.82817227  1.8247795\n",
      "    -2.2689962  -2.9155438  -0.7371273  -1.1245675  -3.761395  ]\n",
      "   [ 1.4200376  -0.8142122  -0.15309595 -2.757525   -0.03373783\n",
      "    -3.5767376  -0.7845442   0.7012546   0.22206096 -1.2080317 ]\n",
      "   [ 0.9523497   2.5999942  -0.72424364 -1.2373599  -0.06541023\n",
      "    -2.280726   -0.30826616 -1.5803338  -0.43647492 -3.3566196 ]\n",
      "   [-0.66845006 -1.2726071   0.88486147 -0.17885678  0.5529859\n",
      "     0.5443401   1.7109423   0.3173334   0.40885562 -0.50677687]\n",
      "   [-1.6181766   2.127004   -3.3242197   2.8492422  -2.3754318\n",
      "    -1.4359603  -2.8194427   0.7514017  -1.035433   -4.766356  ]\n",
      "   [ 1.1412492   0.47997436  0.5518038   2.792524    1.0626367\n",
      "    -0.9522897  -3.9988208  -0.34751797 -2.0532994  -4.8456335 ]]\n",
      "\n",
      "  [[-2.0107505   2.0400176   1.5965837   0.41086924  2.151286\n",
      "     2.0080416   3.8806849   3.7532885   0.60808486  1.1465902 ]\n",
      "   [-1.2814682  -2.6879327   1.4795719   0.70971787  1.2221687\n",
      "     4.831139    4.012818    3.0739448  -0.3462172  -0.15153797]\n",
      "   [ 0.3697977   0.11362538 -0.8682849   1.8676807   1.5285181\n",
      "    -2.5126774  -0.09541667 -0.62111366 -0.5378774  -0.9066283 ]\n",
      "   [ 0.17680757 -0.59016186 -0.07212502  1.788976    1.8290929\n",
      "     1.2414881   2.5110507   0.9378911   1.4032513   2.5400653 ]\n",
      "   [ 0.27183303 -2.8065841  -0.40462255 -2.3706849  -0.47989556\n",
      "    -0.17444041  1.5252     -0.41247755  0.18653843 -1.4950514 ]\n",
      "   [ 0.8829943  -3.017673   -2.083954    2.1678572   2.4494154\n",
      "    -0.16720197  2.1018574  -0.03369299 -0.08712415 -0.03370773]\n",
      "   [ 0.36083797 -0.05401262  0.63395387  1.9816741   1.4668022\n",
      "     2.7947261   0.7702297   0.40087664 -1.3949666   0.64655036]\n",
      "   [ 0.28325427 -1.9323467   0.58271986  2.48945     0.9275244\n",
      "     2.8025713   1.751703    0.5133646  -2.6412737   0.6172831 ]\n",
      "   [-1.0369177  -0.8422439  -0.00286578 -1.2839406  -0.02112548\n",
      "     3.2767828   3.8468044   4.8574357  -0.255869    4.6949525 ]\n",
      "   [ 0.6051889  -2.837119    0.79984003 -3.0200691  -0.1500156\n",
      "     2.4176633   1.0713742   0.5682465  -0.72345775 -1.7475139 ]]]], shape=(1, 8, 10, 10), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 10, 10)\n",
      "output.shape = (1, 8, 10, 32)\n",
      "scaled_attention.shape= (1, 10, 8, 32)\n",
      "concat_attention.shape= (1, 10, 256)\n",
      "outputs.shape= (1, 10, 256)\n",
      "(1, 10, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 10, 256)\n",
      "(1, 10, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 10, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 10, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.5176607    1.9687198    2.0989769   -0.47190297]\n",
      "   [ -0.49124843   0.08534324   0.2991765   -0.48528743]\n",
      "   [  0.24215932  -4.697081    -4.019181     0.1927161 ]\n",
      "   [  0.7396501   -3.0292149   -2.48159      0.7256521 ]\n",
      "   [  0.7377838   -1.2575825   -1.0456651    0.7396505 ]\n",
      "   [  0.12743154  -2.1013763   -1.7714081    0.15186152]\n",
      "   [  0.57692707  -1.6948001   -1.6039941    0.56792045]\n",
      "   [ -0.0456275   -3.2555232   -2.8839726   -0.07279108]\n",
      "   [  0.541922    -4.2346587   -4.088871     0.5205673 ]\n",
      "   [  0.43123668  -4.4550576   -4.342825     0.38334304]]\n",
      "\n",
      "  [[ -0.5409756    2.8193953    2.0520196   -0.5236139 ]\n",
      "   [ -1.0098997    4.226584     3.5640724   -0.9722929 ]\n",
      "   [ -0.31559435  -2.7033126   -2.1731372   -0.34413084]\n",
      "   [  0.67385     -4.3054934   -2.5939085    0.6549963 ]\n",
      "   [  0.7716512   -4.6774607   -3.4361272    0.73228556]\n",
      "   [  0.30699015  -2.0128524   -1.77724      0.3016845 ]\n",
      "   [  0.95172566  -5.123536    -3.7028008    0.9193312 ]\n",
      "   [  0.1625587   -2.8245268   -2.2594504    0.1333035 ]\n",
      "   [  0.80569774  -2.9156864   -1.9677489    0.81221676]\n",
      "   [  0.5848501   -4.6709986   -3.4369607    0.5637138 ]]\n",
      "\n",
      "  [[ -0.42360795   3.4683292    3.2733347   -0.39263082]\n",
      "   [ -0.47245097   0.7355631    1.3526613   -0.47366753]\n",
      "   [  0.8764782   -5.054033    -4.6074905    0.832105  ]\n",
      "   [  1.5158468   -5.5036316   -5.460711     1.4665604 ]\n",
      "   [  1.3198522   -4.7758727   -4.838473     1.2815847 ]\n",
      "   [  0.6756115   -5.767558    -5.86507      0.60872716]\n",
      "   [  0.6050416   -6.19143     -6.4390326    0.54084617]\n",
      "   [  0.44500238  -0.68225163  -0.9241052    0.42594016]\n",
      "   [  1.465985    -3.2105703   -2.976426     1.4414983 ]\n",
      "   [  1.1924549   -4.355453    -4.300772     1.1541699 ]]\n",
      "\n",
      "  [[ -1.6453142    4.41468      3.7185764   -1.6388824 ]\n",
      "   [  0.1504475   -1.3770971   -0.84712774   0.10965424]\n",
      "   [  1.5174129   -5.9414515   -4.9204936    1.4784044 ]\n",
      "   [  1.9338356   -4.261786    -3.374979     1.932654  ]\n",
      "   [  1.5312709   -4.5473847   -3.4884474    1.480693  ]\n",
      "   [  2.4686995   -5.566623    -4.674686     2.4672065 ]\n",
      "   [  2.0171244   -5.156834    -4.0178304    1.9925612 ]\n",
      "   [  1.4369041   -3.667318    -2.7229993    1.4106048 ]\n",
      "   [  1.7216051   -4.4552937   -3.1465628    1.6921057 ]\n",
      "   [  1.9970524   -6.690838    -5.398477     1.966113  ]]\n",
      "\n",
      "  [[ -1.29121      2.8940606    1.9502285   -1.3604355 ]\n",
      "   [  0.00294916  -1.2568096   -0.89668256  -0.00931877]\n",
      "   [  0.7530518   -2.5598295   -1.9020394    0.7952453 ]\n",
      "   [  1.7065613   -5.794166    -4.5830455    1.7274754 ]\n",
      "   [  1.828848    -7.046152    -5.713792     1.8388789 ]\n",
      "   [  2.3361118   -9.356349    -7.713717     2.356966  ]\n",
      "   [  2.4712012   -7.777283    -6.1303487    2.5265574 ]\n",
      "   [  2.178688    -7.1766615   -5.927077     2.238128  ]\n",
      "   [  2.4477205   -5.1921487   -3.4574718    2.464835  ]\n",
      "   [  2.341771    -5.8448753   -4.0209694    2.4139361 ]]\n",
      "\n",
      "  [[ -0.65288585   3.4191878    2.6229937   -0.6571499 ]\n",
      "   [  0.72896236  -2.1471467   -1.9732635    0.77281165]\n",
      "   [  0.32350188  -1.2035745   -1.7607827    0.34396917]\n",
      "   [  1.0276341    1.058868     1.2416905    1.0519707 ]\n",
      "   [  1.6617185   -1.5399216   -1.5241555    1.7414258 ]\n",
      "   [  2.3202841   -3.4387836   -2.1360145    2.3670495 ]\n",
      "   [  2.2568145   -5.2002926   -3.3399537    2.3179612 ]\n",
      "   [  2.10796     -4.739115    -2.9632866    2.1519184 ]\n",
      "   [  1.2009804   -1.6315452   -1.1646751    1.2423136 ]\n",
      "   [  1.612212    -4.7317686   -3.3088357    1.6307019 ]]\n",
      "\n",
      "  [[ -0.13920632   1.920276     1.8218281   -0.1018336 ]\n",
      "   [  0.19900118  -1.852921    -1.364047     0.17529728]\n",
      "   [  0.9201308   -2.3837185   -1.2430844    0.8900646 ]\n",
      "   [  1.0384122   -2.2614129   -0.6760908    1.014745  ]\n",
      "   [  1.3033148   -4.046018    -2.5787446    1.2661915 ]\n",
      "   [  1.1603479   -2.2507179   -1.0619235    1.1338618 ]\n",
      "   [  1.041217    -4.2461724   -2.9057727    1.0158387 ]\n",
      "   [  1.0148015   -1.7258983   -0.37147498   1.0063083 ]\n",
      "   [  0.88598007  -2.206053    -0.84811103   0.8885484 ]\n",
      "   [  1.2307554   -3.4753895   -2.1146924    1.1983802 ]]\n",
      "\n",
      "  [[ -1.5968649    5.382058     4.5500836   -1.5562288 ]\n",
      "   [  0.86753845  -4.5697775   -4.1758018    0.8370436 ]\n",
      "   [  1.9240823   -9.450348    -8.618066     1.8469068 ]\n",
      "   [  1.4563193   -6.307143    -5.9952025    1.4192016 ]\n",
      "   [  1.9546863   -5.530293    -5.110774     1.9280047 ]\n",
      "   [  1.7568917   -6.1661077   -5.8179903    1.6855166 ]\n",
      "   [  3.4059234  -12.176341   -11.224475     3.3215392 ]\n",
      "   [  1.7747686   -8.078717    -7.356744     1.7220855 ]\n",
      "   [  2.41111     -7.0833406   -6.4738836    2.3771992 ]\n",
      "   [  2.242727    -8.106706    -7.0536833    2.1916888 ]]]], shape=(1, 8, 10, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 10, 4)\n",
      "output.shape = (1, 8, 10, 32)\n",
      "scaled_attention.shape= (1, 10, 8, 32)\n",
      "concat_attention.shape= (1, 10, 256)\n",
      "outputs.shape= (1, 10, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 11, 256)\n",
      "(1, 11, 256)\n",
      "(1, 11, 256)\n",
      "split_heads()\n",
      "(1, 11, 256)\n",
      "(1, 11, 8, 32)\n",
      "split_heads()\n",
      "(1, 11, 256)\n",
      "(1, 11, 8, 32)\n",
      "split_heads()\n",
      "(1, 11, 256)\n",
      "(1, 11, 8, 32)\n",
      "(1, 8, 11, 32)\n",
      "(1, 8, 11, 32)\n",
      "(1, 8, 11, 32)\n",
      "matmul_qk.shape = (1, 8, 11, 11)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    0.34484416   0.3515555    0.8269838   -0.1325054\n",
      "      1.1541476    0.36659813   0.6267392    0.7029531    1.932782\n",
      "      0.7453384 ]\n",
      "   [ -1.5524524    0.57553977   1.342203     0.7264702    0.67869383\n",
      "     -2.3639412    0.21526393  -1.575456     4.0412474    3.7598073\n",
      "      1.7110962 ]\n",
      "   [  0.17656448  -2.1603198   -3.1747456   -2.3878162   -0.21427149\n",
      "     -2.5600047   -1.3389277    0.00445623  -1.037421    -3.3319168\n",
      "     -3.3131418 ]\n",
      "   [ -0.27855322  -2.3594024    0.28350914  -6.3847156    2.1768348\n",
      "      0.48724338   4.0435467    0.8452997    2.298873     4.9464703\n",
      "      3.1141565 ]\n",
      "   [ -0.5387738   -1.1847006   -2.5186715    3.639655    -2.1242177\n",
      "      8.928502     3.1669946    5.1829453    4.105474     9.427159\n",
      "      2.1721263 ]\n",
      "   [ -0.30547413  -1.4707135    0.06941595   0.72953105   1.0991143\n",
      "      2.2755632    3.7240849    3.374271     3.1669166    2.325922\n",
      "     -2.3291914 ]\n",
      "   [ -0.0702149   -2.969282    -3.383008     1.3616961    0.42645463\n",
      "     16.02929      8.762581     8.823401     3.3795218    4.6411085\n",
      "     -1.3476764 ]\n",
      "   [ -0.20550568  -0.10201316  -2.2742054   -2.1785567   -1.7485399\n",
      "      2.4331734    1.2430574    4.5232897    0.90502656  -2.2167747\n",
      "      4.4218707 ]\n",
      "   [ -0.590271     1.0130959   -0.09686741   0.5674616    0.8100412\n",
      "      0.53022087   0.5689679    5.691497     0.52202976   1.8942038\n",
      "      4.9346013 ]\n",
      "   [  0.25923175  -3.3190057   -3.0277622   -5.676654     1.3393734\n",
      "     -3.3025331    2.851563     4.2579527   -0.4028998   -8.078694\n",
      "      1.0116674 ]\n",
      "   [ -0.73776734   1.8399746    0.59010684  -4.0468388   -1.4626211\n",
      "     -3.4676483    1.6430769   -0.11242276  -0.20180999   2.43283\n",
      "      2.8597143 ]]\n",
      "\n",
      "  [[ -0.1123142    0.09086403  -0.2228786   -0.18202324   0.14455257\n",
      "     -0.04013486  -0.48076916  -0.06074027  -0.04329262   0.18260159\n",
      "      0.24836148]\n",
      "   [ -0.27778682  -7.685327    -0.20150177   3.3823       3.8902612\n",
      "      4.337893     1.4037256   -1.6104152    0.8058254   -0.02028271\n",
      "     -1.8037955 ]\n",
      "   [  0.30459622  -1.6710656   -3.094567    -1.6013079    0.79777056\n",
      "      0.13481723  -1.8365917   -0.71397376  -0.33064502   2.8611202\n",
      "      1.7720222 ]\n",
      "   [ -0.03799488  -4.1970086    2.696054    -2.7333486    1.6560552\n",
      "      4.530645     4.271422    -1.2604878   -2.5451972    1.8992972\n",
      "      0.24071065]\n",
      "   [  0.6443627   -3.9118009    1.1981791   -4.1091504   -6.4226418\n",
      "     -0.9858088   -1.1690059   -0.22695212   1.5001154   -1.1231505\n",
      "     -0.6763548 ]\n",
      "   [  0.82783276  -3.7707963   -2.1052582   -2.503423    -4.9312515\n",
      "     -4.1483335   -5.133481    -1.3080887   -1.481337    -5.980171\n",
      "     -3.765387  ]\n",
      "   [  0.796586    -0.30842566  -2.7917175   -4.5511785   -0.32458684\n",
      "     -0.10727115  -6.259451     1.659165     1.4761349   -1.4687202\n",
      "     -0.087237  ]\n",
      "   [  0.7033243   -0.6943296    0.38815764   1.9749643   -4.1990952\n",
      "     -3.2755108   -5.034937    -3.387448     3.127423    -5.54149\n",
      "     -2.6343205 ]\n",
      "   [ -0.6092106   -2.0272765   -0.859915     2.4829376    4.411381\n",
      "      5.087132     1.9951113    2.134214    -1.0122576    3.2323248\n",
      "      2.1893842 ]\n",
      "   [  0.0895261   -3.0075777    2.4791887   -0.3216253    6.745585\n",
      "      5.4583244    4.826457    -0.94258535   4.3627224    2.697197\n",
      "      2.0474095 ]\n",
      "   [  0.2823794   -0.01970736   1.9152175   -2.1263983   -2.304781\n",
      "      0.0051903    1.3419027    2.1577578    7.0537715   -1.3675343\n",
      "     -1.3401513 ]]\n",
      "\n",
      "  [[ -0.10918517  -0.21059825   0.3513798    0.34551156   0.83573407\n",
      "      0.38043547   0.42571396  -0.10994706   0.73884016   0.11337968\n",
      "      0.6242574 ]\n",
      "   [  0.2860052   -5.063574    -2.1691      -3.5214968   -3.319986\n",
      "      1.6807239    2.6336646   -3.6028974    1.4836615    0.80234605\n",
      "     -0.38849393]\n",
      "   [  0.6062982    2.719351    -4.9581633   -1.0914348   -1.6627623\n",
      "     -0.31062695  -2.5335972    0.98369557  -2.88995     -1.7919333\n",
      "     -0.9979213 ]\n",
      "   [  0.13651355  -1.777794     3.1865156   -0.07830478  -1.239388\n",
      "      1.7914739    2.9210606   -1.4029268    1.2590778   -1.0561415\n",
      "     -1.8667321 ]\n",
      "   [ -0.21224323  -1.5428834    7.991749    -0.5730239    1.2063524\n",
      "      3.6277044    1.6937939    1.8189023    3.786629     0.13888861\n",
      "     -3.6174667 ]\n",
      "   [ -0.85228556  -2.2769668    6.909945     3.543675     3.6550546\n",
      "      9.817229     3.752735     0.8422704    5.880493    -2.0402415\n",
      "      0.19032261]\n",
      "   [  0.18337622  -3.708946     1.3114123   -0.97666943   0.6310309\n",
      "      0.9075615   -3.0031114   -0.5886862   -0.67060435  -1.9287843\n",
      "     -5.895517  ]\n",
      "   [ -0.1485042   -3.7088253   -2.2962284   -1.5428704    0.04910947\n",
      "      4.634492     2.4989789   -4.185793     3.9252877   -1.8230985\n",
      "      1.8082708 ]\n",
      "   [  0.3454023   -4.214501    -4.5776362   -4.2041674   -5.6266522\n",
      "     -8.447932    -3.633056     1.5147972   -4.597967     3.3158236\n",
      "     -3.3973794 ]\n",
      "   [  0.00192533   4.0501933   -0.5385378   -4.8801765   -4.5654726\n",
      "     -5.769101    -2.7084713   -1.0551419    0.06899593  -6.671878\n",
      "     -6.3473577 ]\n",
      "   [  0.04607371  -2.411195    -0.708032    -4.205224    -0.87881404\n",
      "     -3.4096053    1.305151    -0.7891736   -0.16625388   1.6788616\n",
      "     -2.59234   ]]\n",
      "\n",
      "  [[ -0.22114104  -0.3915212    0.13786234   0.33271423   0.6211036\n",
      "      1.0825356   -0.11799879   0.72872204  -0.1314376   -0.34905735\n",
      "      0.17056341]\n",
      "   [  0.14206643   1.1752353    3.6785405    1.6119353   -0.9453702\n",
      "     -3.2077594   -2.725474     1.8995842   -0.996005     3.998312\n",
      "      1.650684  ]\n",
      "   [ -0.07449952  -3.5981474   -0.31350046   1.4361463    2.564271\n",
      "      2.474412     0.16766883  -0.5836961    1.1178291   -3.7098937\n",
      "      0.93542874]\n",
      "   [ -0.1049557    0.70436597   0.5221487   -0.21612936   0.9116111\n",
      "     -0.48090836   0.18603255  -1.9520314    3.4277797   -0.04625581\n",
      "      1.8237629 ]\n",
      "   [ -0.310017    -2.4498558    2.5756524    1.8039283   -0.97895104\n",
      "      3.0719118    0.6960478    3.8471358    4.3071675    1.5473081\n",
      "      2.4796813 ]\n",
      "   [ -0.39245275  -5.124641     3.6571612    1.2725205    3.1442227\n",
      "      0.02843648  -1.5512888    2.58859     -0.89740974   3.4971538\n",
      "      4.0158606 ]\n",
      "   [ -0.5742471   -4.852531     1.5101644    2.5168405    3.3431916\n",
      "      9.560527     0.73367864   4.908349     4.149948     1.482831\n",
      "      7.8682423 ]\n",
      "   [ -0.97490954  -4.6069136   -1.7809725    1.4556999    4.3725533\n",
      "      9.8998165    0.52443993   0.8263054    0.9018291   -3.7931354\n",
      "      1.2475286 ]\n",
      "   [ -0.5735989   -4.0548587    0.27316695  -2.4717047    3.2703655\n",
      "      3.5540314    0.07714308   8.548014     3.0001826    1.4996339\n",
      "      4.816025  ]\n",
      "   [  0.19105694  -1.2941034    1.8261616   -1.3133595    0.79539776\n",
      "      0.04804294  -0.99305815   3.3403172   -0.65854585  -1.1529343\n",
      "      2.4811525 ]\n",
      "   [ -0.05802697  -1.5689225   -0.16047485  -0.91536826   2.655601\n",
      "      0.939235    -1.4693128    1.3426737    2.0493643    3.9560964\n",
      "      3.4739137 ]]\n",
      "\n",
      "  [[  0.00076126  -0.1641996   -0.14196222  -0.19410133   0.00823206\n",
      "      0.61542094  -0.08737361  -0.10085457  -0.06919557  -0.05508299\n",
      "     -0.00355904]\n",
      "   [  0.78445697  -5.048575     3.2536209   -0.97517645   0.36774525\n",
      "     -3.194747    -0.04737727  -0.09847949   0.45832372  -1.1919119\n",
      "      2.271528  ]\n",
      "   [  0.2599421   -2.3611805    4.198083    -0.59922963  -2.4801793\n",
      "     -5.2311      -3.3485384   -2.4956753   -3.0119252   -0.40281573\n",
      "      2.1800683 ]\n",
      "   [  0.203725     3.9226558    1.8656771    0.77520037  -3.1088407\n",
      "     -2.945639     1.2251574    1.5569918   -1.4664475   -0.32573247\n",
      "      3.2645931 ]\n",
      "   [  0.6548166    0.10552962   1.3786814   -2.6986282   -3.3625326\n",
      "     -3.5189183   -7.556854    -3.5280879   -4.3273387    0.14598486\n",
      "      4.111145  ]\n",
      "   [  0.85031134  -2.967739     2.643215    -0.4758638    1.1578159\n",
      "     -8.628843    -5.5761604   -4.848856    -2.0556912   -4.075636\n",
      "      4.610228  ]\n",
      "   [  0.01436896   0.14466181  -2.1695178    2.4206777    8.998847\n",
      "     -1.5658671   -0.12156205  -0.9806287    3.9562185   -0.5530879\n",
      "      4.220974  ]\n",
      "   [  0.56318194  -4.2370687    1.5977541    1.2324364    2.794963\n",
      "     -3.8853424   -3.6924665   -4.2410336   -0.9847575   -1.2085614\n",
      "      2.6339824 ]\n",
      "   [  0.30105153  -1.6933879   -2.020751    -1.2294377   -0.5758017\n",
      "     -4.3765674    0.27936047  -4.3569603   -5.729614    -6.6753893\n",
      "      1.1173354 ]\n",
      "   [  0.76540506   0.15341733   2.4403636   -2.3432696   -2.6396062\n",
      "     -1.03931      2.470319     4.2786593   -1.5197265   -1.8448883\n",
      "      2.0892816 ]\n",
      "   [  0.86878276  -7.830677     2.7710674   -2.9125304   -3.1482031\n",
      "      0.89338726  -4.6682324   -3.014839    -3.440457    -2.4411628\n",
      "     -2.3352563 ]]\n",
      "\n",
      "  [[ -0.00999826  -0.5171405    0.3842402   -0.26599303  -0.17592163\n",
      "      0.08763131   0.29407236   0.11240291   0.06468003   0.20075996\n",
      "     -0.16126524]\n",
      "   [  0.3875343    2.307618    -2.0652568    0.06119107  -1.376965\n",
      "     -0.84213823   1.6010706   -1.9992107    3.9152644    2.9078257\n",
      "      4.2148504 ]\n",
      "   [  0.5494998    0.9012554   -5.717646    -0.5140254   -1.9746116\n",
      "     -4.1329675    0.38239804  -3.9651098    0.4975535    1.9827068\n",
      "      1.2549968 ]\n",
      "   [ -0.09110849   1.9358311   -2.1756437   -0.7149111   -1.2668245\n",
      "      0.23427002  -0.5892781    1.0475453    4.2543564    0.953046\n",
      "      5.090866  ]\n",
      "   [ -0.99071145   6.797044    -1.4787755    4.8537555   -0.8091358\n",
      "      1.0001513   -0.56871563  -1.046842     7.9517646    4.4362655\n",
      "      5.483596  ]\n",
      "   [  0.1653099   -0.6100134    4.8626056    0.720647     0.9629525\n",
      "     -1.3203751   -4.107916     2.463625    -1.0382884   -0.11817995\n",
      "      0.65832764]\n",
      "   [ -0.4776169   -4.275887     1.1203661    0.41609427   4.5017853\n",
      "     -0.39336598  -2.3056731   -1.7078637   -0.52901644   1.9856396\n",
      "      0.82054085]\n",
      "   [  0.88401735  -3.8715084   -1.8880895   -3.6033385   -1.9594375\n",
      "      1.1121889   -1.3816634   -1.135376    -3.8049717   -7.2075057\n",
      "     -2.3420775 ]\n",
      "   [  1.1133462   -6.4942994   -0.6978988   -4.9526157   -5.715827\n",
      "     -5.9033365   -5.149199    -1.8298197   -7.6704     -10.312497\n",
      "     -5.0730634 ]\n",
      "   [  0.48386398  -3.646351    -3.5835953   -2.0325422   -5.247802\n",
      "     -1.5264621   -1.6238784   -2.1048608   -0.827515    -7.4019194\n",
      "     -1.0366647 ]\n",
      "   [  1.141735    -1.7643471   -3.4391265   -2.2752903  -11.699383\n",
      "     -7.5556536   -2.8469846   -3.6191273   -3.8030713   -6.320883\n",
      "     -7.662214  ]]\n",
      "\n",
      "  [[ -0.20863976   0.0058533    0.141982     0.3954799    0.70690477\n",
      "      0.8365328    0.28462148  -0.24421918   0.6515544    0.92187274\n",
      "      0.7320269 ]\n",
      "   [ -0.5011563   -5.83805      3.0883057    2.4906335    2.8664675\n",
      "      3.39397      0.8982007   -2.3877478    5.1261563    3.4072495\n",
      "      1.0007838 ]\n",
      "   [ -0.08541125   2.1106565    0.02025323   0.89557755   1.3944312\n",
      "      4.8944817    3.2451954    4.2954817    0.5471952   -0.92864716\n",
      "      1.0689429 ]\n",
      "   [  1.3930423    2.344757    -1.3677286   -6.293937    -6.4745502\n",
      "     -5.8228016   -2.4453523   -2.055828    -3.1815403   -8.232062\n",
      "     -5.3696084 ]\n",
      "   [  2.1056752   -0.87906635  -3.2462146   -5.5659313   -6.810574\n",
      "    -12.417996    -5.3918004   -2.5989642  -11.401493    -7.451914\n",
      "     -7.142121  ]\n",
      "   [  0.7307259   -0.62191457   2.2546957   -4.717381    -0.30207893\n",
      "     -5.2433743   -4.031666     0.92758286  -2.1127427   -1.417971\n",
      "      0.00246029]\n",
      "   [  2.2975056   -3.7142315   -2.9478807  -10.469339    -5.418616\n",
      "    -10.1225815   -7.3040056    0.4335872   -9.478246    -4.2933764\n",
      "     -6.1655016 ]\n",
      "   [ -0.6196739   -1.8421621    1.7395886    2.5982852    5.356473\n",
      "      0.73179406   1.0359558   -3.0025475    4.908996     3.7934012\n",
      "      4.0164475 ]\n",
      "   [  1.2433432    0.35926548  -1.4999105   -1.8502873   -3.6189969\n",
      "     -9.892432    -1.9395131   -0.6860688   -8.132981    -3.592384\n",
      "     -3.0187387 ]\n",
      "   [  1.4510003    1.0021973   -2.8439877   -6.589478    -4.991755\n",
      "    -11.815946    -5.369118    -0.26158586  -7.02527    -10.053152\n",
      "     -4.336011  ]\n",
      "   [  1.678265    -0.7698982   -3.5583017   -4.1901755   -4.662891\n",
      "    -10.897082    -4.1174273   -2.1877768   -7.076847    -3.413887\n",
      "     -4.9778514 ]]\n",
      "\n",
      "  [[ -0.15726982   0.34343007  -0.0106815    0.2812659    0.26918355\n",
      "      0.6895108    0.5193374    0.17895888  -0.26491943   0.13644844\n",
      "     -0.01431696]\n",
      "   [  0.42748785  -3.541775    -4.416529    -1.5682534   -0.30482742\n",
      "      0.77651656   1.8078052    1.8106172    3.8573866    1.6207062\n",
      "      1.7149286 ]\n",
      "   [  0.09384713   3.2008502   -4.3988295   -1.0193433   -1.7561591\n",
      "     -0.20635648   1.7356048   -0.08922087   0.29839948  -2.692548\n",
      "     -1.168847  ]\n",
      "   [  0.00172196  -0.81685966   2.484785    -2.7307875    4.4621997\n",
      "      2.2167425    0.05664166  -2.2540293   -2.799476     2.9115417\n",
      "      7.345386  ]\n",
      "   [ -0.26568195   6.4090133    0.19427305   4.194164    -5.8646803\n",
      "      0.7492719    3.420403     4.213038     3.715512     2.1433942\n",
      "      2.389735  ]\n",
      "   [  0.15044671  -0.83800465   5.2889094    3.357029     2.2839093\n",
      "     -3.226359     1.4485415   -1.6294129    1.074201    -2.3407726\n",
      "      4.503136  ]\n",
      "   [  0.5396642   -4.57709     -2.997078    -1.2818362   -2.02568\n",
      "     -0.3638186    4.7706485    3.2261612    0.503986    -0.5691261\n",
      "      1.1641093 ]\n",
      "   [ -0.00558216  -3.0874553   -2.459048     0.15172434   2.5665867\n",
      "      1.0365962    5.3165255   -1.6244347    1.7662504   -0.5312396\n",
      "      3.5896485 ]\n",
      "   [ -0.08250386   0.13179336  -0.65814525  -2.291554    -0.36036602\n",
      "      1.4784904    0.8082328   -0.5854646   -3.5685499    2.199129\n",
      "      5.263602  ]\n",
      "   [  0.4018605   -0.47903845  -3.2845047   -4.582873    -2.3509977\n",
      "     -1.8646562    4.6285706   -1.8439622    2.0653298   -3.029793\n",
      "      2.6395364 ]\n",
      "   [ -0.04158778  -2.3729644   -3.8405085   -0.9600312   -1.6876342\n",
      "      0.26056907   4.184902    -1.1041652   -0.58278877  -3.097527\n",
      "     -5.2292576 ]]]], shape=(1, 8, 11, 11), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 11, 11)\n",
      "output.shape = (1, 8, 11, 32)\n",
      "scaled_attention.shape= (1, 11, 8, 32)\n",
      "concat_attention.shape= (1, 11, 256)\n",
      "outputs.shape= (1, 11, 256)\n",
      "(1, 11, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 11, 256)\n",
      "(1, 11, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 11, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 11, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.6510173   -2.8665452   -1.9706712    0.6258339 ]\n",
      "   [ -1.2761326   -0.6591419   -0.772127    -1.2929033 ]\n",
      "   [  1.0074586   -3.2963974   -3.106445     0.9872349 ]\n",
      "   [  1.2579683   -4.881622    -3.767476     1.2469316 ]\n",
      "   [  1.3234984   -5.9344664   -5.2370906    1.2870476 ]\n",
      "   [  1.7000594   -3.6593008   -2.971424     1.7176253 ]\n",
      "   [  1.2776281   -1.9751713   -1.519901     1.2974825 ]\n",
      "   [  0.58336854  -1.1657993   -0.539427     0.5778812 ]\n",
      "   [  0.7600763   -0.7146235    0.06742804   0.76477766]\n",
      "   [  0.68825406   0.40578374   1.1206802    0.7197399 ]\n",
      "   [  1.5945777   -1.6418726   -0.5148084    1.594585  ]]\n",
      "\n",
      "  [[  1.5699102   -4.8338065   -2.8935153    1.5974954 ]\n",
      "   [  1.602976    -6.0585413   -4.3626466    1.6033887 ]\n",
      "   [  0.84273714  -3.3321726   -3.019569     0.8300638 ]\n",
      "   [  1.739568    -5.32123     -4.1527953    1.7057066 ]\n",
      "   [  1.9442569   -4.8756948   -3.3264403    1.9580569 ]\n",
      "   [  3.9294844  -11.821626    -9.099886     3.8940978 ]\n",
      "   [  2.184128    -5.056767    -3.4162076    2.1828146 ]\n",
      "   [  1.9612429   -4.841177    -3.8993757    1.9119158 ]\n",
      "   [  2.1313474   -8.619019    -7.0290985    2.1060083 ]\n",
      "   [  1.7264353   -4.697804    -4.539785     1.657041  ]\n",
      "   [  2.3086565   -7.632138    -5.643827     2.2703083 ]]\n",
      "\n",
      "  [[ -0.9479617   -1.5584124   -2.3440182   -0.91537315]\n",
      "   [  1.1684959   -6.083402    -5.361126     1.1414931 ]\n",
      "   [  0.8458585   -8.91149     -7.7787905    0.78298134]\n",
      "   [  2.5803494   -5.880795    -5.0921974    2.5411623 ]\n",
      "   [  3.3461082  -11.678779   -10.798813     3.272511  ]\n",
      "   [  3.3453295   -7.0702214   -5.903679     3.2953756 ]\n",
      "   [  1.6342559   -4.6711273   -4.3739734    1.6045712 ]\n",
      "   [  1.7710527   -4.211759    -3.7379825    1.7677531 ]\n",
      "   [  2.3430088   -5.5874043   -4.7693295    2.3498065 ]\n",
      "   [  1.7600528   -4.8194313   -3.658041     1.7406274 ]\n",
      "   [  1.8383347   -4.2881994   -3.964903     1.8433948 ]]\n",
      "\n",
      "  [[ -0.53032047  -2.1487055   -3.3869808   -0.5465881 ]\n",
      "   [ -0.37673792   0.5297353    0.7943683   -0.41485333]\n",
      "   [  1.2794939   -4.2864437   -4.2701783    1.2569122 ]\n",
      "   [  2.427947    -1.600872    -0.90655017   2.481265  ]\n",
      "   [  2.3424263   -7.2077713   -5.824986     2.335523  ]\n",
      "   [  1.7222499   -2.7512228   -2.3087296    1.7864217 ]\n",
      "   [  1.0004832   -7.1413507   -6.550554     0.95411247]\n",
      "   [  1.4302231   -3.8393989   -3.2885814    1.4499868 ]\n",
      "   [  1.734392    -5.0363717   -4.562415     1.7265733 ]\n",
      "   [  1.7437402   -4.61876     -3.9143934    1.71687   ]\n",
      "   [  1.1339672   -3.6319485   -2.8497448    1.1018116 ]]\n",
      "\n",
      "  [[ -1.8786137    3.8044016    1.7851309   -1.9308672 ]\n",
      "   [ -2.0427704    1.9088362    0.8611902   -2.10597   ]\n",
      "   [  0.3713629   -6.5012736   -5.5806484    0.3149101 ]\n",
      "   [  2.3904674   -0.8018907    0.54941446   2.4430196 ]\n",
      "   [  3.2833803   -5.793328    -3.5530388    3.3556385 ]\n",
      "   [  4.031575    -8.23709     -5.352422     4.1288157 ]\n",
      "   [  1.704372    -4.0030203   -2.8744233    1.746541  ]\n",
      "   [  0.55620587  -0.81578505  -0.7211592    0.5843559 ]\n",
      "   [  1.4359853   -5.1196675   -3.4192846    1.4993091 ]\n",
      "   [  3.066259    -8.724568    -6.3771715    3.138459  ]\n",
      "   [  3.077098    -5.4629254   -3.248803     3.1306965 ]]\n",
      "\n",
      "  [[  1.8587611   -2.3004422   -1.999603     1.8540798 ]\n",
      "   [  0.62134135  -1.1685245   -1.0336286    0.63556284]\n",
      "   [  0.93438053  -3.9921112   -3.1974504    0.91454566]\n",
      "   [  2.386284    -8.792911    -6.874719     2.3729997 ]\n",
      "   [  2.8132436  -11.001211    -8.411129     2.7784264 ]\n",
      "   [  2.7941167   -3.5710614   -2.4098222    2.8085887 ]\n",
      "   [  2.5607827   -5.1447167   -4.0192566    2.5128086 ]\n",
      "   [  1.9819213   -4.7589607   -4.3256965    1.9169271 ]\n",
      "   [  2.1862228   -3.8017366   -2.162978     2.196947  ]\n",
      "   [  2.025622    -0.50651217  -0.09534571   2.0341551 ]\n",
      "   [  1.7751181   -5.396942    -3.6041234    1.7526699 ]]\n",
      "\n",
      "  [[  0.263768     1.3027129    0.7912128    0.31364933]\n",
      "   [  0.90733486  -1.2863759   -0.23643312   0.92466456]\n",
      "   [  1.9193958   -5.6561565   -3.624307     1.9176353 ]\n",
      "   [  2.590705    -7.7614427   -5.438415     2.6344485 ]\n",
      "   [  2.0141904   -6.001995    -3.7005918    2.02994   ]\n",
      "   [  2.3413384   -2.5982022   -0.645182     2.369621  ]\n",
      "   [  1.1849254   -0.41514406   0.7534283    1.1835097 ]\n",
      "   [  1.6196365   -2.736624    -1.3294117    1.6487104 ]\n",
      "   [  1.8510432   -2.5615194   -0.7961973    1.8711935 ]\n",
      "   [  2.0705955   -4.3488426   -2.0247169    2.1132114 ]\n",
      "   [  1.9942156   -2.1047823   -0.90358776   2.0466616 ]]\n",
      "\n",
      "  [[ -0.50981736   4.1422954    3.6196024   -0.43590528]\n",
      "   [  0.4603232   -1.0371166   -0.40937394   0.4394898 ]\n",
      "   [  1.2603666   -3.7127948   -2.6582818    1.2983314 ]\n",
      "   [  1.9421246   -6.2256336   -3.6375484    1.9323288 ]\n",
      "   [  1.3231326   -4.248828    -3.1260052    1.3073777 ]\n",
      "   [  2.442132    -7.4952087   -5.2700887    2.4572377 ]\n",
      "   [  2.4187922   -5.27305     -2.1958756    2.4965396 ]\n",
      "   [  1.0935532   -3.028773    -1.9892881    1.1174632 ]\n",
      "   [  1.07857     -1.6685917   -0.5144004    1.1236442 ]\n",
      "   [  1.9439204   -4.8428583   -2.7191334    2.0016615 ]\n",
      "   [  1.3960849   -4.567839    -2.928855     1.4403856 ]]]], shape=(1, 8, 11, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 11, 4)\n",
      "output.shape = (1, 8, 11, 32)\n",
      "scaled_attention.shape= (1, 11, 8, 32)\n",
      "concat_attention.shape= (1, 11, 256)\n",
      "outputs.shape= (1, 11, 256)\n",
      "(1, 11, 256)\n",
      "(1, 11, 256)\n",
      "(1, 11, 256)\n",
      "split_heads()\n",
      "(1, 11, 256)\n",
      "(1, 11, 8, 32)\n",
      "split_heads()\n",
      "(1, 11, 256)\n",
      "(1, 11, 8, 32)\n",
      "split_heads()\n",
      "(1, 11, 256)\n",
      "(1, 11, 8, 32)\n",
      "(1, 8, 11, 32)\n",
      "(1, 8, 11, 32)\n",
      "(1, 8, 11, 32)\n",
      "matmul_qk.shape = (1, 8, 11, 11)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181 -0.6147039  -0.12000973 -0.26990378\n",
      "    -0.12601104  0.12369402  0.6703813  -0.01717847  1.4070063\n",
      "     0.65330005]\n",
      "   [ 3.96568    -0.17723085  1.5590783  -1.8989185  -1.7120768\n",
      "    -2.4874895  -2.461606   -3.3995156  -1.7362796   0.44090855\n",
      "    -2.0952516 ]\n",
      "   [ 3.9833748  -2.2539997   0.15140963 -2.319149    1.4055393\n",
      "     1.8976921  -0.35040918 -0.84194183  2.7204072  -1.7445444\n",
      "    -1.6135567 ]\n",
      "   [ 3.0466342  -1.2768042  -0.9543833  -2.4668362  -3.578708\n",
      "    -2.1999793  -3.10732    -1.4634132  -0.6942425  -2.7284784\n",
      "     0.24954496]\n",
      "   [ 4.7013025   1.1783674   1.1767476  -4.7330627  -2.6853023\n",
      "    -3.841537   -1.8943468  -3.301475   -0.50165665 -3.9194427\n",
      "    -0.22554372]\n",
      "   [ 5.391753   -1.2895486   1.732423   -3.715419   -2.8239722\n",
      "    -3.662196   -3.4674766  -3.3219385  -0.43135288 -2.239058\n",
      "     0.29614234]\n",
      "   [ 4.304015    0.211817    0.16455683 -0.47811216 -3.8062627\n",
      "    -3.590859   -5.4443517  -5.1668825   1.4912223   0.24744101\n",
      "    -1.7331754 ]\n",
      "   [ 4.4580784  -0.61394614  1.2823641   2.367739    2.35907\n",
      "     0.98633593 -3.9488287  -4.141637    1.9198743  -2.5438845\n",
      "    -1.469086  ]\n",
      "   [ 2.5623562  -1.0149673   0.28638697 -1.415009   -1.9392455\n",
      "    -0.7987015  -3.7834046  -2.8124275  -0.08811373 -0.63176244\n",
      "    -0.03427614]\n",
      "   [ 4.277896   -0.13539058  3.4492354  -2.2103589  -0.02510412\n",
      "    -0.8326119  -2.8408864  -7.60088     0.3245765  -1.5360774\n",
      "    -3.084445  ]\n",
      "   [ 1.5408126   0.0754276   0.5586133   3.7598023   3.607371\n",
      "     2.7014189  -0.33315745 -1.3049519   0.61513156 -0.15151803\n",
      "    -2.193409  ]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489  -0.22415894  0.44419977  0.86740375\n",
      "    -1.3531986  -0.1527121  -1.0399705   1.146371   -0.35023493\n",
      "    -0.12138899]\n",
      "   [ 0.22428446 -2.402899   -1.7699208  -0.35874206 -2.1430826\n",
      "    -0.03102632  0.5429309  -1.0086042  -1.0677565  -1.0865968\n",
      "    -1.0353341 ]\n",
      "   [-0.6712342   0.30168235 -3.0742364   0.19597065 -3.3459918\n",
      "    -0.10913967 -1.0004942   0.28558722 -1.2108853  -3.0090327\n",
      "    -3.8419588 ]\n",
      "   [ 1.2226754   2.9617927   2.3518116  -2.0428975   1.0853696\n",
      "    -0.76767504 -0.3099975  -1.1994996   3.250949   -0.02864727\n",
      "    -0.22127622]\n",
      "   [ 3.3925583   4.3809915   0.44789767 -3.760937   -1.4172314\n",
      "    -2.3714716   0.38091192  1.2854881   1.0312135  -1.4300249\n",
      "    -1.1409719 ]\n",
      "   [-2.3619554  -2.1506267  -0.96027523  3.8960817   1.8534632\n",
      "    -0.74298    -2.0000184   2.1462076  -1.9178891   2.6341922\n",
      "     2.9261186 ]\n",
      "   [ 0.09026945  0.7947642   0.09152631  3.2191868   1.7997355\n",
      "    -1.8118993  -1.6655155   0.09941932  0.47267178  1.5012183\n",
      "     0.8387092 ]\n",
      "   [-0.677145   -1.8837119  -1.8422472   5.2752542   0.19712047\n",
      "     0.6287616  -2.4682517  -0.7707627  -1.2582183   0.4480308\n",
      "    -0.31200537]\n",
      "   [ 0.18685156 -1.0029786  -0.48849955  0.7354018  -0.60926133\n",
      "    -2.234462   -2.1479487   2.2899785  -2.94474    -1.5366384\n",
      "     1.3429787 ]\n",
      "   [ 0.5181513   1.1900467   2.245966    3.1080089   1.0276145\n",
      "     0.06970618 -2.1040907   1.4412831  -1.7990694  -0.21974309\n",
      "    -0.31952235]\n",
      "   [ 1.0095067   1.1085582   0.55792695  1.105977    0.22337158\n",
      "     2.6185262   1.427797   -0.9250603   3.245243    0.9542576\n",
      "    -0.23042643]]\n",
      "\n",
      "  [[-3.0174336   0.41961712  1.9252456   0.7638777   1.4724514\n",
      "     3.0431707   2.8573203   3.2402046   2.5931482   2.2301\n",
      "     3.3224475 ]\n",
      "   [-1.3549727  -0.09280814  2.5851169  -0.3922029  -0.2696607\n",
      "     0.5922087   0.36082545 -0.7003431   0.8450714   0.36341256\n",
      "     1.4881623 ]\n",
      "   [-1.7961591  -1.8925554  -0.41655225  2.8299882   0.4466289\n",
      "     3.4457626   0.6450706   0.9305902   1.5253757   0.8443167\n",
      "     1.8832321 ]\n",
      "   [-1.1154089  -1.1745822   1.2023396  -0.06076117 -1.5096283\n",
      "     0.7106908  -1.2446266  -3.0961888   2.4934134  -0.72988504\n",
      "     2.4809296 ]\n",
      "   [-0.09934417  0.8952499  -0.6025727  -0.7601258  -4.950531\n",
      "    -0.91794896 -1.0664147  -1.4203914   0.3035166  -0.86303115\n",
      "    -1.3707315 ]\n",
      "   [-0.730198    0.4987517   0.10699637  0.04278546  0.54071033\n",
      "    -0.50309646 -3.3407922  -1.0143468   0.2559061   2.2120178\n",
      "     1.3231285 ]\n",
      "   [ 0.23826952  2.0639033  -2.888716   -1.358555   -0.28163734\n",
      "     2.404897    0.22699867  3.525614    3.2236834   6.1108885\n",
      "    -0.83842254]\n",
      "   [ 1.2659289  -0.02524091 -2.9593556   0.11135699 -0.94128567\n",
      "    -2.829995   -3.2003918  -3.8595636  -0.99238694 -0.92329174\n",
      "    -0.28336775]\n",
      "   [-0.5888609   0.8837037  -1.3720297  -0.50547075  1.1768699\n",
      "     1.318384    1.8200378   2.1513548   4.0589194   3.615378\n",
      "     2.345666  ]\n",
      "   [ 0.4514389  -2.6888735  -1.3093098  -0.7302314  -2.76163\n",
      "     1.3936642  -1.402236    0.21021515  3.8791788   2.6653714\n",
      "     2.4004152 ]\n",
      "   [ 1.5586158  -1.5779564  -2.6995533  -0.32304433 -1.462305\n",
      "    -3.5722759  -2.8448212  -2.3935144  -2.0391254  -2.3788967\n",
      "    -0.5365632 ]]\n",
      "\n",
      "  [[-3.0523906   1.8179893   4.253165    1.4998401   2.628079\n",
      "     2.1809475   1.6847974   2.1162713   1.0263407   2.2014513\n",
      "     1.4080617 ]\n",
      "   [-7.7598214  -1.2395704   4.424106    5.041027    4.0303817\n",
      "     7.204144   10.943072    3.6315343   0.10899801  3.0159957\n",
      "     3.6257553 ]\n",
      "   [-5.33444    -2.9674113   1.9738437   4.328881    4.872642\n",
      "     8.156278   11.38618     1.9597439  -2.7887135   0.01012839\n",
      "     1.6460056 ]\n",
      "   [-1.8466234  -0.9457812   2.1802762   4.8156037   0.12329892\n",
      "     0.8021099  -0.51522094 -0.3930014   0.88665265 -0.0200246\n",
      "    -0.5124091 ]\n",
      "   [-3.3392012  -4.19972    -0.9612767   6.551047    3.2264879\n",
      "     5.7523003   4.0560446   1.2217602  -3.9085174  -0.95752114\n",
      "     3.2312946 ]\n",
      "   [-1.1705554  -4.4363074  -1.1318855   3.9469442   3.668861\n",
      "     7.9511037   7.7728662   3.1097941   3.9134045   1.9645027\n",
      "     3.9028478 ]\n",
      "   [-0.29538846 -1.5871849  -2.8558958   4.954087    3.0537887\n",
      "     5.3664327   5.922907    0.4448749   2.3723075   0.9585795\n",
      "     1.8618767 ]\n",
      "   [-1.4024625  -3.924694   -1.0780026   2.6823587   2.3963602\n",
      "     4.2347217   6.6275954   2.555661    1.4216021   1.4860326\n",
      "     3.9207594 ]\n",
      "   [ 0.11296152  1.4904397  -0.46121475  4.180045    1.8322803\n",
      "     3.166133    1.6744213   2.8632095   5.733159    1.3540206\n",
      "     0.88576394]\n",
      "   [-1.2402344  -1.0717087  -0.8928821   5.5589666   3.5899298\n",
      "     4.291726    6.806953    0.87921894  1.1001962   0.6661416\n",
      "     2.164386  ]\n",
      "   [ 0.7213299  -2.1395576  -1.3645856   3.0262673   0.04822136\n",
      "     3.2282083   2.892072    0.4845196  -1.4694273  -0.19355759\n",
      "     4.067306  ]]\n",
      "\n",
      "  [[-0.6331823  -1.2349904   0.17108005 -1.7825468   0.4058478\n",
      "    -1.3177248   1.3472757  -1.038774   -0.5425702   0.61429256\n",
      "     0.58920234]\n",
      "   [ 2.119253   -4.246382    1.0599287  -6.814192   -1.6555526\n",
      "    -5.312355   -1.9065204  -3.2599387   0.61110854  1.5822006\n",
      "    -0.07370989]\n",
      "   [-1.1188248   0.14757451  1.0110303   1.6372967   0.67021793\n",
      "     2.4267418  -1.4579339  -3.0462801  -1.2310774   2.1712596\n",
      "    -1.3166082 ]\n",
      "   [ 0.95955515 -1.9993405   2.5704405  -1.7231832  -0.62369555\n",
      "    -1.1314759  -2.2201748  -2.0597012   2.0095713   2.728148\n",
      "     0.25987804]\n",
      "   [ 0.49821588 -1.729508    2.970458   -3.6262503  -2.1428034\n",
      "     0.15667306 -1.009277   -2.750008    2.0951803   3.5743556\n",
      "     0.82289606]\n",
      "   [ 2.4125378   1.1109612   3.4186935  -5.2272053  -2.9276714\n",
      "    -4.479757   -1.5780019   1.15176     4.194589   -0.8862791\n",
      "     0.890991  ]\n",
      "   [ 1.147488    0.25655437  4.761012   -4.4415584   0.4498366\n",
      "    -3.3419423  -5.0206585  -2.8960762   2.6126118   2.0494146\n",
      "     3.3928528 ]\n",
      "   [ 3.2666113  -0.7482998  -2.4978724  -4.6047583  -0.4991162\n",
      "    -1.8868389  -4.1274176  -7.000914   -0.26274645 -0.9070944\n",
      "     0.25558174]\n",
      "   [ 2.625219    0.4134868  -0.2386017  -2.9531605   0.06102017\n",
      "    -2.7949638  -3.0890677  -2.0160155  -0.665806   -1.5237277\n",
      "    -0.38411236]\n",
      "   [ 2.5969262   0.8712801   0.59294933 -2.1528423   1.917804\n",
      "    -3.1793027  -3.1771533  -2.1307032   1.078736   -1.3194213\n",
      "    -1.2344794 ]\n",
      "   [ 1.7800199   1.5686686  -5.8920283  -2.5252345  -2.1150608\n",
      "     0.5413838  -3.7546976  -1.8843988  -1.6341864  -2.627482\n",
      "    -2.8749938 ]]\n",
      "\n",
      "  [[-1.2599558   1.0058122   0.9403136  -2.169106    1.3649019\n",
      "     0.05476408 -0.2606994   0.6737088   0.974384   -0.18888953\n",
      "    -0.44247153]\n",
      "   [ 1.9198732  -0.84344107  2.0494642  -1.7659745   1.8153642\n",
      "    -1.3940618  -1.7129728   0.6147069  -0.501224    1.0274923\n",
      "    -0.02316735]\n",
      "   [ 2.8428354  -1.6724693  -1.4574461  -0.75728846 -2.8203032\n",
      "    -4.540737   -4.3868213  -0.7501175  -0.68200713 -5.1211104\n",
      "    -0.6885932 ]\n",
      "   [ 1.2147404   1.865701   -1.4707285  -1.3224499  -2.9885063\n",
      "    -0.7098579  -2.4524012   0.32578066 -0.1086136   1.557107\n",
      "    -1.5163519 ]\n",
      "   [ 2.2408023   0.45182097  0.23735748 -3.630287   -4.9238787\n",
      "    -1.6159724  -4.2166977  -0.00287914  1.6390876  -1.0723724\n",
      "    -2.1087997 ]\n",
      "   [ 2.0514302   1.124743   -0.66148055 -3.6998794  -3.634573\n",
      "    -3.2806454  -3.53534    -0.587686    0.4574493  -1.4516882\n",
      "     1.0896038 ]\n",
      "   [ 1.8713237   1.459809    4.544932    0.6970329   0.3698051\n",
      "    -1.5995883  -3.4667106  -2.5332448  -1.2041903  -0.62293327\n",
      "    -0.42425266]\n",
      "   [ 0.7016293   0.6996512  -0.24522036 -1.6627809   1.9750067\n",
      "    -0.02285164 -0.6037045  -1.2260524   0.3857271  -0.24273007\n",
      "    -0.7836356 ]\n",
      "   [-0.00567898 -1.5558052   0.70907253  0.3123517  -0.03035606\n",
      "    -0.63739645  2.4996793   0.65522164  0.10999125  3.8119931\n",
      "    -0.628228  ]\n",
      "   [ 1.6093675  -0.25890806  1.8090376   1.2600975   1.3659852\n",
      "    -0.02949921 -1.4901761  -2.0621624  -1.4694445  -1.7734735\n",
      "    -1.4515767 ]\n",
      "   [ 1.3534318  -2.1399765   0.44380033 -2.2658684   0.56147534\n",
      "    -0.23087564 -2.495326    0.37854964 -1.5594982  -2.9111617\n",
      "    -1.4390806 ]]\n",
      "\n",
      "  [[-2.1587913   1.7147038   1.544837    2.9021912  -0.54687065\n",
      "     0.69791436  0.48692545  0.3874992   1.747807    0.6198854\n",
      "    -0.5201634 ]\n",
      "   [-0.00582398 -2.3462925   1.799922    0.32839322  0.3320445\n",
      "    -2.016989   -2.8966556  -2.9691656   0.9186906  -3.0179012\n",
      "    -0.61179817]\n",
      "   [-2.1043587  -2.8178024   2.2148285   3.891037    2.2759385\n",
      "     2.491633    0.34263963  3.354882    2.579082    2.9529788\n",
      "     3.6382704 ]\n",
      "   [ 1.8959482  -3.3403587   2.0201633  -2.8513105   2.2833853\n",
      "    -1.5907921  -5.267708   -0.7608538  -1.667431   -4.55585\n",
      "     4.9639316 ]\n",
      "   [ 1.2664658  -1.2141082   0.86169004 -0.82817227  1.8247795\n",
      "    -2.2689962  -2.9155438  -0.7371273  -1.1245675  -3.761395\n",
      "     2.009546  ]\n",
      "   [ 1.4200376  -0.8142122  -0.15309595 -2.757525   -0.03373783\n",
      "    -3.5767376  -0.7845442   0.7012546   0.22206096 -1.2080317\n",
      "     3.7817411 ]\n",
      "   [ 0.9523497   2.5999942  -0.72424364 -1.2373599  -0.06541023\n",
      "    -2.280726   -0.30826616 -1.5803338  -0.43647492 -3.3566196\n",
      "     0.5201951 ]\n",
      "   [-0.66845006 -1.2726071   0.88486147 -0.17885678  0.5529859\n",
      "     0.5443401   1.7109423   0.3173334   0.40885562 -0.50677687\n",
      "    -0.56862044]\n",
      "   [-1.6181766   2.127004   -3.3242197   2.8492422  -2.3754318\n",
      "    -1.4359603  -2.8194427   0.7514017  -1.035433   -4.766356\n",
      "     1.134394  ]\n",
      "   [ 1.1412492   0.47997436  0.5518038   2.792524    1.0626367\n",
      "    -0.9522897  -3.9988208  -0.34751797 -2.0532994  -4.8456335\n",
      "     2.3797278 ]\n",
      "   [-0.3825621  -2.0746894  -1.9134712   0.4164186  -1.1360947\n",
      "     0.30515397  0.87765074  1.9375149   1.431657    1.5040631\n",
      "     0.85646003]]\n",
      "\n",
      "  [[-2.0107505   2.0400176   1.5965837   0.41086924  2.151286\n",
      "     2.0080416   3.8806849   3.7532885   0.60808486  1.1465902\n",
      "     2.6378512 ]\n",
      "   [-1.2814682  -2.6879327   1.4795719   0.70971787  1.2221687\n",
      "     4.831139    4.012818    3.0739448  -0.3462172  -0.15153797\n",
      "     3.445116  ]\n",
      "   [ 0.3697977   0.11362538 -0.8682849   1.8676807   1.5285181\n",
      "    -2.5126774  -0.09541667 -0.62111366 -0.5378774  -0.9066283\n",
      "    -2.19526   ]\n",
      "   [ 0.17680757 -0.59016186 -0.07212502  1.788976    1.8290929\n",
      "     1.2414881   2.5110507   0.9378911   1.4032513   2.5400653\n",
      "     3.61587   ]\n",
      "   [ 0.27183303 -2.8065841  -0.40462255 -2.3706849  -0.47989556\n",
      "    -0.17444041  1.5252     -0.41247755  0.18653843 -1.4950514\n",
      "     2.1371236 ]\n",
      "   [ 0.8829943  -3.017673   -2.083954    2.1678572   2.4494154\n",
      "    -0.16720197  2.1018574  -0.03369299 -0.08712415 -0.03370773\n",
      "     5.117769  ]\n",
      "   [ 0.36083797 -0.05401262  0.63395387  1.9816741   1.4668022\n",
      "     2.7947261   0.7702297   0.40087664 -1.3949666   0.64655036\n",
      "     2.5119135 ]\n",
      "   [ 0.28325427 -1.9323467   0.58271986  2.48945     0.9275244\n",
      "     2.8025713   1.751703    0.5133646  -2.6412737   0.6172831\n",
      "     3.7134004 ]\n",
      "   [-1.0369177  -0.8422439  -0.00286578 -1.2839406  -0.02112548\n",
      "     3.2767828   3.8468044   4.8574357  -0.255869    4.6949525\n",
      "     4.5702515 ]\n",
      "   [ 0.6051889  -2.837119    0.79984003 -3.0200691  -0.1500156\n",
      "     2.4176633   1.0713742   0.5682465  -0.72345775 -1.7475139\n",
      "     2.119047  ]\n",
      "   [-0.03567908 -1.4176497  -1.6316394  -2.158847   -3.8596373\n",
      "     0.2778311  -2.6070874  -0.22384848 -2.9864957  -1.0438336\n",
      "    -2.087145  ]]]], shape=(1, 8, 11, 11), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 11, 11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape = (1, 8, 11, 32)\n",
      "scaled_attention.shape= (1, 11, 8, 32)\n",
      "concat_attention.shape= (1, 11, 256)\n",
      "outputs.shape= (1, 11, 256)\n",
      "(1, 11, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 11, 256)\n",
      "(1, 11, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 11, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 11, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.5176607    1.9687198    2.0989769   -0.47190297]\n",
      "   [ -0.49124843   0.08534324   0.2991765   -0.48528743]\n",
      "   [  0.24215932  -4.697081    -4.019181     0.1927161 ]\n",
      "   [  0.7396501   -3.0292149   -2.48159      0.7256521 ]\n",
      "   [  0.7377838   -1.2575825   -1.0456651    0.7396505 ]\n",
      "   [  0.12743154  -2.1013763   -1.7714081    0.15186152]\n",
      "   [  0.57692707  -1.6948001   -1.6039941    0.56792045]\n",
      "   [ -0.0456275   -3.2555232   -2.8839726   -0.07279108]\n",
      "   [  0.541922    -4.2346587   -4.088871     0.5205673 ]\n",
      "   [  0.43123668  -4.4550576   -4.342825     0.38334304]\n",
      "   [  0.2683728   -2.690839    -3.0562706    0.2541689 ]]\n",
      "\n",
      "  [[ -0.5409756    2.8193953    2.0520196   -0.5236139 ]\n",
      "   [ -1.0098997    4.226584     3.5640724   -0.9722929 ]\n",
      "   [ -0.31559435  -2.7033126   -2.1731372   -0.34413084]\n",
      "   [  0.67385     -4.3054934   -2.5939085    0.6549963 ]\n",
      "   [  0.7716512   -4.6774607   -3.4361272    0.73228556]\n",
      "   [  0.30699015  -2.0128524   -1.77724      0.3016845 ]\n",
      "   [  0.95172566  -5.123536    -3.7028008    0.9193312 ]\n",
      "   [  0.1625587   -2.8245268   -2.2594504    0.1333035 ]\n",
      "   [  0.80569774  -2.9156864   -1.9677489    0.81221676]\n",
      "   [  0.5848501   -4.6709986   -3.4369607    0.5637138 ]\n",
      "   [  0.8923555   -4.06759     -2.6578312    0.8754998 ]]\n",
      "\n",
      "  [[ -0.42360795   3.4683292    3.2733347   -0.39263082]\n",
      "   [ -0.47245097   0.7355631    1.3526613   -0.47366753]\n",
      "   [  0.8764782   -5.054033    -4.6074905    0.832105  ]\n",
      "   [  1.5158468   -5.5036316   -5.460711     1.4665604 ]\n",
      "   [  1.3198522   -4.7758727   -4.838473     1.2815847 ]\n",
      "   [  0.6756115   -5.767558    -5.86507      0.60872716]\n",
      "   [  0.6050416   -6.19143     -6.4390326    0.54084617]\n",
      "   [  0.44500238  -0.68225163  -0.9241052    0.42594016]\n",
      "   [  1.465985    -3.2105703   -2.976426     1.4414983 ]\n",
      "   [  1.1924549   -4.355453    -4.300772     1.1541699 ]\n",
      "   [  1.3496062   -1.49384     -1.159258     1.3560332 ]]\n",
      "\n",
      "  [[ -1.6453142    4.41468      3.7185764   -1.6388824 ]\n",
      "   [  0.1504475   -1.3770971   -0.84712774   0.10965424]\n",
      "   [  1.5174129   -5.9414515   -4.9204936    1.4784044 ]\n",
      "   [  1.9338356   -4.261786    -3.374979     1.932654  ]\n",
      "   [  1.5312709   -4.5473847   -3.4884474    1.480693  ]\n",
      "   [  2.4686995   -5.566623    -4.674686     2.4672065 ]\n",
      "   [  2.0171244   -5.156834    -4.0178304    1.9925612 ]\n",
      "   [  1.4369041   -3.667318    -2.7229993    1.4106048 ]\n",
      "   [  1.7216051   -4.4552937   -3.1465628    1.6921057 ]\n",
      "   [  1.9970524   -6.690838    -5.398477     1.966113  ]\n",
      "   [  1.4490299   -3.933407    -3.2366366    1.4087825 ]]\n",
      "\n",
      "  [[ -1.29121      2.8940606    1.9502285   -1.3604355 ]\n",
      "   [  0.00294916  -1.2568096   -0.89668256  -0.00931877]\n",
      "   [  0.7530518   -2.5598295   -1.9020394    0.7952453 ]\n",
      "   [  1.7065613   -5.794166    -4.5830455    1.7274754 ]\n",
      "   [  1.828848    -7.046152    -5.713792     1.8388789 ]\n",
      "   [  2.3361118   -9.356349    -7.713717     2.356966  ]\n",
      "   [  2.4712012   -7.777283    -6.1303487    2.5265574 ]\n",
      "   [  2.178688    -7.1766615   -5.927077     2.238128  ]\n",
      "   [  2.4477205   -5.1921487   -3.4574718    2.464835  ]\n",
      "   [  2.341771    -5.8448753   -4.0209694    2.4139361 ]\n",
      "   [  1.5746661   -4.5661435   -4.1088657    1.5959098 ]]\n",
      "\n",
      "  [[ -0.65288585   3.4191878    2.6229937   -0.6571499 ]\n",
      "   [  0.72896236  -2.1471467   -1.9732635    0.77281165]\n",
      "   [  0.32350188  -1.2035745   -1.7607827    0.34396917]\n",
      "   [  1.0276341    1.058868     1.2416905    1.0519707 ]\n",
      "   [  1.6617185   -1.5399216   -1.5241555    1.7414258 ]\n",
      "   [  2.3202841   -3.4387836   -2.1360145    2.3670495 ]\n",
      "   [  2.2568145   -5.2002926   -3.3399537    2.3179612 ]\n",
      "   [  2.10796     -4.739115    -2.9632866    2.1519184 ]\n",
      "   [  1.2009804   -1.6315452   -1.1646751    1.2423136 ]\n",
      "   [  1.612212    -4.7317686   -3.3088357    1.6307019 ]\n",
      "   [  0.8388548   -2.1863418   -1.2897081    0.847988  ]]\n",
      "\n",
      "  [[ -0.13920632   1.920276     1.8218281   -0.1018336 ]\n",
      "   [  0.19900118  -1.852921    -1.364047     0.17529728]\n",
      "   [  0.9201308   -2.3837185   -1.2430844    0.8900646 ]\n",
      "   [  1.0384122   -2.2614129   -0.6760908    1.014745  ]\n",
      "   [  1.3033148   -4.046018    -2.5787446    1.2661915 ]\n",
      "   [  1.1603479   -2.2507179   -1.0619235    1.1338618 ]\n",
      "   [  1.041217    -4.2461724   -2.9057727    1.0158387 ]\n",
      "   [  1.0148015   -1.7258983   -0.37147498   1.0063083 ]\n",
      "   [  0.88598007  -2.206053    -0.84811103   0.8885484 ]\n",
      "   [  1.2307554   -3.4753895   -2.1146924    1.1983802 ]\n",
      "   [  0.5709064   -1.697938    -1.0712342    0.54415566]]\n",
      "\n",
      "  [[ -1.5968649    5.382058     4.5500836   -1.5562288 ]\n",
      "   [  0.86753845  -4.5697775   -4.1758018    0.8370436 ]\n",
      "   [  1.9240823   -9.450348    -8.618066     1.8469068 ]\n",
      "   [  1.4563193   -6.307143    -5.9952025    1.4192016 ]\n",
      "   [  1.9546863   -5.530293    -5.110774     1.9280047 ]\n",
      "   [  1.7568917   -6.1661077   -5.8179903    1.6855166 ]\n",
      "   [  3.4059234  -12.176341   -11.224475     3.3215392 ]\n",
      "   [  1.7747686   -8.078717    -7.356744     1.7220855 ]\n",
      "   [  2.41111     -7.0833406   -6.4738836    2.3771992 ]\n",
      "   [  2.242727    -8.106706    -7.0536833    2.1916888 ]\n",
      "   [  0.52623034  -3.2455208   -2.7358427    0.4870345 ]]]], shape=(1, 8, 11, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 11, 4)\n",
      "output.shape = (1, 8, 11, 32)\n",
      "scaled_attention.shape= (1, 11, 8, 32)\n",
      "concat_attention.shape= (1, 11, 256)\n",
      "outputs.shape= (1, 11, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 12, 256)\n",
      "(1, 12, 256)\n",
      "(1, 12, 256)\n",
      "split_heads()\n",
      "(1, 12, 256)\n",
      "(1, 12, 8, 32)\n",
      "split_heads()\n",
      "(1, 12, 256)\n",
      "(1, 12, 8, 32)\n",
      "split_heads()\n",
      "(1, 12, 256)\n",
      "(1, 12, 8, 32)\n",
      "(1, 8, 12, 32)\n",
      "(1, 8, 12, 32)\n",
      "(1, 8, 12, 32)\n",
      "matmul_qk.shape = (1, 8, 12, 12)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    0.34484416   0.3515555  ...   1.932782\n",
      "      0.7453384    1.336325  ]\n",
      "   [ -1.5524524    0.57553977   1.342203   ...   3.7598073\n",
      "      1.7110962    3.572267  ]\n",
      "   [  0.17656448  -2.1603198   -3.1747456  ...  -3.3319168\n",
      "     -3.3131418   -2.024164  ]\n",
      "   ...\n",
      "   [  0.25923175  -3.3190057   -3.0277622  ...  -8.078694\n",
      "      1.0116674   -0.6146575 ]\n",
      "   [ -0.73776734   1.8399746    0.59010684 ...   2.43283\n",
      "      2.8597143    2.0337913 ]\n",
      "   [  0.09002788   1.109401    -4.6057897  ...   2.5962481\n",
      "      3.1739016   -1.6673135 ]]\n",
      "\n",
      "  [[ -0.1123142    0.09086403  -0.2228786  ...   0.18260159\n",
      "      0.24836148   0.3236551 ]\n",
      "   [ -0.27778682  -7.685327    -0.20150177 ...  -0.02028271\n",
      "     -1.8037955    4.0160594 ]\n",
      "   [  0.30459622  -1.6710656   -3.094567   ...   2.8611202\n",
      "      1.7720222    1.0634472 ]\n",
      "   ...\n",
      "   [  0.0895261   -3.0075777    2.4791887  ...   2.697197\n",
      "      2.0474095    5.3758783 ]\n",
      "   [  0.2823794   -0.01970736   1.9152175  ...  -1.3675343\n",
      "     -1.3401513    1.7858346 ]\n",
      "   [  0.665571    -0.15358742  -2.876155   ...   0.5935589\n",
      "      2.8704362    2.6577487 ]]\n",
      "\n",
      "  [[ -0.10918517  -0.21059825   0.3513798  ...   0.11337968\n",
      "      0.6242574    0.13861714]\n",
      "   [  0.2860052   -5.063574    -2.1691     ...   0.80234605\n",
      "     -0.38849393   0.5963741 ]\n",
      "   [  0.6062982    2.719351    -4.9581633  ...  -1.7919333\n",
      "     -0.9979213   -2.2543988 ]\n",
      "   ...\n",
      "   [  0.00192533   4.0501933   -0.5385378  ...  -6.671878\n",
      "     -6.3473577   -4.991295  ]\n",
      "   [  0.04607371  -2.411195    -0.708032   ...   1.6788616\n",
      "     -2.59234      1.8564506 ]\n",
      "   [ -0.41917393   3.2796376    2.2793686  ...  -2.277859\n",
      "     -1.6428612    0.2138124 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ -0.00999826  -0.5171405    0.3842402  ...   0.20075996\n",
      "     -0.16126524  -0.11150979]\n",
      "   [  0.3875343    2.307618    -2.0652568  ...   2.9078257\n",
      "      4.2148504    2.116757  ]\n",
      "   [  0.5494998    0.9012554   -5.717646   ...   1.9827068\n",
      "      1.2549968    2.7565124 ]\n",
      "   ...\n",
      "   [  0.48386398  -3.646351    -3.5835953  ...  -7.4019194\n",
      "     -1.0366647    0.75298184]\n",
      "   [  1.141735    -1.7643471   -3.4391265  ...  -6.320883\n",
      "     -7.662214    -3.9245918 ]\n",
      "   [ -0.61578333  -0.81781656   1.0908073  ...  -0.67034334\n",
      "     -1.118294    -4.1658053 ]]\n",
      "\n",
      "  [[ -0.20863976   0.0058533    0.141982   ...   0.92187274\n",
      "      0.7320269    0.6067154 ]\n",
      "   [ -0.5011563   -5.83805      3.0883057  ...   3.4072495\n",
      "      1.0007838    0.7504345 ]\n",
      "   [ -0.08541125   2.1106565    0.02025323 ...  -0.92864716\n",
      "      1.0689429    3.5151935 ]\n",
      "   ...\n",
      "   [  1.4510003    1.0021973   -2.8439877  ... -10.053152\n",
      "     -4.336011    -6.991597  ]\n",
      "   [  1.678265    -0.7698982   -3.5583017  ...  -3.413887\n",
      "     -4.9778514   -6.6539764 ]\n",
      "   [  2.8199084   -4.274163    -4.867208   ...  -9.563255\n",
      "     -5.7247386  -10.785435  ]]\n",
      "\n",
      "  [[ -0.15726982   0.34343007  -0.0106815  ...   0.13644844\n",
      "     -0.01431696   0.10441692]\n",
      "   [  0.42748785  -3.541775    -4.416529   ...   1.6207062\n",
      "      1.7149286    1.8963295 ]\n",
      "   [  0.09384713   3.2008502   -4.3988295  ...  -2.692548\n",
      "     -1.168847    -0.9526548 ]\n",
      "   ...\n",
      "   [  0.4018605   -0.47903845  -3.2845047  ...  -3.029793\n",
      "      2.6395364    1.3257711 ]\n",
      "   [ -0.04158778  -2.3729644   -3.8405085  ...  -3.097527\n",
      "     -5.2292576    0.38647527]\n",
      "   [  0.7140645   -2.3547983   -0.05257716 ...   0.18929115\n",
      "      3.4063368    3.2287633 ]]]], shape=(1, 8, 12, 12), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 12, 12)\n",
      "output.shape = (1, 8, 12, 32)\n",
      "scaled_attention.shape= (1, 12, 8, 32)\n",
      "concat_attention.shape= (1, 12, 256)\n",
      "outputs.shape= (1, 12, 256)\n",
      "(1, 12, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 12, 256)\n",
      "(1, 12, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 12, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 12, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.6510173   -2.8665452   -1.9706712    0.6258339 ]\n",
      "   [ -1.2761326   -0.6591419   -0.772127    -1.2929033 ]\n",
      "   [  1.0074586   -3.2963974   -3.106445     0.9872349 ]\n",
      "   [  1.2579683   -4.881622    -3.767476     1.2469316 ]\n",
      "   [  1.3234984   -5.9344664   -5.2370906    1.2870476 ]\n",
      "   [  1.7000594   -3.6593008   -2.971424     1.7176253 ]\n",
      "   [  1.2776281   -1.9751713   -1.519901     1.2974825 ]\n",
      "   [  0.58336854  -1.1657993   -0.539427     0.5778812 ]\n",
      "   [  0.7600763   -0.7146235    0.06742804   0.76477766]\n",
      "   [  0.68825406   0.40578374   1.1206802    0.7197399 ]\n",
      "   [  1.5945777   -1.6418726   -0.5148084    1.594585  ]\n",
      "   [  1.7847351   -0.36713108   0.68376136   1.8038828 ]]\n",
      "\n",
      "  [[  1.5699102   -4.8338065   -2.8935153    1.5974954 ]\n",
      "   [  1.602976    -6.0585413   -4.3626466    1.6033887 ]\n",
      "   [  0.84273714  -3.3321726   -3.019569     0.8300638 ]\n",
      "   [  1.739568    -5.32123     -4.1527953    1.7057066 ]\n",
      "   [  1.9442569   -4.8756948   -3.3264403    1.9580569 ]\n",
      "   [  3.9294844  -11.821626    -9.099886     3.8940978 ]\n",
      "   [  2.184128    -5.056767    -3.4162076    2.1828146 ]\n",
      "   [  1.9612429   -4.841177    -3.8993757    1.9119158 ]\n",
      "   [  2.1313474   -8.619019    -7.0290985    2.1060083 ]\n",
      "   [  1.7264353   -4.697804    -4.539785     1.657041  ]\n",
      "   [  2.3086565   -7.632138    -5.643827     2.2703083 ]\n",
      "   [  3.4966753   -9.881069    -7.4213333    3.445355  ]]\n",
      "\n",
      "  [[ -0.9479617   -1.5584124   -2.3440182   -0.91537315]\n",
      "   [  1.1684959   -6.083402    -5.361126     1.1414931 ]\n",
      "   [  0.8458585   -8.91149     -7.7787905    0.78298134]\n",
      "   [  2.5803494   -5.880795    -5.0921974    2.5411623 ]\n",
      "   [  3.3461082  -11.678779   -10.798813     3.272511  ]\n",
      "   [  3.3453295   -7.0702214   -5.903679     3.2953756 ]\n",
      "   [  1.6342559   -4.6711273   -4.3739734    1.6045712 ]\n",
      "   [  1.7710527   -4.211759    -3.7379825    1.7677531 ]\n",
      "   [  2.3430088   -5.5874043   -4.7693295    2.3498065 ]\n",
      "   [  1.7600528   -4.8194313   -3.658041     1.7406274 ]\n",
      "   [  1.8383347   -4.2881994   -3.964903     1.8433948 ]\n",
      "   [  3.1041694   -6.2948246   -5.899711     3.0869215 ]]\n",
      "\n",
      "  [[ -0.53032047  -2.1487055   -3.3869808   -0.5465881 ]\n",
      "   [ -0.37673792   0.5297353    0.7943683   -0.41485333]\n",
      "   [  1.2794939   -4.2864437   -4.2701783    1.2569122 ]\n",
      "   [  2.427947    -1.600872    -0.90655017   2.481265  ]\n",
      "   [  2.3424263   -7.2077713   -5.824986     2.335523  ]\n",
      "   [  1.7222499   -2.7512228   -2.3087296    1.7864217 ]\n",
      "   [  1.0004832   -7.1413507   -6.550554     0.95411247]\n",
      "   [  1.4302231   -3.8393989   -3.2885814    1.4499868 ]\n",
      "   [  1.734392    -5.0363717   -4.562415     1.7265733 ]\n",
      "   [  1.7437402   -4.61876     -3.9143934    1.71687   ]\n",
      "   [  1.1339672   -3.6319485   -2.8497448    1.1018116 ]\n",
      "   [  1.510794    -1.2551379   -0.05228771   1.5522768 ]]\n",
      "\n",
      "  [[ -1.8786137    3.8044016    1.7851309   -1.9308672 ]\n",
      "   [ -2.0427704    1.9088362    0.8611902   -2.10597   ]\n",
      "   [  0.3713629   -6.5012736   -5.5806484    0.3149101 ]\n",
      "   [  2.3904674   -0.8018907    0.54941446   2.4430196 ]\n",
      "   [  3.2833803   -5.793328    -3.5530388    3.3556385 ]\n",
      "   [  4.031575    -8.23709     -5.352422     4.1288157 ]\n",
      "   [  1.704372    -4.0030203   -2.8744233    1.746541  ]\n",
      "   [  0.55620587  -0.81578505  -0.7211592    0.5843559 ]\n",
      "   [  1.4359853   -5.1196675   -3.4192846    1.4993091 ]\n",
      "   [  3.066259    -8.724568    -6.3771715    3.138459  ]\n",
      "   [  3.077098    -5.4629254   -3.248803     3.1306965 ]\n",
      "   [  4.6866465   -9.521488    -5.9786706    4.7835746 ]]\n",
      "\n",
      "  [[  1.8587611   -2.3004422   -1.999603     1.8540798 ]\n",
      "   [  0.62134135  -1.1685245   -1.0336286    0.63556284]\n",
      "   [  0.93438053  -3.9921112   -3.1974504    0.91454566]\n",
      "   [  2.386284    -8.792911    -6.874719     2.3729997 ]\n",
      "   [  2.8132436  -11.001211    -8.411129     2.7784264 ]\n",
      "   [  2.7941167   -3.5710614   -2.4098222    2.8085887 ]\n",
      "   [  2.5607827   -5.1447167   -4.0192566    2.5128086 ]\n",
      "   [  1.9819213   -4.7589607   -4.3256965    1.9169271 ]\n",
      "   [  2.1862228   -3.8017366   -2.162978     2.196947  ]\n",
      "   [  2.025622    -0.50651217  -0.09534571   2.0341551 ]\n",
      "   [  1.7751181   -5.396942    -3.6041234    1.7526699 ]\n",
      "   [  1.8905165   -2.984126    -2.7361436    1.8944441 ]]\n",
      "\n",
      "  [[  0.263768     1.3027129    0.7912128    0.31364933]\n",
      "   [  0.90733486  -1.2863759   -0.23643312   0.92466456]\n",
      "   [  1.9193958   -5.6561565   -3.624307     1.9176353 ]\n",
      "   [  2.590705    -7.7614427   -5.438415     2.6344485 ]\n",
      "   [  2.0141904   -6.001995    -3.7005918    2.02994   ]\n",
      "   [  2.3413384   -2.5982022   -0.645182     2.369621  ]\n",
      "   [  1.1849254   -0.41514406   0.7534283    1.1835097 ]\n",
      "   [  1.6196365   -2.736624    -1.3294117    1.6487104 ]\n",
      "   [  1.8510432   -2.5615194   -0.7961973    1.8711935 ]\n",
      "   [  2.0705955   -4.3488426   -2.0247169    2.1132114 ]\n",
      "   [  1.9942156   -2.1047823   -0.90358776   2.0466616 ]\n",
      "   [  0.49939027  -1.217362    -0.46714777   0.52691543]]\n",
      "\n",
      "  [[ -0.50981736   4.1422954    3.6196024   -0.43590528]\n",
      "   [  0.4603232   -1.0371166   -0.40937394   0.4394898 ]\n",
      "   [  1.2603666   -3.7127948   -2.6582818    1.2983314 ]\n",
      "   [  1.9421246   -6.2256336   -3.6375484    1.9323288 ]\n",
      "   [  1.3231326   -4.248828    -3.1260052    1.3073777 ]\n",
      "   [  2.442132    -7.4952087   -5.2700887    2.4572377 ]\n",
      "   [  2.4187922   -5.27305     -2.1958756    2.4965396 ]\n",
      "   [  1.0935532   -3.028773    -1.9892881    1.1174632 ]\n",
      "   [  1.07857     -1.6685917   -0.5144004    1.1236442 ]\n",
      "   [  1.9439204   -4.8428583   -2.7191334    2.0016615 ]\n",
      "   [  1.3960849   -4.567839    -2.928855     1.4403856 ]\n",
      "   [  1.6036419   -5.08729     -3.1608024    1.6399496 ]]]], shape=(1, 8, 12, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 12, 4)\n",
      "output.shape = (1, 8, 12, 32)\n",
      "scaled_attention.shape= (1, 12, 8, 32)\n",
      "concat_attention.shape= (1, 12, 256)\n",
      "outputs.shape= (1, 12, 256)\n",
      "(1, 12, 256)\n",
      "(1, 12, 256)\n",
      "(1, 12, 256)\n",
      "split_heads()\n",
      "(1, 12, 256)\n",
      "(1, 12, 8, 32)\n",
      "split_heads()\n",
      "(1, 12, 256)\n",
      "(1, 12, 8, 32)\n",
      "split_heads()\n",
      "(1, 12, 256)\n",
      "(1, 12, 8, 32)\n",
      "(1, 8, 12, 32)\n",
      "(1, 8, 12, 32)\n",
      "(1, 8, 12, 32)\n",
      "matmul_qk.shape = (1, 8, 12, 12)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181 -0.6147039  ...  1.4070063   0.65330005\n",
      "     0.3939021 ]\n",
      "   [ 3.96568    -0.17723085  1.5590783  ...  0.44090855 -2.0952516\n",
      "    -0.88445807]\n",
      "   [ 3.9833748  -2.2539997   0.15140963 ... -1.7445444  -1.6135567\n",
      "     0.05757904]\n",
      "   ...\n",
      "   [ 4.277896   -0.13539058  3.4492354  ... -1.5360774  -3.084445\n",
      "    -1.4161737 ]\n",
      "   [ 1.5408126   0.0754276   0.5586133  ... -0.15151803 -2.193409\n",
      "    -2.6331093 ]\n",
      "   [ 4.2881947   1.0258555   2.2919047  ... -5.0450525  -2.8535032\n",
      "    -4.5004025 ]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489  -0.22415894 ... -0.35023493 -0.12138899\n",
      "    -0.79827315]\n",
      "   [ 0.22428446 -2.402899   -1.7699208  ... -1.0865968  -1.0353341\n",
      "    -2.03192   ]\n",
      "   [-0.6712342   0.30168235 -3.0742364  ... -3.0090327  -3.8419588\n",
      "    -1.2976645 ]\n",
      "   ...\n",
      "   [ 0.5181513   1.1900467   2.245966   ... -0.21974309 -0.31952235\n",
      "     0.10209978]\n",
      "   [ 1.0095067   1.1085582   0.55792695 ...  0.9542576  -0.23042643\n",
      "    -3.101134  ]\n",
      "   [ 4.978718    3.8684566   3.078466   ...  1.9019464   0.18915287\n",
      "    -4.669372  ]]\n",
      "\n",
      "  [[-3.0174336   0.41961712  1.9252456  ...  2.2301      3.3224475\n",
      "     3.3229702 ]\n",
      "   [-1.3549727  -0.09280814  2.5851169  ...  0.36341256  1.4881623\n",
      "    -0.4787041 ]\n",
      "   [-1.7961591  -1.8925554  -0.41655225 ...  0.8443167   1.8832321\n",
      "    -0.76940006]\n",
      "   ...\n",
      "   [ 0.4514389  -2.6888735  -1.3093098  ...  2.6653714   2.4004152\n",
      "     1.59694   ]\n",
      "   [ 1.5586158  -1.5779564  -2.6995533  ... -2.3788967  -0.5365632\n",
      "    -0.945924  ]\n",
      "   [ 0.8576953  -3.6549006  -5.4420285  ...  1.9438738   1.936718\n",
      "     4.0205626 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.2599558   1.0058122   0.9403136  ... -0.18888953 -0.44247153\n",
      "    -1.5453403 ]\n",
      "   [ 1.9198732  -0.84344107  2.0494642  ...  1.0274923  -0.02316735\n",
      "     0.35112238]\n",
      "   [ 2.8428354  -1.6724693  -1.4574461  ... -5.1211104  -0.6885932\n",
      "    -0.15051317]\n",
      "   ...\n",
      "   [ 1.6093675  -0.25890806  1.8090376  ... -1.7734735  -1.4515767\n",
      "    -0.25420216]\n",
      "   [ 1.3534318  -2.1399765   0.44380033 ... -2.9111617  -1.4390806\n",
      "    -0.27244264]\n",
      "   [ 0.84742606  0.0772418   1.5468452  ... -0.16069375  0.02474112\n",
      "    -1.8008659 ]]\n",
      "\n",
      "  [[-2.1587913   1.7147038   1.544837   ...  0.6198854  -0.5201634\n",
      "     0.18126747]\n",
      "   [-0.00582398 -2.3462925   1.799922   ... -3.0179012  -0.61179817\n",
      "    -2.3670635 ]\n",
      "   [-2.1043587  -2.8178024   2.2148285  ...  2.9529788   3.6382704\n",
      "     2.1391535 ]\n",
      "   ...\n",
      "   [ 1.1412492   0.47997436  0.5518038  ... -4.8456335   2.3797278\n",
      "     0.40395227]\n",
      "   [-0.3825621  -2.0746894  -1.9134712  ...  1.5040631   0.85646003\n",
      "     1.4463311 ]\n",
      "   [ 1.7960831   2.5962052  -1.3429192  ... -3.1949847  -5.2735734\n",
      "    -4.4235926 ]]\n",
      "\n",
      "  [[-2.0107505   2.0400176   1.5965837  ...  1.1465902   2.6378512\n",
      "     2.0324154 ]\n",
      "   [-1.2814682  -2.6879327   1.4795719  ... -0.15153797  3.445116\n",
      "     2.5140755 ]\n",
      "   [ 0.3697977   0.11362538 -0.8682849  ... -0.9066283  -2.19526\n",
      "     0.45149922]\n",
      "   ...\n",
      "   [ 0.6051889  -2.837119    0.79984003 ... -1.7475139   2.119047\n",
      "     2.131584  ]\n",
      "   [-0.03567908 -1.4176497  -1.6316394  ... -1.0438336  -2.087145\n",
      "    -0.23750614]\n",
      "   [ 0.44519848 -4.1498594  -0.6811251  ... -2.6795666   2.9410086\n",
      "    -0.93907857]]]], shape=(1, 8, 12, 12), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 12, 12)\n",
      "output.shape = (1, 8, 12, 32)\n",
      "scaled_attention.shape= (1, 12, 8, 32)\n",
      "concat_attention.shape= (1, 12, 256)\n",
      "outputs.shape= (1, 12, 256)\n",
      "(1, 12, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 12, 256)\n",
      "(1, 12, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 12, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 12, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.5176607    1.9687198    2.0989769   -0.47190297]\n",
      "   [ -0.49124843   0.08534324   0.2991765   -0.48528743]\n",
      "   [  0.24215932  -4.697081    -4.019181     0.1927161 ]\n",
      "   [  0.7396501   -3.0292149   -2.48159      0.7256521 ]\n",
      "   [  0.7377838   -1.2575825   -1.0456651    0.7396505 ]\n",
      "   [  0.12743154  -2.1013763   -1.7714081    0.15186152]\n",
      "   [  0.57692707  -1.6948001   -1.6039941    0.56792045]\n",
      "   [ -0.0456275   -3.2555232   -2.8839726   -0.07279108]\n",
      "   [  0.541922    -4.2346587   -4.088871     0.5205673 ]\n",
      "   [  0.43123668  -4.4550576   -4.342825     0.38334304]\n",
      "   [  0.2683728   -2.6908388   -3.0562704    0.25416884]\n",
      "   [  0.65151817  -2.3087127   -2.7818563    0.6532232 ]]\n",
      "\n",
      "  [[ -0.5409756    2.8193953    2.0520196   -0.5236139 ]\n",
      "   [ -1.0098997    4.226584     3.5640724   -0.9722929 ]\n",
      "   [ -0.31559435  -2.7033126   -2.1731372   -0.34413084]\n",
      "   [  0.67385     -4.3054934   -2.5939085    0.6549963 ]\n",
      "   [  0.7716512   -4.6774607   -3.4361272    0.73228556]\n",
      "   [  0.30699015  -2.0128524   -1.77724      0.3016845 ]\n",
      "   [  0.95172566  -5.123536    -3.7028008    0.9193312 ]\n",
      "   [  0.1625587   -2.8245268   -2.2594504    0.1333035 ]\n",
      "   [  0.80569774  -2.9156864   -1.9677489    0.81221676]\n",
      "   [  0.5848501   -4.6709986   -3.4369607    0.5637138 ]\n",
      "   [  0.8923553   -4.0675893   -2.657831     0.87549967]\n",
      "   [  1.2629194   -3.7040567   -2.924148     1.2593203 ]]\n",
      "\n",
      "  [[ -0.42360795   3.4683292    3.2733347   -0.39263082]\n",
      "   [ -0.47245097   0.7355631    1.3526613   -0.47366753]\n",
      "   [  0.8764782   -5.054033    -4.6074905    0.832105  ]\n",
      "   [  1.5158468   -5.5036316   -5.460711     1.4665604 ]\n",
      "   [  1.3198522   -4.7758727   -4.838473     1.2815847 ]\n",
      "   [  0.6756115   -5.767558    -5.86507      0.60872716]\n",
      "   [  0.6050416   -6.19143     -6.4390326    0.54084617]\n",
      "   [  0.44500238  -0.68225163  -0.9241052    0.42594016]\n",
      "   [  1.465985    -3.2105703   -2.976426     1.4414983 ]\n",
      "   [  1.1924549   -4.355453    -4.300772     1.1541699 ]\n",
      "   [  1.3496063   -1.4938401   -1.1592588    1.3560332 ]\n",
      "   [  0.79303217  -2.173227    -1.8807111    0.78589594]]\n",
      "\n",
      "  [[ -1.6453142    4.41468      3.7185764   -1.6388824 ]\n",
      "   [  0.1504475   -1.3770971   -0.84712774   0.10965424]\n",
      "   [  1.5174129   -5.9414515   -4.9204936    1.4784044 ]\n",
      "   [  1.9338356   -4.261786    -3.374979     1.932654  ]\n",
      "   [  1.5312709   -4.5473847   -3.4884474    1.480693  ]\n",
      "   [  2.4686995   -5.566623    -4.674686     2.4672065 ]\n",
      "   [  2.0171244   -5.156834    -4.0178304    1.9925612 ]\n",
      "   [  1.4369041   -3.667318    -2.7229993    1.4106048 ]\n",
      "   [  1.7216051   -4.4552937   -3.1465628    1.6921057 ]\n",
      "   [  1.9970524   -6.690838    -5.398477     1.966113  ]\n",
      "   [  1.44903     -3.9334073   -3.2366369    1.4087826 ]\n",
      "   [  1.3023354   -4.519708    -3.6945662    1.2822565 ]]\n",
      "\n",
      "  [[ -1.29121      2.8940606    1.9502285   -1.3604355 ]\n",
      "   [  0.00294916  -1.2568096   -0.89668256  -0.00931877]\n",
      "   [  0.7530518   -2.5598295   -1.9020394    0.7952453 ]\n",
      "   [  1.7065613   -5.794166    -4.5830455    1.7274754 ]\n",
      "   [  1.828848    -7.046152    -5.713792     1.8388789 ]\n",
      "   [  2.3361118   -9.356349    -7.713717     2.356966  ]\n",
      "   [  2.4712012   -7.777283    -6.1303487    2.5265574 ]\n",
      "   [  2.178688    -7.1766615   -5.927077     2.238128  ]\n",
      "   [  2.4477205   -5.1921487   -3.4574718    2.464835  ]\n",
      "   [  2.341771    -5.8448753   -4.0209694    2.4139361 ]\n",
      "   [  1.574666    -4.566143    -4.1088657    1.5959097 ]\n",
      "   [  1.2687012   -5.661861    -4.5558453    1.2785684 ]]\n",
      "\n",
      "  [[ -0.65288585   3.4191878    2.6229937   -0.6571499 ]\n",
      "   [  0.72896236  -2.1471467   -1.9732635    0.77281165]\n",
      "   [  0.32350188  -1.2035745   -1.7607827    0.34396917]\n",
      "   [  1.0276341    1.058868     1.2416905    1.0519707 ]\n",
      "   [  1.6617185   -1.5399216   -1.5241555    1.7414258 ]\n",
      "   [  2.3202841   -3.4387836   -2.1360145    2.3670495 ]\n",
      "   [  2.2568145   -5.2002926   -3.3399537    2.3179612 ]\n",
      "   [  2.10796     -4.739115    -2.9632866    2.1519184 ]\n",
      "   [  1.2009804   -1.6315452   -1.1646751    1.2423136 ]\n",
      "   [  1.612212    -4.7317686   -3.3088357    1.6307019 ]\n",
      "   [  0.8388549   -2.1863418   -1.2897081    0.847988  ]\n",
      "   [  1.036908    -3.0587974   -2.584841     1.0773487 ]]\n",
      "\n",
      "  [[ -0.13920632   1.920276     1.8218281   -0.1018336 ]\n",
      "   [  0.19900118  -1.852921    -1.364047     0.17529728]\n",
      "   [  0.9201308   -2.3837185   -1.2430844    0.8900646 ]\n",
      "   [  1.0384122   -2.2614129   -0.6760908    1.014745  ]\n",
      "   [  1.3033148   -4.046018    -2.5787446    1.2661915 ]\n",
      "   [  1.1603479   -2.2507179   -1.0619235    1.1338618 ]\n",
      "   [  1.041217    -4.2461724   -2.9057727    1.0158387 ]\n",
      "   [  1.0148015   -1.7258983   -0.37147498   1.0063083 ]\n",
      "   [  0.88598007  -2.206053    -0.84811103   0.8885484 ]\n",
      "   [  1.2307554   -3.4753895   -2.1146924    1.1983802 ]\n",
      "   [  0.5709063   -1.6979381   -1.0712341    0.5441556 ]\n",
      "   [  0.938992    -2.1750786   -1.6815557    0.93939686]]\n",
      "\n",
      "  [[ -1.5968649    5.382058     4.5500836   -1.5562288 ]\n",
      "   [  0.86753845  -4.5697775   -4.1758018    0.8370436 ]\n",
      "   [  1.9240823   -9.450348    -8.618066     1.8469068 ]\n",
      "   [  1.4563193   -6.307143    -5.9952025    1.4192016 ]\n",
      "   [  1.9546863   -5.530293    -5.110774     1.9280047 ]\n",
      "   [  1.7568917   -6.1661077   -5.8179903    1.6855166 ]\n",
      "   [  3.4059234  -12.176341   -11.224475     3.3215392 ]\n",
      "   [  1.7747686   -8.078717    -7.356744     1.7220855 ]\n",
      "   [  2.41111     -7.0833406   -6.4738836    2.3771992 ]\n",
      "   [  2.242727    -8.106706    -7.0536833    2.1916888 ]\n",
      "   [  0.5262304   -3.2455208   -2.7358432    0.4870345 ]\n",
      "   [  0.8685509   -1.3043313   -0.5787912    0.8402956 ]]]], shape=(1, 8, 12, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 12, 4)\n",
      "output.shape = (1, 8, 12, 32)\n",
      "scaled_attention.shape= (1, 12, 8, 32)\n",
      "concat_attention.shape= (1, 12, 256)\n",
      "outputs.shape= (1, 12, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 13, 256)\n",
      "(1, 13, 256)\n",
      "(1, 13, 256)\n",
      "split_heads()\n",
      "(1, 13, 256)\n",
      "(1, 13, 8, 32)\n",
      "split_heads()\n",
      "(1, 13, 256)\n",
      "(1, 13, 8, 32)\n",
      "split_heads()\n",
      "(1, 13, 256)\n",
      "(1, 13, 8, 32)\n",
      "(1, 8, 13, 32)\n",
      "(1, 8, 13, 32)\n",
      "(1, 8, 13, 32)\n",
      "matmul_qk.shape = (1, 8, 13, 13)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    0.34484416   0.3515555  ...   0.7453384\n",
      "      1.336325     0.9456161 ]\n",
      "   [ -1.5524524    0.57553977   1.342203   ...   1.7110962\n",
      "      3.572267     2.3425648 ]\n",
      "   [  0.17656448  -2.1603198   -3.1747456  ...  -3.3131418\n",
      "     -2.024164    -4.0464153 ]\n",
      "   ...\n",
      "   [ -0.73776734   1.8399746    0.59010684 ...   2.8597143\n",
      "      2.0337913    1.4952028 ]\n",
      "   [  0.09002788   1.109401    -4.6057897  ...   3.1739016\n",
      "     -1.6673135   -3.7334933 ]\n",
      "   [  0.34657544   0.18045545   0.53776133 ...   4.838447\n",
      "      3.5885258    4.162922  ]]\n",
      "\n",
      "  [[ -0.1123142    0.09086403  -0.2228786  ...   0.24836148\n",
      "      0.3236551    0.57024544]\n",
      "   [ -0.27778682  -7.685327    -0.20150177 ...  -1.8037955\n",
      "      4.0160594    3.3938026 ]\n",
      "   [  0.30459622  -1.6710656   -3.094567   ...   1.7720222\n",
      "      1.0634472    0.6632749 ]\n",
      "   ...\n",
      "   [  0.2823794   -0.01970736   1.9152175  ...  -1.3401513\n",
      "      1.7858346    0.6290318 ]\n",
      "   [  0.665571    -0.15358742  -2.876155   ...   2.8704362\n",
      "      2.6577487   -0.64374214]\n",
      "   [ -0.26803553   0.828541     0.58118385 ...   0.1599716\n",
      "      6.6018744    1.5224228 ]]\n",
      "\n",
      "  [[ -0.10918517  -0.21059825   0.3513798  ...   0.6242574\n",
      "      0.13861714  -0.02299009]\n",
      "   [  0.2860052   -5.063574    -2.1691     ...  -0.38849393\n",
      "      0.5963741   -2.6556163 ]\n",
      "   [  0.6062982    2.719351    -4.9581633  ...  -0.9979213\n",
      "     -2.2543988    0.7751893 ]\n",
      "   ...\n",
      "   [  0.04607371  -2.411195    -0.708032   ...  -2.59234\n",
      "      1.8564506   -1.8926599 ]\n",
      "   [ -0.41917393   3.2796376    2.2793686  ...  -1.6428612\n",
      "      0.2138124    0.8842392 ]\n",
      "   [ -0.18127608  -1.6091987    1.6848273  ...   3.6075828\n",
      "      6.1303234   -2.4234788 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ -0.00999826  -0.5171405    0.3842402  ...  -0.16126524\n",
      "     -0.11150979   0.00809446]\n",
      "   [  0.3875343    2.307618    -2.0652568  ...   4.2148504\n",
      "      2.116757    -1.4210955 ]\n",
      "   [  0.5494998    0.9012554   -5.717646   ...   1.2549968\n",
      "      2.7565124   -2.5381784 ]\n",
      "   ...\n",
      "   [  1.141735    -1.7643471   -3.4391265  ...  -7.662214\n",
      "     -3.9245918   -3.5348408 ]\n",
      "   [ -0.61578333  -0.81781656   1.0908073  ...  -1.118294\n",
      "     -4.1658053    1.8191723 ]\n",
      "   [  0.4221106   -0.0660239   -1.7760104  ...   2.1242397\n",
      "     -1.6004187   -1.519908  ]]\n",
      "\n",
      "  [[ -0.20863976   0.0058533    0.141982   ...   0.7320269\n",
      "      0.6067154    0.22268027]\n",
      "   [ -0.5011563   -5.83805      3.0883057  ...   1.0007838\n",
      "      0.7504345   -3.6622307 ]\n",
      "   [ -0.08541125   2.1106565    0.02025323 ...   1.0689429\n",
      "      3.5151935    1.3210721 ]\n",
      "   ...\n",
      "   [  1.678265    -0.7698982   -3.5583017  ...  -4.9778514\n",
      "     -6.6539764   -0.24892233]\n",
      "   [  2.8199084   -4.274163    -4.867208   ...  -5.7247386\n",
      "    -10.785435    -3.5077507 ]\n",
      "   [  0.7425708    0.3618796   -1.2513036  ...   0.9216355\n",
      "     -4.8839936    1.2826715 ]]\n",
      "\n",
      "  [[ -0.15726982   0.34343007  -0.0106815  ...  -0.01431696\n",
      "      0.10441692   0.22036563]\n",
      "   [  0.42748785  -3.541775    -4.416529   ...   1.7149286\n",
      "      1.8963295    2.315991  ]\n",
      "   [  0.09384713   3.2008502   -4.3988295  ...  -1.168847\n",
      "     -0.9526548   -0.2447487 ]\n",
      "   ...\n",
      "   [ -0.04158778  -2.3729644   -3.8405085  ...  -5.2292576\n",
      "      0.38647527  -0.37548274]\n",
      "   [  0.7140645   -2.3547983   -0.05257716 ...   3.4063368\n",
      "      3.2287633    4.156477  ]\n",
      "   [ -0.3709949   -1.8810337    4.8812547  ...   8.825337\n",
      "      4.385398     0.9184051 ]]]], shape=(1, 8, 13, 13), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 13, 13)\n",
      "output.shape = (1, 8, 13, 32)\n",
      "scaled_attention.shape= (1, 13, 8, 32)\n",
      "concat_attention.shape= (1, 13, 256)\n",
      "outputs.shape= (1, 13, 256)\n",
      "(1, 13, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 13, 256)\n",
      "(1, 13, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 13, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 13, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.6510173   -2.8665452   -1.9706712    0.6258339 ]\n",
      "   [ -1.2761326   -0.6591419   -0.772127    -1.2929033 ]\n",
      "   [  1.0074586   -3.2963974   -3.106445     0.9872349 ]\n",
      "   [  1.2579683   -4.881622    -3.767476     1.2469316 ]\n",
      "   [  1.3234984   -5.9344664   -5.2370906    1.2870476 ]\n",
      "   [  1.7000594   -3.6593008   -2.971424     1.7176253 ]\n",
      "   [  1.2776281   -1.9751713   -1.519901     1.2974825 ]\n",
      "   [  0.58336854  -1.1657993   -0.539427     0.5778812 ]\n",
      "   [  0.7600763   -0.7146235    0.06742804   0.76477766]\n",
      "   [  0.68825406   0.40578374   1.1206802    0.7197399 ]\n",
      "   [  1.5945777   -1.6418726   -0.5148084    1.594585  ]\n",
      "   [  1.7847351   -0.36713108   0.68376136   1.8038828 ]\n",
      "   [  1.5564973    0.14751111   0.38390478   1.6049845 ]]\n",
      "\n",
      "  [[  1.5699102   -4.8338065   -2.8935153    1.5974954 ]\n",
      "   [  1.602976    -6.0585413   -4.3626466    1.6033887 ]\n",
      "   [  0.84273714  -3.3321726   -3.019569     0.8300638 ]\n",
      "   [  1.739568    -5.32123     -4.1527953    1.7057066 ]\n",
      "   [  1.9442569   -4.8756948   -3.3264403    1.9580569 ]\n",
      "   [  3.9294844  -11.821626    -9.099886     3.8940978 ]\n",
      "   [  2.184128    -5.056767    -3.4162076    2.1828146 ]\n",
      "   [  1.9612429   -4.841177    -3.8993757    1.9119158 ]\n",
      "   [  2.1313474   -8.619019    -7.0290985    2.1060083 ]\n",
      "   [  1.7264353   -4.697804    -4.539785     1.657041  ]\n",
      "   [  2.3086565   -7.632138    -5.643827     2.2703083 ]\n",
      "   [  3.4966753   -9.881069    -7.4213333    3.445355  ]\n",
      "   [  2.082511    -6.267626    -4.8032327    2.0366788 ]]\n",
      "\n",
      "  [[ -0.9479617   -1.5584124   -2.3440182   -0.91537315]\n",
      "   [  1.1684959   -6.083402    -5.361126     1.1414931 ]\n",
      "   [  0.8458585   -8.91149     -7.7787905    0.78298134]\n",
      "   [  2.5803494   -5.880795    -5.0921974    2.5411623 ]\n",
      "   [  3.3461082  -11.678779   -10.798813     3.272511  ]\n",
      "   [  3.3453295   -7.0702214   -5.903679     3.2953756 ]\n",
      "   [  1.6342559   -4.6711273   -4.3739734    1.6045712 ]\n",
      "   [  1.7710527   -4.211759    -3.7379825    1.7677531 ]\n",
      "   [  2.3430088   -5.5874043   -4.7693295    2.3498065 ]\n",
      "   [  1.7600528   -4.8194313   -3.658041     1.7406274 ]\n",
      "   [  1.8383347   -4.2881994   -3.964903     1.8433948 ]\n",
      "   [  3.1041694   -6.2948246   -5.899711     3.0869215 ]\n",
      "   [  2.1902268   -8.600747    -8.473362     2.176129  ]]\n",
      "\n",
      "  [[ -0.53032047  -2.1487055   -3.3869808   -0.5465881 ]\n",
      "   [ -0.37673792   0.5297353    0.7943683   -0.41485333]\n",
      "   [  1.2794939   -4.2864437   -4.2701783    1.2569122 ]\n",
      "   [  2.427947    -1.600872    -0.90655017   2.481265  ]\n",
      "   [  2.3424263   -7.2077713   -5.824986     2.335523  ]\n",
      "   [  1.7222499   -2.7512228   -2.3087296    1.7864217 ]\n",
      "   [  1.0004832   -7.1413507   -6.550554     0.95411247]\n",
      "   [  1.4302231   -3.8393989   -3.2885814    1.4499868 ]\n",
      "   [  1.734392    -5.0363717   -4.562415     1.7265733 ]\n",
      "   [  1.7437402   -4.61876     -3.9143934    1.71687   ]\n",
      "   [  1.1339672   -3.6319485   -2.8497448    1.1018116 ]\n",
      "   [  1.510794    -1.2551379   -0.05228771   1.5522768 ]\n",
      "   [  1.0197178   -1.2675874   -0.6745764    1.067578  ]]\n",
      "\n",
      "  [[ -1.8786137    3.8044016    1.7851309   -1.9308672 ]\n",
      "   [ -2.0427704    1.9088362    0.8611902   -2.10597   ]\n",
      "   [  0.3713629   -6.5012736   -5.5806484    0.3149101 ]\n",
      "   [  2.3904674   -0.8018907    0.54941446   2.4430196 ]\n",
      "   [  3.2833803   -5.793328    -3.5530388    3.3556385 ]\n",
      "   [  4.031575    -8.23709     -5.352422     4.1288157 ]\n",
      "   [  1.704372    -4.0030203   -2.8744233    1.746541  ]\n",
      "   [  0.55620587  -0.81578505  -0.7211592    0.5843559 ]\n",
      "   [  1.4359853   -5.1196675   -3.4192846    1.4993091 ]\n",
      "   [  3.066259    -8.724568    -6.3771715    3.138459  ]\n",
      "   [  3.077098    -5.4629254   -3.248803     3.1306965 ]\n",
      "   [  4.6866465   -9.521488    -5.9786706    4.7835746 ]\n",
      "   [  3.749898    -6.577472    -4.5180645    3.8417404 ]]\n",
      "\n",
      "  [[  1.8587611   -2.3004422   -1.999603     1.8540798 ]\n",
      "   [  0.62134135  -1.1685245   -1.0336286    0.63556284]\n",
      "   [  0.93438053  -3.9921112   -3.1974504    0.91454566]\n",
      "   [  2.386284    -8.792911    -6.874719     2.3729997 ]\n",
      "   [  2.8132436  -11.001211    -8.411129     2.7784264 ]\n",
      "   [  2.7941167   -3.5710614   -2.4098222    2.8085887 ]\n",
      "   [  2.5607827   -5.1447167   -4.0192566    2.5128086 ]\n",
      "   [  1.9819213   -4.7589607   -4.3256965    1.9169271 ]\n",
      "   [  2.1862228   -3.8017366   -2.162978     2.196947  ]\n",
      "   [  2.025622    -0.50651217  -0.09534571   2.0341551 ]\n",
      "   [  1.7751181   -5.396942    -3.6041234    1.7526699 ]\n",
      "   [  1.8905165   -2.984126    -2.7361436    1.8944441 ]\n",
      "   [  2.19154     -4.3217793   -3.247157     2.1910028 ]]\n",
      "\n",
      "  [[  0.263768     1.3027129    0.7912128    0.31364933]\n",
      "   [  0.90733486  -1.2863759   -0.23643312   0.92466456]\n",
      "   [  1.9193958   -5.6561565   -3.624307     1.9176353 ]\n",
      "   [  2.590705    -7.7614427   -5.438415     2.6344485 ]\n",
      "   [  2.0141904   -6.001995    -3.7005918    2.02994   ]\n",
      "   [  2.3413384   -2.5982022   -0.645182     2.369621  ]\n",
      "   [  1.1849254   -0.41514406   0.7534283    1.1835097 ]\n",
      "   [  1.6196365   -2.736624    -1.3294117    1.6487104 ]\n",
      "   [  1.8510432   -2.5615194   -0.7961973    1.8711935 ]\n",
      "   [  2.0705955   -4.3488426   -2.0247169    2.1132114 ]\n",
      "   [  1.9942156   -2.1047823   -0.90358776   2.0466616 ]\n",
      "   [  0.49939027  -1.217362    -0.46714777   0.52691543]\n",
      "   [  1.1221981   -0.761073     0.50912005   1.1667402 ]]\n",
      "\n",
      "  [[ -0.50981736   4.1422954    3.6196024   -0.43590528]\n",
      "   [  0.4603232   -1.0371166   -0.40937394   0.4394898 ]\n",
      "   [  1.2603666   -3.7127948   -2.6582818    1.2983314 ]\n",
      "   [  1.9421246   -6.2256336   -3.6375484    1.9323288 ]\n",
      "   [  1.3231326   -4.248828    -3.1260052    1.3073777 ]\n",
      "   [  2.442132    -7.4952087   -5.2700887    2.4572377 ]\n",
      "   [  2.4187922   -5.27305     -2.1958756    2.4965396 ]\n",
      "   [  1.0935532   -3.028773    -1.9892881    1.1174632 ]\n",
      "   [  1.07857     -1.6685917   -0.5144004    1.1236442 ]\n",
      "   [  1.9439204   -4.8428583   -2.7191334    2.0016615 ]\n",
      "   [  1.3960849   -4.567839    -2.928855     1.4403856 ]\n",
      "   [  1.6036419   -5.08729     -3.1608024    1.6399496 ]\n",
      "   [  2.1432507   -5.3504367   -3.4768052    2.1538277 ]]]], shape=(1, 8, 13, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 13, 4)\n",
      "output.shape = (1, 8, 13, 32)\n",
      "scaled_attention.shape= (1, 13, 8, 32)\n",
      "concat_attention.shape= (1, 13, 256)\n",
      "outputs.shape= (1, 13, 256)\n",
      "(1, 13, 256)\n",
      "(1, 13, 256)\n",
      "(1, 13, 256)\n",
      "split_heads()\n",
      "(1, 13, 256)\n",
      "(1, 13, 8, 32)\n",
      "split_heads()\n",
      "(1, 13, 256)\n",
      "(1, 13, 8, 32)\n",
      "split_heads()\n",
      "(1, 13, 256)\n",
      "(1, 13, 8, 32)\n",
      "(1, 8, 13, 32)\n",
      "(1, 8, 13, 32)\n",
      "(1, 8, 13, 32)\n",
      "matmul_qk.shape = (1, 8, 13, 13)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181 -0.6147039  ...  0.65330005  0.3939021\n",
      "     1.0203538 ]\n",
      "   [ 3.96568    -0.17723085  1.5590783  ... -2.0952516  -0.88445807\n",
      "     0.33617422]\n",
      "   [ 3.9833748  -2.2539997   0.15140963 ... -1.6135567   0.05757904\n",
      "    -3.7684631 ]\n",
      "   ...\n",
      "   [ 1.5408126   0.0754276   0.5586133  ... -2.193409   -2.6331093\n",
      "    -3.1094139 ]\n",
      "   [ 4.2881947   1.0258555   2.2919047  ... -2.8535032  -4.5004025\n",
      "    -4.499259  ]\n",
      "   [ 3.9610004  -1.3537385   1.7564551  ... -2.8989072  -3.6048942\n",
      "    -5.123631  ]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489  -0.22415894 ... -0.12138899 -0.79827315\n",
      "    -0.34874386]\n",
      "   [ 0.22428446 -2.402899   -1.7699208  ... -1.0353341  -2.03192\n",
      "    -0.4989716 ]\n",
      "   [-0.6712342   0.30168235 -3.0742364  ... -3.8419588  -1.2976645\n",
      "     0.28522968]\n",
      "   ...\n",
      "   [ 1.0095067   1.1085582   0.55792695 ... -0.23042643 -3.101134\n",
      "    -0.5643825 ]\n",
      "   [ 4.978718    3.8684566   3.078466   ...  0.18915287 -4.669372\n",
      "    -4.0727487 ]\n",
      "   [ 1.1230094   3.0626888   2.791873   ... -0.8169808  -1.7717091\n",
      "     1.9098277 ]]\n",
      "\n",
      "  [[-3.0174336   0.41961712  1.9252456  ...  3.3224475   3.3229702\n",
      "     2.1282005 ]\n",
      "   [-1.3549727  -0.09280814  2.5851169  ...  1.4881623  -0.4787041\n",
      "    -0.16480741]\n",
      "   [-1.7961591  -1.8925554  -0.41655225 ...  1.8832321  -0.76940006\n",
      "     2.3132296 ]\n",
      "   ...\n",
      "   [ 1.5586158  -1.5779564  -2.6995533  ... -0.5365632  -0.945924\n",
      "    -1.3739336 ]\n",
      "   [ 0.8576953  -3.6549006  -5.4420285  ...  1.936718    4.0205626\n",
      "    -0.1758729 ]\n",
      "   [ 0.6897928  -3.7852154  -2.1934628  ...  2.6731453   4.2492924\n",
      "     1.8425139 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.2599558   1.0058122   0.9403136  ... -0.44247153 -1.5453403\n",
      "    -0.08242535]\n",
      "   [ 1.9198732  -0.84344107  2.0494642  ... -0.02316735  0.35112238\n",
      "    -5.1893573 ]\n",
      "   [ 2.8428354  -1.6724693  -1.4574461  ... -0.6885932  -0.15051317\n",
      "    -1.5127541 ]\n",
      "   ...\n",
      "   [ 1.3534318  -2.1399765   0.44380033 ... -1.4390806  -0.27244264\n",
      "    -2.065072  ]\n",
      "   [ 0.84742606  0.0772418   1.5468452  ...  0.02474112 -1.8008659\n",
      "    -2.2035553 ]\n",
      "   [ 2.533928   -2.0408306   3.9096775  ...  1.0483642  -0.3570001\n",
      "    -4.475288  ]]\n",
      "\n",
      "  [[-2.1587913   1.7147038   1.544837   ... -0.5201634   0.18126747\n",
      "     2.5825894 ]\n",
      "   [-0.00582398 -2.3462925   1.799922   ... -0.61179817 -2.3670635\n",
      "    -1.1330837 ]\n",
      "   [-2.1043587  -2.8178024   2.2148285  ...  3.6382704   2.1391535\n",
      "     4.9675913 ]\n",
      "   ...\n",
      "   [-0.3825621  -2.0746894  -1.9134712  ...  0.85646003  1.4463311\n",
      "     0.9687261 ]\n",
      "   [ 1.7960831   2.5962052  -1.3429192  ... -5.2735734  -4.4235926\n",
      "    -3.333553  ]\n",
      "   [ 0.83172405  1.051401   -1.6721448  ...  0.3204316   0.18235834\n",
      "     1.3213887 ]]\n",
      "\n",
      "  [[-2.0107505   2.0400176   1.5965837  ...  2.6378512   2.0324154\n",
      "     0.5348753 ]\n",
      "   [-1.2814682  -2.6879327   1.4795719  ...  3.445116    2.5140755\n",
      "     1.8072864 ]\n",
      "   [ 0.3697977   0.11362538 -0.8682849  ... -2.19526     0.45149922\n",
      "    -0.7533272 ]\n",
      "   ...\n",
      "   [-0.03567908 -1.4176497  -1.6316394  ... -2.087145   -0.23750614\n",
      "     2.3246417 ]\n",
      "   [ 0.44519848 -4.1498594  -0.6811251  ...  2.9410086  -0.93907857\n",
      "    -0.5211465 ]\n",
      "   [ 0.9062684  -4.861451   -0.665113   ...  3.8485882   0.5005594\n",
      "    -0.97615194]]]], shape=(1, 8, 13, 13), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 13, 13)\n",
      "output.shape = (1, 8, 13, 32)\n",
      "scaled_attention.shape= (1, 13, 8, 32)\n",
      "concat_attention.shape= (1, 13, 256)\n",
      "outputs.shape= (1, 13, 256)\n",
      "(1, 13, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 13, 256)\n",
      "(1, 13, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 13, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 13, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.5176607    1.9687198    2.0989769   -0.47190297]\n",
      "   [ -0.49124843   0.08534324   0.2991765   -0.48528743]\n",
      "   [  0.24215932  -4.697081    -4.019181     0.1927161 ]\n",
      "   [  0.7396501   -3.0292149   -2.48159      0.7256521 ]\n",
      "   [  0.7377838   -1.2575825   -1.0456651    0.7396505 ]\n",
      "   [  0.12743154  -2.1013763   -1.7714081    0.15186152]\n",
      "   [  0.57692707  -1.6948001   -1.6039941    0.56792045]\n",
      "   [ -0.0456275   -3.2555232   -2.8839726   -0.07279108]\n",
      "   [  0.541922    -4.2346587   -4.088871     0.5205673 ]\n",
      "   [  0.43123668  -4.4550576   -4.342825     0.38334304]\n",
      "   [  0.2683728   -2.6908388   -3.0562704    0.25416884]\n",
      "   [  0.65151817  -2.3087127   -2.7818563    0.6532232 ]\n",
      "   [  0.7016252   -3.254609    -3.4820383    0.7049014 ]]\n",
      "\n",
      "  [[ -0.5409756    2.8193953    2.0520196   -0.5236139 ]\n",
      "   [ -1.0098997    4.226584     3.5640724   -0.9722929 ]\n",
      "   [ -0.31559435  -2.7033126   -2.1731372   -0.34413084]\n",
      "   [  0.67385     -4.3054934   -2.5939085    0.6549963 ]\n",
      "   [  0.7716512   -4.6774607   -3.4361272    0.73228556]\n",
      "   [  0.30699015  -2.0128524   -1.77724      0.3016845 ]\n",
      "   [  0.95172566  -5.123536    -3.7028008    0.9193312 ]\n",
      "   [  0.1625587   -2.8245268   -2.2594504    0.1333035 ]\n",
      "   [  0.80569774  -2.9156864   -1.9677489    0.81221676]\n",
      "   [  0.5848501   -4.6709986   -3.4369607    0.5637138 ]\n",
      "   [  0.8923553   -4.0675893   -2.657831     0.87549967]\n",
      "   [  1.2629194   -3.7040567   -2.924148     1.2593203 ]\n",
      "   [  1.5493715   -7.2488613   -5.396356     1.5176687 ]]\n",
      "\n",
      "  [[ -0.42360795   3.4683292    3.2733347   -0.39263082]\n",
      "   [ -0.47245097   0.7355631    1.3526613   -0.47366753]\n",
      "   [  0.8764782   -5.054033    -4.6074905    0.832105  ]\n",
      "   [  1.5158468   -5.5036316   -5.460711     1.4665604 ]\n",
      "   [  1.3198522   -4.7758727   -4.838473     1.2815847 ]\n",
      "   [  0.6756115   -5.767558    -5.86507      0.60872716]\n",
      "   [  0.6050416   -6.19143     -6.4390326    0.54084617]\n",
      "   [  0.44500238  -0.68225163  -0.9241052    0.42594016]\n",
      "   [  1.465985    -3.2105703   -2.976426     1.4414983 ]\n",
      "   [  1.1924549   -4.355453    -4.300772     1.1541699 ]\n",
      "   [  1.3496063   -1.4938401   -1.1592588    1.3560332 ]\n",
      "   [  0.79303217  -2.173227    -1.8807111    0.78589594]\n",
      "   [  2.1267772   -3.1087718   -3.386968     2.1233454 ]]\n",
      "\n",
      "  [[ -1.6453142    4.41468      3.7185764   -1.6388824 ]\n",
      "   [  0.1504475   -1.3770971   -0.84712774   0.10965424]\n",
      "   [  1.5174129   -5.9414515   -4.9204936    1.4784044 ]\n",
      "   [  1.9338356   -4.261786    -3.374979     1.932654  ]\n",
      "   [  1.5312709   -4.5473847   -3.4884474    1.480693  ]\n",
      "   [  2.4686995   -5.566623    -4.674686     2.4672065 ]\n",
      "   [  2.0171244   -5.156834    -4.0178304    1.9925612 ]\n",
      "   [  1.4369041   -3.667318    -2.7229993    1.4106048 ]\n",
      "   [  1.7216051   -4.4552937   -3.1465628    1.6921057 ]\n",
      "   [  1.9970524   -6.690838    -5.398477     1.966113  ]\n",
      "   [  1.44903     -3.9334073   -3.2366369    1.4087826 ]\n",
      "   [  1.3023354   -4.519708    -3.6945662    1.2822565 ]\n",
      "   [  1.6942405   -3.9762888   -3.1610596    1.6870621 ]]\n",
      "\n",
      "  [[ -1.29121      2.8940606    1.9502285   -1.3604355 ]\n",
      "   [  0.00294916  -1.2568096   -0.89668256  -0.00931877]\n",
      "   [  0.7530518   -2.5598295   -1.9020394    0.7952453 ]\n",
      "   [  1.7065613   -5.794166    -4.5830455    1.7274754 ]\n",
      "   [  1.828848    -7.046152    -5.713792     1.8388789 ]\n",
      "   [  2.3361118   -9.356349    -7.713717     2.356966  ]\n",
      "   [  2.4712012   -7.777283    -6.1303487    2.5265574 ]\n",
      "   [  2.178688    -7.1766615   -5.927077     2.238128  ]\n",
      "   [  2.4477205   -5.1921487   -3.4574718    2.464835  ]\n",
      "   [  2.341771    -5.8448753   -4.0209694    2.4139361 ]\n",
      "   [  1.574666    -4.566143    -4.1088657    1.5959097 ]\n",
      "   [  1.2687012   -5.661861    -4.5558453    1.2785684 ]\n",
      "   [  2.2869496   -8.59737     -7.4518332    2.2985146 ]]\n",
      "\n",
      "  [[ -0.65288585   3.4191878    2.6229937   -0.6571499 ]\n",
      "   [  0.72896236  -2.1471467   -1.9732635    0.77281165]\n",
      "   [  0.32350188  -1.2035745   -1.7607827    0.34396917]\n",
      "   [  1.0276341    1.058868     1.2416905    1.0519707 ]\n",
      "   [  1.6617185   -1.5399216   -1.5241555    1.7414258 ]\n",
      "   [  2.3202841   -3.4387836   -2.1360145    2.3670495 ]\n",
      "   [  2.2568145   -5.2002926   -3.3399537    2.3179612 ]\n",
      "   [  2.10796     -4.739115    -2.9632866    2.1519184 ]\n",
      "   [  1.2009804   -1.6315452   -1.1646751    1.2423136 ]\n",
      "   [  1.612212    -4.7317686   -3.3088357    1.6307019 ]\n",
      "   [  0.8388549   -2.1863418   -1.2897081    0.847988  ]\n",
      "   [  1.036908    -3.0587974   -2.584841     1.0773487 ]\n",
      "   [  1.8100622   -3.9532287   -3.6608858    1.8912151 ]]\n",
      "\n",
      "  [[ -0.13920632   1.920276     1.8218281   -0.1018336 ]\n",
      "   [  0.19900118  -1.852921    -1.364047     0.17529728]\n",
      "   [  0.9201308   -2.3837185   -1.2430844    0.8900646 ]\n",
      "   [  1.0384122   -2.2614129   -0.6760908    1.014745  ]\n",
      "   [  1.3033148   -4.046018    -2.5787446    1.2661915 ]\n",
      "   [  1.1603479   -2.2507179   -1.0619235    1.1338618 ]\n",
      "   [  1.041217    -4.2461724   -2.9057727    1.0158387 ]\n",
      "   [  1.0148015   -1.7258983   -0.37147498   1.0063083 ]\n",
      "   [  0.88598007  -2.206053    -0.84811103   0.8885484 ]\n",
      "   [  1.2307554   -3.4753895   -2.1146924    1.1983802 ]\n",
      "   [  0.5709063   -1.6979381   -1.0712341    0.5441556 ]\n",
      "   [  0.938992    -2.1750786   -1.6815557    0.93939686]\n",
      "   [  0.90540814  -3.5624843   -3.3321912    0.87981683]]\n",
      "\n",
      "  [[ -1.5968649    5.382058     4.5500836   -1.5562288 ]\n",
      "   [  0.86753845  -4.5697775   -4.1758018    0.8370436 ]\n",
      "   [  1.9240823   -9.450348    -8.618066     1.8469068 ]\n",
      "   [  1.4563193   -6.307143    -5.9952025    1.4192016 ]\n",
      "   [  1.9546863   -5.530293    -5.110774     1.9280047 ]\n",
      "   [  1.7568917   -6.1661077   -5.8179903    1.6855166 ]\n",
      "   [  3.4059234  -12.176341   -11.224475     3.3215392 ]\n",
      "   [  1.7747686   -8.078717    -7.356744     1.7220855 ]\n",
      "   [  2.41111     -7.0833406   -6.4738836    2.3771992 ]\n",
      "   [  2.242727    -8.106706    -7.0536833    2.1916888 ]\n",
      "   [  0.5262304   -3.2455208   -2.7358432    0.4870345 ]\n",
      "   [  0.8685509   -1.3043313   -0.5787912    0.8402956 ]\n",
      "   [  2.9131413   -6.756786    -5.5475316    2.8893347 ]]]], shape=(1, 8, 13, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 13, 4)\n",
      "output.shape = (1, 8, 13, 32)\n",
      "scaled_attention.shape= (1, 13, 8, 32)\n",
      "concat_attention.shape= (1, 13, 256)\n",
      "outputs.shape= (1, 13, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 14, 256)\n",
      "(1, 14, 256)\n",
      "(1, 14, 256)\n",
      "split_heads()\n",
      "(1, 14, 256)\n",
      "(1, 14, 8, 32)\n",
      "split_heads()\n",
      "(1, 14, 256)\n",
      "(1, 14, 8, 32)\n",
      "split_heads()\n",
      "(1, 14, 256)\n",
      "(1, 14, 8, 32)\n",
      "(1, 8, 14, 32)\n",
      "(1, 8, 14, 32)\n",
      "(1, 8, 14, 32)\n",
      "matmul_qk.shape = (1, 8, 14, 14)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    0.34484416   0.3515555  ...   1.336325\n",
      "      0.9456161    0.6615005 ]\n",
      "   [ -1.5524524    0.57553977   1.342203   ...   3.572267\n",
      "      2.3425648    3.5891137 ]\n",
      "   [  0.17656448  -2.1603198   -3.1747456  ...  -2.024164\n",
      "     -4.0464153    2.018118  ]\n",
      "   ...\n",
      "   [  0.09002788   1.109401    -4.6057897  ...  -1.6673135\n",
      "     -3.7334933    5.074838  ]\n",
      "   [  0.34657544   0.18045545   0.53776133 ...   3.5885258\n",
      "      4.162922     5.9773464 ]\n",
      "   [ -0.28322992   3.1288378   -0.01700863 ...   5.3286905\n",
      "      3.6691072   -3.5476904 ]]\n",
      "\n",
      "  [[ -0.1123142    0.09086403  -0.2228786  ...   0.3236551\n",
      "      0.57024544   0.08032935]\n",
      "   [ -0.27778682  -7.685327    -0.20150177 ...   4.0160594\n",
      "      3.3938026   -2.3570178 ]\n",
      "   [  0.30459622  -1.6710656   -3.094567   ...   1.0634472\n",
      "      0.6632749   -1.517201  ]\n",
      "   ...\n",
      "   [  0.665571    -0.15358742  -2.876155   ...   2.6577487\n",
      "     -0.64374214   2.0966947 ]\n",
      "   [ -0.26803553   0.828541     0.58118385 ...   6.6018744\n",
      "      1.5224228   -0.038258  ]\n",
      "   [  0.07438865  -3.7665203   -3.0217247  ...   2.054097\n",
      "      2.158518    -3.2898862 ]]\n",
      "\n",
      "  [[ -0.10918517  -0.21059825   0.3513798  ...   0.13861714\n",
      "     -0.02299009   0.6733783 ]\n",
      "   [  0.2860052   -5.063574    -2.1691     ...   0.5963741\n",
      "     -2.6556163   -0.24466045]\n",
      "   [  0.6062982    2.719351    -4.9581633  ...  -2.2543988\n",
      "      0.7751893   -3.8892925 ]\n",
      "   ...\n",
      "   [ -0.41917393   3.2796376    2.2793686  ...   0.2138124\n",
      "      0.8842392    1.5377055 ]\n",
      "   [ -0.18127608  -1.6091987    1.6848273  ...   6.1303234\n",
      "     -2.4234788    3.1261995 ]\n",
      "   [ -0.0538262    0.6051362    1.4847     ...   0.6904416\n",
      "     -0.87357515  -4.348027  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ -0.00999826  -0.5171405    0.3842402  ...  -0.11150979\n",
      "      0.00809446  -0.05267796]\n",
      "   [  0.3875343    2.307618    -2.0652568  ...   2.116757\n",
      "     -1.4210955   -1.0692489 ]\n",
      "   [  0.5494998    0.9012554   -5.717646   ...   2.7565124\n",
      "     -2.5381784   -2.9159274 ]\n",
      "   ...\n",
      "   [ -0.61578333  -0.81781656   1.0908073  ...  -4.1658053\n",
      "      1.8191723   -0.20622014]\n",
      "   [  0.4221106   -0.0660239   -1.7760104  ...  -1.6004187\n",
      "     -1.519908    -1.273347  ]\n",
      "   [  0.33041304  -2.5043929   -1.3398037  ...   1.4896641\n",
      "     -5.2653575   -9.975928  ]]\n",
      "\n",
      "  [[ -0.20863976   0.0058533    0.141982   ...   0.6067154\n",
      "      0.22268027   0.13776878]\n",
      "   [ -0.5011563   -5.83805      3.0883057  ...   0.7504345\n",
      "     -3.6622307    0.4724806 ]\n",
      "   [ -0.08541125   2.1106565    0.02025323 ...   3.5151935\n",
      "      1.3210721    2.305009  ]\n",
      "   ...\n",
      "   [  2.8199084   -4.274163    -4.867208   ... -10.785435\n",
      "     -3.5077507   -9.194747  ]\n",
      "   [  0.7425708    0.3618796   -1.2513036  ...  -4.8839936\n",
      "      1.2826715   -3.4366665 ]\n",
      "   [  1.5437251   -6.752349    -2.278639   ...  -3.6408417\n",
      "     -2.443643    -7.9576335 ]]\n",
      "\n",
      "  [[ -0.15726982   0.34343007  -0.0106815  ...   0.10441692\n",
      "      0.22036563   0.67823213]\n",
      "   [  0.42748785  -3.541775    -4.416529   ...   1.8963295\n",
      "      2.315991     0.66909575]\n",
      "   [  0.09384713   3.2008502   -4.3988295  ...  -0.9526548\n",
      "     -0.2447487   -2.1684322 ]\n",
      "   ...\n",
      "   [  0.7140645   -2.3547983   -0.05257716 ...   3.2287633\n",
      "      4.156477     4.5982914 ]\n",
      "   [ -0.3709949   -1.8810337    4.8812547  ...   4.385398\n",
      "      0.9184051    2.088492  ]\n",
      "   [  0.8418388   -1.6215484   -0.95807385 ...   3.6361263\n",
      "     -1.1528292   -1.4905474 ]]]], shape=(1, 8, 14, 14), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 14, 14)\n",
      "output.shape = (1, 8, 14, 32)\n",
      "scaled_attention.shape= (1, 14, 8, 32)\n",
      "concat_attention.shape= (1, 14, 256)\n",
      "outputs.shape= (1, 14, 256)\n",
      "(1, 14, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 14, 256)\n",
      "(1, 14, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 14, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 14, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.6510173   -2.8665452   -1.9706712    0.6258339 ]\n",
      "   [ -1.2761326   -0.6591419   -0.772127    -1.2929033 ]\n",
      "   [  1.0074586   -3.2963974   -3.106445     0.9872349 ]\n",
      "   [  1.2579683   -4.881622    -3.767476     1.2469316 ]\n",
      "   [  1.3234984   -5.9344664   -5.2370906    1.2870476 ]\n",
      "   [  1.7000594   -3.6593008   -2.971424     1.7176253 ]\n",
      "   [  1.2776281   -1.9751713   -1.519901     1.2974825 ]\n",
      "   [  0.58336854  -1.1657993   -0.539427     0.5778812 ]\n",
      "   [  0.7600763   -0.7146235    0.06742804   0.76477766]\n",
      "   [  0.68825406   0.40578374   1.1206802    0.7197399 ]\n",
      "   [  1.5945777   -1.6418726   -0.5148084    1.594585  ]\n",
      "   [  1.7847351   -0.36713108   0.68376136   1.8038828 ]\n",
      "   [  1.5564973    0.14751111   0.38390478   1.6049845 ]\n",
      "   [  1.6079514   -0.09540926   0.7814428    1.6541259 ]]\n",
      "\n",
      "  [[  1.5699102   -4.8338065   -2.8935153    1.5974954 ]\n",
      "   [  1.602976    -6.0585413   -4.3626466    1.6033887 ]\n",
      "   [  0.84273714  -3.3321726   -3.019569     0.8300638 ]\n",
      "   [  1.739568    -5.32123     -4.1527953    1.7057066 ]\n",
      "   [  1.9442569   -4.8756948   -3.3264403    1.9580569 ]\n",
      "   [  3.9294844  -11.821626    -9.099886     3.8940978 ]\n",
      "   [  2.184128    -5.056767    -3.4162076    2.1828146 ]\n",
      "   [  1.9612429   -4.841177    -3.8993757    1.9119158 ]\n",
      "   [  2.1313474   -8.619019    -7.0290985    2.1060083 ]\n",
      "   [  1.7264353   -4.697804    -4.539785     1.657041  ]\n",
      "   [  2.3086565   -7.632138    -5.643827     2.2703083 ]\n",
      "   [  3.4966753   -9.881069    -7.4213333    3.445355  ]\n",
      "   [  2.082511    -6.267626    -4.8032327    2.0366788 ]\n",
      "   [  2.4420376   -7.92061     -6.2677646    2.3764477 ]]\n",
      "\n",
      "  [[ -0.9479617   -1.5584124   -2.3440182   -0.91537315]\n",
      "   [  1.1684959   -6.083402    -5.361126     1.1414931 ]\n",
      "   [  0.8458585   -8.91149     -7.7787905    0.78298134]\n",
      "   [  2.5803494   -5.880795    -5.0921974    2.5411623 ]\n",
      "   [  3.3461082  -11.678779   -10.798813     3.272511  ]\n",
      "   [  3.3453295   -7.0702214   -5.903679     3.2953756 ]\n",
      "   [  1.6342559   -4.6711273   -4.3739734    1.6045712 ]\n",
      "   [  1.7710527   -4.211759    -3.7379825    1.7677531 ]\n",
      "   [  2.3430088   -5.5874043   -4.7693295    2.3498065 ]\n",
      "   [  1.7600528   -4.8194313   -3.658041     1.7406274 ]\n",
      "   [  1.8383347   -4.2881994   -3.964903     1.8433948 ]\n",
      "   [  3.1041694   -6.2948246   -5.899711     3.0869215 ]\n",
      "   [  2.1902268   -8.600747    -8.473362     2.176129  ]\n",
      "   [  2.3792868   -7.585834    -7.168655     2.3181617 ]]\n",
      "\n",
      "  [[ -0.53032047  -2.1487055   -3.3869808   -0.5465881 ]\n",
      "   [ -0.37673792   0.5297353    0.7943683   -0.41485333]\n",
      "   [  1.2794939   -4.2864437   -4.2701783    1.2569122 ]\n",
      "   [  2.427947    -1.600872    -0.90655017   2.481265  ]\n",
      "   [  2.3424263   -7.2077713   -5.824986     2.335523  ]\n",
      "   [  1.7222499   -2.7512228   -2.3087296    1.7864217 ]\n",
      "   [  1.0004832   -7.1413507   -6.550554     0.95411247]\n",
      "   [  1.4302231   -3.8393989   -3.2885814    1.4499868 ]\n",
      "   [  1.734392    -5.0363717   -4.562415     1.7265733 ]\n",
      "   [  1.7437402   -4.61876     -3.9143934    1.71687   ]\n",
      "   [  1.1339672   -3.6319485   -2.8497448    1.1018116 ]\n",
      "   [  1.510794    -1.2551379   -0.05228771   1.5522768 ]\n",
      "   [  1.0197178   -1.2675874   -0.6745764    1.067578  ]\n",
      "   [  0.9115422   -3.4677386   -2.6383572    0.8930571 ]]\n",
      "\n",
      "  [[ -1.8786137    3.8044016    1.7851309   -1.9308672 ]\n",
      "   [ -2.0427704    1.9088362    0.8611902   -2.10597   ]\n",
      "   [  0.3713629   -6.5012736   -5.5806484    0.3149101 ]\n",
      "   [  2.3904674   -0.8018907    0.54941446   2.4430196 ]\n",
      "   [  3.2833803   -5.793328    -3.5530388    3.3556385 ]\n",
      "   [  4.031575    -8.23709     -5.352422     4.1288157 ]\n",
      "   [  1.704372    -4.0030203   -2.8744233    1.746541  ]\n",
      "   [  0.55620587  -0.81578505  -0.7211592    0.5843559 ]\n",
      "   [  1.4359853   -5.1196675   -3.4192846    1.4993091 ]\n",
      "   [  3.066259    -8.724568    -6.3771715    3.138459  ]\n",
      "   [  3.077098    -5.4629254   -3.248803     3.1306965 ]\n",
      "   [  4.6866465   -9.521488    -5.9786706    4.7835746 ]\n",
      "   [  3.749898    -6.577472    -4.5180645    3.8417404 ]\n",
      "   [  5.4376407   -9.574739    -5.771271     5.5562477 ]]\n",
      "\n",
      "  [[  1.8587611   -2.3004422   -1.999603     1.8540798 ]\n",
      "   [  0.62134135  -1.1685245   -1.0336286    0.63556284]\n",
      "   [  0.93438053  -3.9921112   -3.1974504    0.91454566]\n",
      "   [  2.386284    -8.792911    -6.874719     2.3729997 ]\n",
      "   [  2.8132436  -11.001211    -8.411129     2.7784264 ]\n",
      "   [  2.7941167   -3.5710614   -2.4098222    2.8085887 ]\n",
      "   [  2.5607827   -5.1447167   -4.0192566    2.5128086 ]\n",
      "   [  1.9819213   -4.7589607   -4.3256965    1.9169271 ]\n",
      "   [  2.1862228   -3.8017366   -2.162978     2.196947  ]\n",
      "   [  2.025622    -0.50651217  -0.09534571   2.0341551 ]\n",
      "   [  1.7751181   -5.396942    -3.6041234    1.7526699 ]\n",
      "   [  1.8905165   -2.984126    -2.7361436    1.8944441 ]\n",
      "   [  2.19154     -4.3217793   -3.247157     2.1910028 ]\n",
      "   [  2.6326275   -2.9615233   -1.8003258    2.6587393 ]]\n",
      "\n",
      "  [[  0.263768     1.3027129    0.7912128    0.31364933]\n",
      "   [  0.90733486  -1.2863759   -0.23643312   0.92466456]\n",
      "   [  1.9193958   -5.6561565   -3.624307     1.9176353 ]\n",
      "   [  2.590705    -7.7614427   -5.438415     2.6344485 ]\n",
      "   [  2.0141904   -6.001995    -3.7005918    2.02994   ]\n",
      "   [  2.3413384   -2.5982022   -0.645182     2.369621  ]\n",
      "   [  1.1849254   -0.41514406   0.7534283    1.1835097 ]\n",
      "   [  1.6196365   -2.736624    -1.3294117    1.6487104 ]\n",
      "   [  1.8510432   -2.5615194   -0.7961973    1.8711935 ]\n",
      "   [  2.0705955   -4.3488426   -2.0247169    2.1132114 ]\n",
      "   [  1.9942156   -2.1047823   -0.90358776   2.0466616 ]\n",
      "   [  0.49939027  -1.217362    -0.46714777   0.52691543]\n",
      "   [  1.1221981   -0.761073     0.50912005   1.1667402 ]\n",
      "   [  1.2064869   -1.128277    -0.1816193    1.2461394 ]]\n",
      "\n",
      "  [[ -0.50981736   4.1422954    3.6196024   -0.43590528]\n",
      "   [  0.4603232   -1.0371166   -0.40937394   0.4394898 ]\n",
      "   [  1.2603666   -3.7127948   -2.6582818    1.2983314 ]\n",
      "   [  1.9421246   -6.2256336   -3.6375484    1.9323288 ]\n",
      "   [  1.3231326   -4.248828    -3.1260052    1.3073777 ]\n",
      "   [  2.442132    -7.4952087   -5.2700887    2.4572377 ]\n",
      "   [  2.4187922   -5.27305     -2.1958756    2.4965396 ]\n",
      "   [  1.0935532   -3.028773    -1.9892881    1.1174632 ]\n",
      "   [  1.07857     -1.6685917   -0.5144004    1.1236442 ]\n",
      "   [  1.9439204   -4.8428583   -2.7191334    2.0016615 ]\n",
      "   [  1.3960849   -4.567839    -2.928855     1.4403856 ]\n",
      "   [  1.6036419   -5.08729     -3.1608024    1.6399496 ]\n",
      "   [  2.1432507   -5.3504367   -3.4768052    2.1538277 ]\n",
      "   [  2.0789034   -8.089037    -6.1146235    2.1081893 ]]]], shape=(1, 8, 14, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 14, 4)\n",
      "output.shape = (1, 8, 14, 32)\n",
      "scaled_attention.shape= (1, 14, 8, 32)\n",
      "concat_attention.shape= (1, 14, 256)\n",
      "outputs.shape= (1, 14, 256)\n",
      "(1, 14, 256)\n",
      "(1, 14, 256)\n",
      "(1, 14, 256)\n",
      "split_heads()\n",
      "(1, 14, 256)\n",
      "(1, 14, 8, 32)\n",
      "split_heads()\n",
      "(1, 14, 256)\n",
      "(1, 14, 8, 32)\n",
      "split_heads()\n",
      "(1, 14, 256)\n",
      "(1, 14, 8, 32)\n",
      "(1, 8, 14, 32)\n",
      "(1, 8, 14, 32)\n",
      "(1, 8, 14, 32)\n",
      "matmul_qk.shape = (1, 8, 14, 14)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181 -0.6147039  ...  0.3939021   1.0203538\n",
      "     1.229891  ]\n",
      "   [ 3.96568    -0.17723085  1.5590783  ... -0.88445807  0.33617422\n",
      "    -6.952222  ]\n",
      "   [ 3.9833748  -2.2539997   0.15140963 ...  0.05757904 -3.7684631\n",
      "    -1.7606908 ]\n",
      "   ...\n",
      "   [ 4.2881947   1.0258555   2.2919047  ... -4.5004025  -4.499259\n",
      "    -3.7514205 ]\n",
      "   [ 3.9610004  -1.3537385   1.7564551  ... -3.6048942  -5.123631\n",
      "    -3.2233567 ]\n",
      "   [ 1.616248   -0.41301975  2.1102164  ...  0.8559148  -0.30368668\n",
      "    -3.1939867 ]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489  -0.22415894 ... -0.79827315 -0.34874386\n",
      "    -1.5257807 ]\n",
      "   [ 0.22428446 -2.402899   -1.7699208  ... -2.03192    -0.4989716\n",
      "    -0.9432326 ]\n",
      "   [-0.6712342   0.30168235 -3.0742364  ... -1.2976645   0.28522968\n",
      "    -3.5610971 ]\n",
      "   ...\n",
      "   [ 4.978718    3.8684566   3.078466   ... -4.669372   -4.0727487\n",
      "    -3.2210658 ]\n",
      "   [ 1.1230094   3.0626888   2.791873   ... -1.7717091   1.9098277\n",
      "    -0.8282971 ]\n",
      "   [-1.4047577  -1.0068825   0.7469123  ...  1.6014967   2.0464864\n",
      "     2.0891628 ]]\n",
      "\n",
      "  [[-3.0174336   0.41961712  1.9252456  ...  3.3229702   2.1282005\n",
      "     2.8171477 ]\n",
      "   [-1.3549727  -0.09280814  2.5851169  ... -0.4787041  -0.16480741\n",
      "    -0.67767453]\n",
      "   [-1.7961591  -1.8925554  -0.41655225 ... -0.76940006  2.3132296\n",
      "    -0.21110387]\n",
      "   ...\n",
      "   [ 0.8576953  -3.6549006  -5.4420285  ...  4.0205626  -0.1758729\n",
      "     0.98035526]\n",
      "   [ 0.6897928  -3.7852154  -2.1934628  ...  4.2492924   1.8425139\n",
      "     2.78854   ]\n",
      "   [-0.9834899  -3.0471325  -5.533581   ... 11.835345    4.651074\n",
      "     7.672307  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.2599558   1.0058122   0.9403136  ... -1.5453403  -0.08242535\n",
      "    -0.7897071 ]\n",
      "   [ 1.9198732  -0.84344107  2.0494642  ...  0.35112238 -5.1893573\n",
      "     0.65555036]\n",
      "   [ 2.8428354  -1.6724693  -1.4574461  ... -0.15051317 -1.5127541\n",
      "    -1.5860063 ]\n",
      "   ...\n",
      "   [ 0.84742606  0.0772418   1.5468452  ... -1.8008659  -2.2035553\n",
      "     2.1760502 ]\n",
      "   [ 2.533928   -2.0408306   3.9096775  ... -0.3570001  -4.475288\n",
      "     3.3145204 ]\n",
      "   [ 1.9546982  -0.16918159  4.8530087  ... -1.1391959  -3.8258421\n",
      "     1.032651  ]]\n",
      "\n",
      "  [[-2.1587913   1.7147038   1.544837   ...  0.18126747  2.5825894\n",
      "     1.5435201 ]\n",
      "   [-0.00582398 -2.3462925   1.799922   ... -2.3670635  -1.1330837\n",
      "    -1.2828292 ]\n",
      "   [-2.1043587  -2.8178024   2.2148285  ...  2.1391535   4.9675913\n",
      "     4.4482775 ]\n",
      "   ...\n",
      "   [ 1.7960831   2.5962052  -1.3429192  ... -4.4235926  -3.333553\n",
      "    -2.332367  ]\n",
      "   [ 0.83172405  1.051401   -1.6721448  ...  0.18235834  1.3213887\n",
      "     4.2624054 ]\n",
      "   [ 2.2817585  -0.95722556  3.0716457  ...  1.906099   -0.18643014\n",
      "    -2.2234964 ]]\n",
      "\n",
      "  [[-2.0107505   2.0400176   1.5965837  ...  2.0324154   0.5348753\n",
      "     2.6790552 ]\n",
      "   [-1.2814682  -2.6879327   1.4795719  ...  2.5140755   1.8072864\n",
      "     0.5364746 ]\n",
      "   [ 0.3697977   0.11362538 -0.8682849  ...  0.45149922 -0.7533272\n",
      "    -0.11516427]\n",
      "   ...\n",
      "   [ 0.44519848 -4.1498594  -0.6811251  ... -0.93907857 -0.5211465\n",
      "     1.2558149 ]\n",
      "   [ 0.9062684  -4.861451   -0.665113   ...  0.5005594  -0.97615194\n",
      "     0.1718299 ]\n",
      "   [-1.0379298  -3.6122782   4.838326   ...  5.3155637   1.1697037\n",
      "     2.0175195 ]]]], shape=(1, 8, 14, 14), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 14, 14)\n",
      "output.shape = (1, 8, 14, 32)\n",
      "scaled_attention.shape= (1, 14, 8, 32)\n",
      "concat_attention.shape= (1, 14, 256)\n",
      "outputs.shape= (1, 14, 256)\n",
      "(1, 14, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 14, 256)\n",
      "(1, 14, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 14, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 14, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.5176607    1.9687198    2.0989769   -0.47190297]\n",
      "   [ -0.49124843   0.08534324   0.2991765   -0.48528743]\n",
      "   [  0.24215932  -4.697081    -4.019181     0.1927161 ]\n",
      "   [  0.7396501   -3.0292149   -2.48159      0.7256521 ]\n",
      "   [  0.7377838   -1.2575825   -1.0456651    0.7396505 ]\n",
      "   [  0.12743154  -2.1013763   -1.7714081    0.15186152]\n",
      "   [  0.57692707  -1.6948001   -1.6039941    0.56792045]\n",
      "   [ -0.0456275   -3.2555232   -2.8839726   -0.07279108]\n",
      "   [  0.541922    -4.2346587   -4.088871     0.5205673 ]\n",
      "   [  0.43123668  -4.4550576   -4.342825     0.38334304]\n",
      "   [  0.2683728   -2.6908388   -3.0562704    0.25416884]\n",
      "   [  0.65151817  -2.3087127   -2.7818563    0.6532232 ]\n",
      "   [  0.7016252   -3.254609    -3.4820383    0.7049014 ]\n",
      "   [  1.2430699   -4.737128    -4.638749     1.2318369 ]]\n",
      "\n",
      "  [[ -0.5409756    2.8193953    2.0520196   -0.5236139 ]\n",
      "   [ -1.0098997    4.226584     3.5640724   -0.9722929 ]\n",
      "   [ -0.31559435  -2.7033126   -2.1731372   -0.34413084]\n",
      "   [  0.67385     -4.3054934   -2.5939085    0.6549963 ]\n",
      "   [  0.7716512   -4.6774607   -3.4361272    0.73228556]\n",
      "   [  0.30699015  -2.0128524   -1.77724      0.3016845 ]\n",
      "   [  0.95172566  -5.123536    -3.7028008    0.9193312 ]\n",
      "   [  0.1625587   -2.8245268   -2.2594504    0.1333035 ]\n",
      "   [  0.80569774  -2.9156864   -1.9677489    0.81221676]\n",
      "   [  0.5848501   -4.6709986   -3.4369607    0.5637138 ]\n",
      "   [  0.8923553   -4.0675893   -2.657831     0.87549967]\n",
      "   [  1.2629194   -3.7040567   -2.924148     1.2593203 ]\n",
      "   [  1.5493715   -7.2488613   -5.396356     1.5176687 ]\n",
      "   [  1.5049651   -6.2729235   -4.4762535    1.4955918 ]]\n",
      "\n",
      "  [[ -0.42360795   3.4683292    3.2733347   -0.39263082]\n",
      "   [ -0.47245097   0.7355631    1.3526613   -0.47366753]\n",
      "   [  0.8764782   -5.054033    -4.6074905    0.832105  ]\n",
      "   [  1.5158468   -5.5036316   -5.460711     1.4665604 ]\n",
      "   [  1.3198522   -4.7758727   -4.838473     1.2815847 ]\n",
      "   [  0.6756115   -5.767558    -5.86507      0.60872716]\n",
      "   [  0.6050416   -6.19143     -6.4390326    0.54084617]\n",
      "   [  0.44500238  -0.68225163  -0.9241052    0.42594016]\n",
      "   [  1.465985    -3.2105703   -2.976426     1.4414983 ]\n",
      "   [  1.1924549   -4.355453    -4.300772     1.1541699 ]\n",
      "   [  1.3496063   -1.4938401   -1.1592588    1.3560332 ]\n",
      "   [  0.79303217  -2.173227    -1.8807111    0.78589594]\n",
      "   [  2.1267772   -3.1087718   -3.386968     2.1233454 ]\n",
      "   [  1.1335939   -4.0095863   -3.878332     1.1041279 ]]\n",
      "\n",
      "  [[ -1.6453142    4.41468      3.7185764   -1.6388824 ]\n",
      "   [  0.1504475   -1.3770971   -0.84712774   0.10965424]\n",
      "   [  1.5174129   -5.9414515   -4.9204936    1.4784044 ]\n",
      "   [  1.9338356   -4.261786    -3.374979     1.932654  ]\n",
      "   [  1.5312709   -4.5473847   -3.4884474    1.480693  ]\n",
      "   [  2.4686995   -5.566623    -4.674686     2.4672065 ]\n",
      "   [  2.0171244   -5.156834    -4.0178304    1.9925612 ]\n",
      "   [  1.4369041   -3.667318    -2.7229993    1.4106048 ]\n",
      "   [  1.7216051   -4.4552937   -3.1465628    1.6921057 ]\n",
      "   [  1.9970524   -6.690838    -5.398477     1.966113  ]\n",
      "   [  1.44903     -3.9334073   -3.2366369    1.4087826 ]\n",
      "   [  1.3023354   -4.519708    -3.6945662    1.2822565 ]\n",
      "   [  1.6942405   -3.9762888   -3.1610596    1.6870621 ]\n",
      "   [  1.6402903   -3.3537028   -2.8734157    1.6528132 ]]\n",
      "\n",
      "  [[ -1.29121      2.8940606    1.9502285   -1.3604355 ]\n",
      "   [  0.00294916  -1.2568096   -0.89668256  -0.00931877]\n",
      "   [  0.7530518   -2.5598295   -1.9020394    0.7952453 ]\n",
      "   [  1.7065613   -5.794166    -4.5830455    1.7274754 ]\n",
      "   [  1.828848    -7.046152    -5.713792     1.8388789 ]\n",
      "   [  2.3361118   -9.356349    -7.713717     2.356966  ]\n",
      "   [  2.4712012   -7.777283    -6.1303487    2.5265574 ]\n",
      "   [  2.178688    -7.1766615   -5.927077     2.238128  ]\n",
      "   [  2.4477205   -5.1921487   -3.4574718    2.464835  ]\n",
      "   [  2.341771    -5.8448753   -4.0209694    2.4139361 ]\n",
      "   [  1.574666    -4.566143    -4.1088657    1.5959097 ]\n",
      "   [  1.2687012   -5.661861    -4.5558453    1.2785684 ]\n",
      "   [  2.2869496   -8.59737     -7.4518332    2.2985146 ]\n",
      "   [  1.9157221   -8.846792    -8.410634     1.9216846 ]]\n",
      "\n",
      "  [[ -0.65288585   3.4191878    2.6229937   -0.6571499 ]\n",
      "   [  0.72896236  -2.1471467   -1.9732635    0.77281165]\n",
      "   [  0.32350188  -1.2035745   -1.7607827    0.34396917]\n",
      "   [  1.0276341    1.058868     1.2416905    1.0519707 ]\n",
      "   [  1.6617185   -1.5399216   -1.5241555    1.7414258 ]\n",
      "   [  2.3202841   -3.4387836   -2.1360145    2.3670495 ]\n",
      "   [  2.2568145   -5.2002926   -3.3399537    2.3179612 ]\n",
      "   [  2.10796     -4.739115    -2.9632866    2.1519184 ]\n",
      "   [  1.2009804   -1.6315452   -1.1646751    1.2423136 ]\n",
      "   [  1.612212    -4.7317686   -3.3088357    1.6307019 ]\n",
      "   [  0.8388549   -2.1863418   -1.2897081    0.847988  ]\n",
      "   [  1.036908    -3.0587974   -2.584841     1.0773487 ]\n",
      "   [  1.8100622   -3.9532287   -3.6608858    1.8912151 ]\n",
      "   [  1.5701225    0.33523917   0.4572183    1.6209648 ]]\n",
      "\n",
      "  [[ -0.13920632   1.920276     1.8218281   -0.1018336 ]\n",
      "   [  0.19900118  -1.852921    -1.364047     0.17529728]\n",
      "   [  0.9201308   -2.3837185   -1.2430844    0.8900646 ]\n",
      "   [  1.0384122   -2.2614129   -0.6760908    1.014745  ]\n",
      "   [  1.3033148   -4.046018    -2.5787446    1.2661915 ]\n",
      "   [  1.1603479   -2.2507179   -1.0619235    1.1338618 ]\n",
      "   [  1.041217    -4.2461724   -2.9057727    1.0158387 ]\n",
      "   [  1.0148015   -1.7258983   -0.37147498   1.0063083 ]\n",
      "   [  0.88598007  -2.206053    -0.84811103   0.8885484 ]\n",
      "   [  1.2307554   -3.4753895   -2.1146924    1.1983802 ]\n",
      "   [  0.5709063   -1.6979381   -1.0712341    0.5441556 ]\n",
      "   [  0.938992    -2.1750786   -1.6815557    0.93939686]\n",
      "   [  0.90540814  -3.5624843   -3.3321912    0.87981683]\n",
      "   [  0.78987503  -2.7473283   -1.3229544    0.7772658 ]]\n",
      "\n",
      "  [[ -1.5968649    5.382058     4.5500836   -1.5562288 ]\n",
      "   [  0.86753845  -4.5697775   -4.1758018    0.8370436 ]\n",
      "   [  1.9240823   -9.450348    -8.618066     1.8469068 ]\n",
      "   [  1.4563193   -6.307143    -5.9952025    1.4192016 ]\n",
      "   [  1.9546863   -5.530293    -5.110774     1.9280047 ]\n",
      "   [  1.7568917   -6.1661077   -5.8179903    1.6855166 ]\n",
      "   [  3.4059234  -12.176341   -11.224475     3.3215392 ]\n",
      "   [  1.7747686   -8.078717    -7.356744     1.7220855 ]\n",
      "   [  2.41111     -7.0833406   -6.4738836    2.3771992 ]\n",
      "   [  2.242727    -8.106706    -7.0536833    2.1916888 ]\n",
      "   [  0.5262304   -3.2455208   -2.7358432    0.4870345 ]\n",
      "   [  0.8685509   -1.3043313   -0.5787912    0.8402956 ]\n",
      "   [  2.9131413   -6.756786    -5.5475316    2.8893347 ]\n",
      "   [  1.6566273   -6.555101    -6.1553826    1.6073426 ]]]], shape=(1, 8, 14, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 14, 4)\n",
      "output.shape = (1, 8, 14, 32)\n",
      "scaled_attention.shape= (1, 14, 8, 32)\n",
      "concat_attention.shape= (1, 14, 256)\n",
      "outputs.shape= (1, 14, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 15, 256)\n",
      "(1, 15, 256)\n",
      "(1, 15, 256)\n",
      "split_heads()\n",
      "(1, 15, 256)\n",
      "(1, 15, 8, 32)\n",
      "split_heads()\n",
      "(1, 15, 256)\n",
      "(1, 15, 8, 32)\n",
      "split_heads()\n",
      "(1, 15, 256)\n",
      "(1, 15, 8, 32)\n",
      "(1, 8, 15, 32)\n",
      "(1, 8, 15, 32)\n",
      "(1, 8, 15, 32)\n",
      "matmul_qk.shape = (1, 8, 15, 15)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.34484416  0.3515555  ...  0.9456161   0.6615005\n",
      "     0.63461095]\n",
      "   [-1.5524524   0.57553977  1.342203   ...  2.3425648   3.5891137\n",
      "     2.5784872 ]\n",
      "   [ 0.17656448 -2.1603198  -3.1747456  ... -4.0464153   2.018118\n",
      "    -3.3761356 ]\n",
      "   ...\n",
      "   [ 0.34657544  0.18045545  0.53776133 ...  4.162922    5.9773464\n",
      "     3.4780626 ]\n",
      "   [-0.28322992  3.1288378  -0.01700863 ...  3.6691072  -3.5476904\n",
      "     2.7394054 ]\n",
      "   [-0.92679524  3.6383579   0.98130333 ...  5.306303    8.729458\n",
      "     1.1505873 ]]\n",
      "\n",
      "  [[-0.1123142   0.09086403 -0.2228786  ...  0.57024544  0.08032935\n",
      "    -0.2231797 ]\n",
      "   [-0.27778682 -7.685327   -0.20150177 ...  3.3938026  -2.3570178\n",
      "    -1.3007069 ]\n",
      "   [ 0.30459622 -1.6710656  -3.094567   ...  0.6632749  -1.517201\n",
      "     1.251946  ]\n",
      "   ...\n",
      "   [-0.26803553  0.828541    0.58118385 ...  1.5224228  -0.038258\n",
      "    -1.1598006 ]\n",
      "   [ 0.07438865 -3.7665203  -3.0217247  ...  2.158518   -3.2898862\n",
      "    -1.5476816 ]\n",
      "   [-0.41796187 -3.853597   -0.58920324 ...  2.1069765   4.2388678\n",
      "     2.4200866 ]]\n",
      "\n",
      "  [[-0.10918517 -0.21059825  0.3513798  ... -0.02299009  0.6733783\n",
      "     0.436986  ]\n",
      "   [ 0.2860052  -5.063574   -2.1691     ... -2.6556163  -0.24466045\n",
      "    -1.3468456 ]\n",
      "   [ 0.6062982   2.719351   -4.9581633  ...  0.7751893  -3.8892925\n",
      "    -2.699599  ]\n",
      "   ...\n",
      "   [-0.18127608 -1.6091987   1.6848273  ... -2.4234788   3.1261995\n",
      "     5.9258065 ]\n",
      "   [-0.0538262   0.6051362   1.4847     ... -0.87357515 -4.348027\n",
      "     1.8271141 ]\n",
      "   [-0.27212387 -0.8335445  -2.8549159  ...  6.143021   -3.4290743\n",
      "    -6.074763  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.00999826 -0.5171405   0.3842402  ...  0.00809446 -0.05267796\n",
      "    -0.25493878]\n",
      "   [ 0.3875343   2.307618   -2.0652568  ... -1.4210955  -1.0692489\n",
      "     1.1628796 ]\n",
      "   [ 0.5494998   0.9012554  -5.717646   ... -2.5381784  -2.9159274\n",
      "     0.43149132]\n",
      "   ...\n",
      "   [ 0.4221106  -0.0660239  -1.7760104  ... -1.519908   -1.273347\n",
      "     1.5201464 ]\n",
      "   [ 0.33041304 -2.5043929  -1.3398037  ... -5.2653575  -9.975928\n",
      "     0.45637536]\n",
      "   [ 0.41852745  2.3563616  -3.8839002  ...  2.091183    2.3679385\n",
      "    -2.8890147 ]]\n",
      "\n",
      "  [[-0.20863976  0.0058533   0.141982   ...  0.22268027  0.13776878\n",
      "    -0.21355845]\n",
      "   [-0.5011563  -5.83805     3.0883057  ... -3.6622307   0.4724806\n",
      "    -1.5432332 ]\n",
      "   [-0.08541125  2.1106565   0.02025323 ...  1.3210721   2.305009\n",
      "     1.669142  ]\n",
      "   ...\n",
      "   [ 0.7425708   0.3618796  -1.2513036  ...  1.2826715  -3.4366665\n",
      "    -1.2998371 ]\n",
      "   [ 1.5437251  -6.752349   -2.278639   ... -2.443643   -7.9576335\n",
      "    -1.7474873 ]\n",
      "   [ 0.29995587  0.06776552  1.1632988  ... -0.32721785 -3.3951726\n",
      "    -6.324067  ]]\n",
      "\n",
      "  [[-0.15726982  0.34343007 -0.0106815  ...  0.22036563  0.67823213\n",
      "    -1.0174865 ]\n",
      "   [ 0.42748785 -3.541775   -4.416529   ...  2.315991    0.66909575\n",
      "     3.8918982 ]\n",
      "   [ 0.09384713  3.2008502  -4.3988295  ... -0.2447487  -2.1684322\n",
      "     1.2025319 ]\n",
      "   ...\n",
      "   [-0.3709949  -1.8810337   4.8812547  ...  0.9184051   2.088492\n",
      "     0.8183651 ]\n",
      "   [ 0.8418388  -1.6215484  -0.95807385 ... -1.1528292  -1.4905474\n",
      "     5.5907626 ]\n",
      "   [-0.19823378  1.7834846  -3.0872905  ...  3.087575   -0.6042437\n",
      "     0.9021961 ]]]], shape=(1, 8, 15, 15), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 15, 15)\n",
      "output.shape = (1, 8, 15, 32)\n",
      "scaled_attention.shape= (1, 15, 8, 32)\n",
      "concat_attention.shape= (1, 15, 256)\n",
      "outputs.shape= (1, 15, 256)\n",
      "(1, 15, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 15, 256)\n",
      "(1, 15, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 15, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 15, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.6510173   -2.8665452   -1.9706712    0.6258339 ]\n",
      "   [ -1.2761326   -0.6591419   -0.772127    -1.2929033 ]\n",
      "   [  1.0074586   -3.2963974   -3.106445     0.9872349 ]\n",
      "   [  1.2579683   -4.881622    -3.767476     1.2469316 ]\n",
      "   [  1.3234984   -5.9344664   -5.2370906    1.2870476 ]\n",
      "   [  1.7000594   -3.6593008   -2.971424     1.7176253 ]\n",
      "   [  1.2776281   -1.9751713   -1.519901     1.2974825 ]\n",
      "   [  0.58336854  -1.1657993   -0.539427     0.5778812 ]\n",
      "   [  0.7600763   -0.7146235    0.06742804   0.76477766]\n",
      "   [  0.68825406   0.40578374   1.1206802    0.7197399 ]\n",
      "   [  1.5945777   -1.6418726   -0.5148084    1.594585  ]\n",
      "   [  1.7847351   -0.36713108   0.68376136   1.8038828 ]\n",
      "   [  1.5564973    0.14751111   0.38390478   1.6049845 ]\n",
      "   [  1.6079514   -0.09540926   0.7814428    1.6541259 ]\n",
      "   [  0.81602544  -0.8113432    0.67859685   0.80059373]]\n",
      "\n",
      "  [[  1.5699102   -4.8338065   -2.8935153    1.5974954 ]\n",
      "   [  1.602976    -6.0585413   -4.3626466    1.6033887 ]\n",
      "   [  0.84273714  -3.3321726   -3.019569     0.8300638 ]\n",
      "   [  1.739568    -5.32123     -4.1527953    1.7057066 ]\n",
      "   [  1.9442569   -4.8756948   -3.3264403    1.9580569 ]\n",
      "   [  3.9294844  -11.821626    -9.099886     3.8940978 ]\n",
      "   [  2.184128    -5.056767    -3.4162076    2.1828146 ]\n",
      "   [  1.9612429   -4.841177    -3.8993757    1.9119158 ]\n",
      "   [  2.1313474   -8.619019    -7.0290985    2.1060083 ]\n",
      "   [  1.7264353   -4.697804    -4.539785     1.657041  ]\n",
      "   [  2.3086565   -7.632138    -5.643827     2.2703083 ]\n",
      "   [  3.4966753   -9.881069    -7.4213333    3.445355  ]\n",
      "   [  2.082511    -6.267626    -4.8032327    2.0366788 ]\n",
      "   [  2.4420376   -7.92061     -6.2677646    2.3764477 ]\n",
      "   [  1.5715002   -2.5319417   -1.8216251    1.5918317 ]]\n",
      "\n",
      "  [[ -0.9479617   -1.5584124   -2.3440182   -0.91537315]\n",
      "   [  1.1684959   -6.083402    -5.361126     1.1414931 ]\n",
      "   [  0.8458585   -8.91149     -7.7787905    0.78298134]\n",
      "   [  2.5803494   -5.880795    -5.0921974    2.5411623 ]\n",
      "   [  3.3461082  -11.678779   -10.798813     3.272511  ]\n",
      "   [  3.3453295   -7.0702214   -5.903679     3.2953756 ]\n",
      "   [  1.6342559   -4.6711273   -4.3739734    1.6045712 ]\n",
      "   [  1.7710527   -4.211759    -3.7379825    1.7677531 ]\n",
      "   [  2.3430088   -5.5874043   -4.7693295    2.3498065 ]\n",
      "   [  1.7600528   -4.8194313   -3.658041     1.7406274 ]\n",
      "   [  1.8383347   -4.2881994   -3.964903     1.8433948 ]\n",
      "   [  3.1041694   -6.2948246   -5.899711     3.0869215 ]\n",
      "   [  2.1902268   -8.600747    -8.473362     2.176129  ]\n",
      "   [  2.3792868   -7.585834    -7.168655     2.3181617 ]\n",
      "   [ -0.32808784   3.1995692    2.6395712   -0.32183775]]\n",
      "\n",
      "  [[ -0.53032047  -2.1487055   -3.3869808   -0.5465881 ]\n",
      "   [ -0.37673792   0.5297353    0.7943683   -0.41485333]\n",
      "   [  1.2794939   -4.2864437   -4.2701783    1.2569122 ]\n",
      "   [  2.427947    -1.600872    -0.90655017   2.481265  ]\n",
      "   [  2.3424263   -7.2077713   -5.824986     2.335523  ]\n",
      "   [  1.7222499   -2.7512228   -2.3087296    1.7864217 ]\n",
      "   [  1.0004832   -7.1413507   -6.550554     0.95411247]\n",
      "   [  1.4302231   -3.8393989   -3.2885814    1.4499868 ]\n",
      "   [  1.734392    -5.0363717   -4.562415     1.7265733 ]\n",
      "   [  1.7437402   -4.61876     -3.9143934    1.71687   ]\n",
      "   [  1.1339672   -3.6319485   -2.8497448    1.1018116 ]\n",
      "   [  1.510794    -1.2551379   -0.05228771   1.5522768 ]\n",
      "   [  1.0197178   -1.2675874   -0.6745764    1.067578  ]\n",
      "   [  0.9115422   -3.4677386   -2.6383572    0.8930571 ]\n",
      "   [  0.81263214  -2.756228    -2.4330876    0.81427544]]\n",
      "\n",
      "  [[ -1.8786137    3.8044016    1.7851309   -1.9308672 ]\n",
      "   [ -2.0427704    1.9088362    0.8611902   -2.10597   ]\n",
      "   [  0.3713629   -6.5012736   -5.5806484    0.3149101 ]\n",
      "   [  2.3904674   -0.8018907    0.54941446   2.4430196 ]\n",
      "   [  3.2833803   -5.793328    -3.5530388    3.3556385 ]\n",
      "   [  4.031575    -8.23709     -5.352422     4.1288157 ]\n",
      "   [  1.704372    -4.0030203   -2.8744233    1.746541  ]\n",
      "   [  0.55620587  -0.81578505  -0.7211592    0.5843559 ]\n",
      "   [  1.4359853   -5.1196675   -3.4192846    1.4993091 ]\n",
      "   [  3.066259    -8.724568    -6.3771715    3.138459  ]\n",
      "   [  3.077098    -5.4629254   -3.248803     3.1306965 ]\n",
      "   [  4.6866465   -9.521488    -5.9786706    4.7835746 ]\n",
      "   [  3.749898    -6.577472    -4.5180645    3.8417404 ]\n",
      "   [  5.4376407   -9.574739    -5.771271     5.5562477 ]\n",
      "   [  2.670423    -2.9949458   -1.8302813    2.7224376 ]]\n",
      "\n",
      "  [[  1.8587611   -2.3004422   -1.999603     1.8540798 ]\n",
      "   [  0.62134135  -1.1685245   -1.0336286    0.63556284]\n",
      "   [  0.93438053  -3.9921112   -3.1974504    0.91454566]\n",
      "   [  2.386284    -8.792911    -6.874719     2.3729997 ]\n",
      "   [  2.8132436  -11.001211    -8.411129     2.7784264 ]\n",
      "   [  2.7941167   -3.5710614   -2.4098222    2.8085887 ]\n",
      "   [  2.5607827   -5.1447167   -4.0192566    2.5128086 ]\n",
      "   [  1.9819213   -4.7589607   -4.3256965    1.9169271 ]\n",
      "   [  2.1862228   -3.8017366   -2.162978     2.196947  ]\n",
      "   [  2.025622    -0.50651217  -0.09534571   2.0341551 ]\n",
      "   [  1.7751181   -5.396942    -3.6041234    1.7526699 ]\n",
      "   [  1.8905165   -2.984126    -2.7361436    1.8944441 ]\n",
      "   [  2.19154     -4.3217793   -3.247157     2.1910028 ]\n",
      "   [  2.6326275   -2.9615233   -1.8003258    2.6587393 ]\n",
      "   [  1.8826354   -1.9406146   -0.6737225    1.887764  ]]\n",
      "\n",
      "  [[  0.263768     1.3027129    0.7912128    0.31364933]\n",
      "   [  0.90733486  -1.2863759   -0.23643312   0.92466456]\n",
      "   [  1.9193958   -5.6561565   -3.624307     1.9176353 ]\n",
      "   [  2.590705    -7.7614427   -5.438415     2.6344485 ]\n",
      "   [  2.0141904   -6.001995    -3.7005918    2.02994   ]\n",
      "   [  2.3413384   -2.5982022   -0.645182     2.369621  ]\n",
      "   [  1.1849254   -0.41514406   0.7534283    1.1835097 ]\n",
      "   [  1.6196365   -2.736624    -1.3294117    1.6487104 ]\n",
      "   [  1.8510432   -2.5615194   -0.7961973    1.8711935 ]\n",
      "   [  2.0705955   -4.3488426   -2.0247169    2.1132114 ]\n",
      "   [  1.9942156   -2.1047823   -0.90358776   2.0466616 ]\n",
      "   [  0.49939027  -1.217362    -0.46714777   0.52691543]\n",
      "   [  1.1221981   -0.761073     0.50912005   1.1667402 ]\n",
      "   [  1.2064869   -1.128277    -0.1816193    1.2461394 ]\n",
      "   [  1.2066578   -2.3339682   -0.79060626   1.220551  ]]\n",
      "\n",
      "  [[ -0.50981736   4.1422954    3.6196024   -0.43590528]\n",
      "   [  0.4603232   -1.0371166   -0.40937394   0.4394898 ]\n",
      "   [  1.2603666   -3.7127948   -2.6582818    1.2983314 ]\n",
      "   [  1.9421246   -6.2256336   -3.6375484    1.9323288 ]\n",
      "   [  1.3231326   -4.248828    -3.1260052    1.3073777 ]\n",
      "   [  2.442132    -7.4952087   -5.2700887    2.4572377 ]\n",
      "   [  2.4187922   -5.27305     -2.1958756    2.4965396 ]\n",
      "   [  1.0935532   -3.028773    -1.9892881    1.1174632 ]\n",
      "   [  1.07857     -1.6685917   -0.5144004    1.1236442 ]\n",
      "   [  1.9439204   -4.8428583   -2.7191334    2.0016615 ]\n",
      "   [  1.3960849   -4.567839    -2.928855     1.4403856 ]\n",
      "   [  1.6036419   -5.08729     -3.1608024    1.6399496 ]\n",
      "   [  2.1432507   -5.3504367   -3.4768052    2.1538277 ]\n",
      "   [  2.0789034   -8.089037    -6.1146235    2.1081893 ]\n",
      "   [  0.6491808   -1.4989835   -0.8922873    0.6634323 ]]]], shape=(1, 8, 15, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 15, 4)\n",
      "output.shape = (1, 8, 15, 32)\n",
      "scaled_attention.shape= (1, 15, 8, 32)\n",
      "concat_attention.shape= (1, 15, 256)\n",
      "outputs.shape= (1, 15, 256)\n",
      "(1, 15, 256)\n",
      "(1, 15, 256)\n",
      "(1, 15, 256)\n",
      "split_heads()\n",
      "(1, 15, 256)\n",
      "(1, 15, 8, 32)\n",
      "split_heads()\n",
      "(1, 15, 256)\n",
      "(1, 15, 8, 32)\n",
      "split_heads()\n",
      "(1, 15, 256)\n",
      "(1, 15, 8, 32)\n",
      "(1, 8, 15, 32)\n",
      "(1, 8, 15, 32)\n",
      "(1, 8, 15, 32)\n",
      "matmul_qk.shape = (1, 8, 15, 15)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181 -0.6147039  ...  1.0203538   1.229891\n",
      "     0.10202239]\n",
      "   [ 3.96568    -0.17723085  1.5590783  ...  0.33617422 -6.952222\n",
      "     3.4967818 ]\n",
      "   [ 3.9833748  -2.2539997   0.15140963 ... -3.7684631  -1.7606908\n",
      "     0.5195799 ]\n",
      "   ...\n",
      "   [ 3.9610004  -1.3537385   1.7564551  ... -5.123631   -3.2233567\n",
      "     0.66035277]\n",
      "   [ 1.616248   -0.41301975  2.1102164  ... -0.30368668 -3.1939867\n",
      "     1.1761485 ]\n",
      "   [ 4.979084   -2.5243933   1.1940266  ... -2.5406103  -7.0161867\n",
      "    -2.0503643 ]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489  -0.22415894 ... -0.34874386 -1.5257807\n",
      "     0.43077925]\n",
      "   [ 0.22428446 -2.402899   -1.7699208  ... -0.4989716  -0.9432326\n",
      "    -0.39453703]\n",
      "   [-0.6712342   0.30168235 -3.0742364  ...  0.28522968 -3.5610971\n",
      "    -1.3580014 ]\n",
      "   ...\n",
      "   [ 1.1230094   3.0626888   2.791873   ...  1.9098277  -0.8282971\n",
      "     1.0368469 ]\n",
      "   [-1.4047577  -1.0068825   0.7469123  ...  2.0464864   2.0891628\n",
      "     0.02782362]\n",
      "   [-0.8975079  -0.94379324  1.5547016  ...  0.63981944  1.8971337\n",
      "    -2.2477436 ]]\n",
      "\n",
      "  [[-3.0174336   0.41961712  1.9252456  ...  2.1282005   2.8171477\n",
      "     0.6974792 ]\n",
      "   [-1.3549727  -0.09280814  2.5851169  ... -0.16480741 -0.67767453\n",
      "    -0.24939446]\n",
      "   [-1.7961591  -1.8925554  -0.41655225 ...  2.3132296  -0.21110387\n",
      "    -0.30785158]\n",
      "   ...\n",
      "   [ 0.6897928  -3.7852154  -2.1934628  ...  1.8425139   2.78854\n",
      "     0.860141  ]\n",
      "   [-0.9834899  -3.0471325  -5.533581   ...  4.651074    7.672307\n",
      "     6.376149  ]\n",
      "   [ 0.31654188  1.2054088   4.509779   ... -1.5109179  -2.3320508\n",
      "    -2.4442647 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.2599558   1.0058122   0.9403136  ... -0.08242535 -0.7897071\n",
      "     0.8426995 ]\n",
      "   [ 1.9198732  -0.84344107  2.0494642  ... -5.1893573   0.65555036\n",
      "    -3.7020385 ]\n",
      "   [ 2.8428354  -1.6724693  -1.4574461  ... -1.5127541  -1.5860063\n",
      "    -5.172351  ]\n",
      "   ...\n",
      "   [ 2.533928   -2.0408306   3.9096775  ... -4.475288    3.3145204\n",
      "    -4.4558744 ]\n",
      "   [ 1.9546982  -0.16918159  4.8530087  ... -3.8258421   1.032651\n",
      "     0.17833906]\n",
      "   [-0.69396204  2.2259943   3.0461872  ...  0.44502428  2.4366105\n",
      "     2.7817082 ]]\n",
      "\n",
      "  [[-2.1587913   1.7147038   1.544837   ...  2.5825894   1.5435201\n",
      "     1.5954434 ]\n",
      "   [-0.00582398 -2.3462925   1.799922   ... -1.1330837  -1.2828292\n",
      "    -1.1819105 ]\n",
      "   [-2.1043587  -2.8178024   2.2148285  ...  4.9675913   4.4482775\n",
      "     0.9823559 ]\n",
      "   ...\n",
      "   [ 0.83172405  1.051401   -1.6721448  ...  1.3213887   4.2624054\n",
      "    -0.06496749]\n",
      "   [ 2.2817585  -0.95722556  3.0716457  ... -0.18643014 -2.2234964\n",
      "    -3.2230518 ]\n",
      "   [ 4.4061084   3.3855317  -3.9696262  ... -4.231593   -3.8530157\n",
      "    -0.28425494]]\n",
      "\n",
      "  [[-2.0107505   2.0400176   1.5965837  ...  0.5348753   2.6790552\n",
      "     0.27028453]\n",
      "   [-1.2814682  -2.6879327   1.4795719  ...  1.8072864   0.5364746\n",
      "    -1.2062161 ]\n",
      "   [ 0.3697977   0.11362538 -0.8682849  ... -0.7533272  -0.11516427\n",
      "    -2.3599706 ]\n",
      "   ...\n",
      "   [ 0.9062684  -4.861451   -0.665113   ... -0.97615194  0.1718299\n",
      "     0.45203355]\n",
      "   [-1.0379298  -3.6122782   4.838326   ...  1.1697037   2.0175195\n",
      "     1.247986  ]\n",
      "   [-1.7247764  -2.6890588  -0.49130026 ...  2.2805429   3.6633792\n",
      "    -1.7957431 ]]]], shape=(1, 8, 15, 15), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 15, 15)\n",
      "output.shape = (1, 8, 15, 32)\n",
      "scaled_attention.shape= (1, 15, 8, 32)\n",
      "concat_attention.shape= (1, 15, 256)\n",
      "outputs.shape= (1, 15, 256)\n",
      "(1, 15, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 15, 256)\n",
      "(1, 15, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 15, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 15, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.5176607    1.9687198    2.0989769   -0.47190297]\n",
      "   [ -0.49124843   0.08534324   0.2991765   -0.48528743]\n",
      "   [  0.24215932  -4.697081    -4.019181     0.1927161 ]\n",
      "   [  0.7396501   -3.0292149   -2.48159      0.7256521 ]\n",
      "   [  0.7377838   -1.2575825   -1.0456651    0.7396505 ]\n",
      "   [  0.12743154  -2.1013763   -1.7714081    0.15186152]\n",
      "   [  0.57692707  -1.6948001   -1.6039941    0.56792045]\n",
      "   [ -0.0456275   -3.2555232   -2.8839726   -0.07279108]\n",
      "   [  0.541922    -4.2346587   -4.088871     0.5205673 ]\n",
      "   [  0.43123668  -4.4550576   -4.342825     0.38334304]\n",
      "   [  0.2683728   -2.6908388   -3.0562704    0.25416884]\n",
      "   [  0.65151817  -2.3087127   -2.7818563    0.6532232 ]\n",
      "   [  0.7016252   -3.254609    -3.4820383    0.7049014 ]\n",
      "   [  1.2430699   -4.737128    -4.638749     1.2318369 ]\n",
      "   [  0.49773553  -2.68543     -2.2522824    0.4736727 ]]\n",
      "\n",
      "  [[ -0.5409756    2.8193953    2.0520196   -0.5236139 ]\n",
      "   [ -1.0098997    4.226584     3.5640724   -0.9722929 ]\n",
      "   [ -0.31559435  -2.7033126   -2.1731372   -0.34413084]\n",
      "   [  0.67385     -4.3054934   -2.5939085    0.6549963 ]\n",
      "   [  0.7716512   -4.6774607   -3.4361272    0.73228556]\n",
      "   [  0.30699015  -2.0128524   -1.77724      0.3016845 ]\n",
      "   [  0.95172566  -5.123536    -3.7028008    0.9193312 ]\n",
      "   [  0.1625587   -2.8245268   -2.2594504    0.1333035 ]\n",
      "   [  0.80569774  -2.9156864   -1.9677489    0.81221676]\n",
      "   [  0.5848501   -4.6709986   -3.4369607    0.5637138 ]\n",
      "   [  0.8923553   -4.0675893   -2.657831     0.87549967]\n",
      "   [  1.2629194   -3.7040567   -2.924148     1.2593203 ]\n",
      "   [  1.5493715   -7.2488613   -5.396356     1.5176687 ]\n",
      "   [  1.5049651   -6.2729235   -4.4762535    1.4955918 ]\n",
      "   [  0.5231369   -4.513709    -2.8817887    0.49956077]]\n",
      "\n",
      "  [[ -0.42360795   3.4683292    3.2733347   -0.39263082]\n",
      "   [ -0.47245097   0.7355631    1.3526613   -0.47366753]\n",
      "   [  0.8764782   -5.054033    -4.6074905    0.832105  ]\n",
      "   [  1.5158468   -5.5036316   -5.460711     1.4665604 ]\n",
      "   [  1.3198522   -4.7758727   -4.838473     1.2815847 ]\n",
      "   [  0.6756115   -5.767558    -5.86507      0.60872716]\n",
      "   [  0.6050416   -6.19143     -6.4390326    0.54084617]\n",
      "   [  0.44500238  -0.68225163  -0.9241052    0.42594016]\n",
      "   [  1.465985    -3.2105703   -2.976426     1.4414983 ]\n",
      "   [  1.1924549   -4.355453    -4.300772     1.1541699 ]\n",
      "   [  1.3496063   -1.4938401   -1.1592588    1.3560332 ]\n",
      "   [  0.79303217  -2.173227    -1.8807111    0.78589594]\n",
      "   [  2.1267772   -3.1087718   -3.386968     2.1233454 ]\n",
      "   [  1.1335939   -4.0095863   -3.878332     1.1041279 ]\n",
      "   [  0.9435391   -4.901097    -4.4360285    0.9156989 ]]\n",
      "\n",
      "  [[ -1.6453142    4.41468      3.7185764   -1.6388824 ]\n",
      "   [  0.1504475   -1.3770971   -0.84712774   0.10965424]\n",
      "   [  1.5174129   -5.9414515   -4.9204936    1.4784044 ]\n",
      "   [  1.9338356   -4.261786    -3.374979     1.932654  ]\n",
      "   [  1.5312709   -4.5473847   -3.4884474    1.480693  ]\n",
      "   [  2.4686995   -5.566623    -4.674686     2.4672065 ]\n",
      "   [  2.0171244   -5.156834    -4.0178304    1.9925612 ]\n",
      "   [  1.4369041   -3.667318    -2.7229993    1.4106048 ]\n",
      "   [  1.7216051   -4.4552937   -3.1465628    1.6921057 ]\n",
      "   [  1.9970524   -6.690838    -5.398477     1.966113  ]\n",
      "   [  1.44903     -3.9334073   -3.2366369    1.4087826 ]\n",
      "   [  1.3023354   -4.519708    -3.6945662    1.2822565 ]\n",
      "   [  1.6942405   -3.9762888   -3.1610596    1.6870621 ]\n",
      "   [  1.6402903   -3.3537028   -2.8734157    1.6528132 ]\n",
      "   [  1.8542731   -4.373769    -3.7191234    1.8499788 ]]\n",
      "\n",
      "  [[ -1.29121      2.8940606    1.9502285   -1.3604355 ]\n",
      "   [  0.00294916  -1.2568096   -0.89668256  -0.00931877]\n",
      "   [  0.7530518   -2.5598295   -1.9020394    0.7952453 ]\n",
      "   [  1.7065613   -5.794166    -4.5830455    1.7274754 ]\n",
      "   [  1.828848    -7.046152    -5.713792     1.8388789 ]\n",
      "   [  2.3361118   -9.356349    -7.713717     2.356966  ]\n",
      "   [  2.4712012   -7.777283    -6.1303487    2.5265574 ]\n",
      "   [  2.178688    -7.1766615   -5.927077     2.238128  ]\n",
      "   [  2.4477205   -5.1921487   -3.4574718    2.464835  ]\n",
      "   [  2.341771    -5.8448753   -4.0209694    2.4139361 ]\n",
      "   [  1.574666    -4.566143    -4.1088657    1.5959097 ]\n",
      "   [  1.2687012   -5.661861    -4.5558453    1.2785684 ]\n",
      "   [  2.2869496   -8.59737     -7.4518332    2.2985146 ]\n",
      "   [  1.9157221   -8.846792    -8.410634     1.9216846 ]\n",
      "   [  1.6665998   -8.258074    -6.8935685    1.6862932 ]]\n",
      "\n",
      "  [[ -0.65288585   3.4191878    2.6229937   -0.6571499 ]\n",
      "   [  0.72896236  -2.1471467   -1.9732635    0.77281165]\n",
      "   [  0.32350188  -1.2035745   -1.7607827    0.34396917]\n",
      "   [  1.0276341    1.058868     1.2416905    1.0519707 ]\n",
      "   [  1.6617185   -1.5399216   -1.5241555    1.7414258 ]\n",
      "   [  2.3202841   -3.4387836   -2.1360145    2.3670495 ]\n",
      "   [  2.2568145   -5.2002926   -3.3399537    2.3179612 ]\n",
      "   [  2.10796     -4.739115    -2.9632866    2.1519184 ]\n",
      "   [  1.2009804   -1.6315452   -1.1646751    1.2423136 ]\n",
      "   [  1.612212    -4.7317686   -3.3088357    1.6307019 ]\n",
      "   [  0.8388549   -2.1863418   -1.2897081    0.847988  ]\n",
      "   [  1.036908    -3.0587974   -2.584841     1.0773487 ]\n",
      "   [  1.8100622   -3.9532287   -3.6608858    1.8912151 ]\n",
      "   [  1.5701225    0.33523917   0.4572183    1.6209648 ]\n",
      "   [  0.7436306   -3.9707983   -3.7097435    0.7386792 ]]\n",
      "\n",
      "  [[ -0.13920632   1.920276     1.8218281   -0.1018336 ]\n",
      "   [  0.19900118  -1.852921    -1.364047     0.17529728]\n",
      "   [  0.9201308   -2.3837185   -1.2430844    0.8900646 ]\n",
      "   [  1.0384122   -2.2614129   -0.6760908    1.014745  ]\n",
      "   [  1.3033148   -4.046018    -2.5787446    1.2661915 ]\n",
      "   [  1.1603479   -2.2507179   -1.0619235    1.1338618 ]\n",
      "   [  1.041217    -4.2461724   -2.9057727    1.0158387 ]\n",
      "   [  1.0148015   -1.7258983   -0.37147498   1.0063083 ]\n",
      "   [  0.88598007  -2.206053    -0.84811103   0.8885484 ]\n",
      "   [  1.2307554   -3.4753895   -2.1146924    1.1983802 ]\n",
      "   [  0.5709063   -1.6979381   -1.0712341    0.5441556 ]\n",
      "   [  0.938992    -2.1750786   -1.6815557    0.93939686]\n",
      "   [  0.90540814  -3.5624843   -3.3321912    0.87981683]\n",
      "   [  0.78987503  -2.7473283   -1.3229544    0.7772658 ]\n",
      "   [  0.8431015    1.9753726    2.62057      0.83691   ]]\n",
      "\n",
      "  [[ -1.5968649    5.382058     4.5500836   -1.5562288 ]\n",
      "   [  0.86753845  -4.5697775   -4.1758018    0.8370436 ]\n",
      "   [  1.9240823   -9.450348    -8.618066     1.8469068 ]\n",
      "   [  1.4563193   -6.307143    -5.9952025    1.4192016 ]\n",
      "   [  1.9546863   -5.530293    -5.110774     1.9280047 ]\n",
      "   [  1.7568917   -6.1661077   -5.8179903    1.6855166 ]\n",
      "   [  3.4059234  -12.176341   -11.224475     3.3215392 ]\n",
      "   [  1.7747686   -8.078717    -7.356744     1.7220855 ]\n",
      "   [  2.41111     -7.0833406   -6.4738836    2.3771992 ]\n",
      "   [  2.242727    -8.106706    -7.0536833    2.1916888 ]\n",
      "   [  0.5262304   -3.2455208   -2.7358432    0.4870345 ]\n",
      "   [  0.8685509   -1.3043313   -0.5787912    0.8402956 ]\n",
      "   [  2.9131413   -6.756786    -5.5475316    2.8893347 ]\n",
      "   [  1.6566273   -6.555101    -6.1553826    1.6073426 ]\n",
      "   [  2.781743    -9.328011    -8.780681     2.7215238 ]]]], shape=(1, 8, 15, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 15, 4)\n",
      "output.shape = (1, 8, 15, 32)\n",
      "scaled_attention.shape= (1, 15, 8, 32)\n",
      "concat_attention.shape= (1, 15, 256)\n",
      "outputs.shape= (1, 15, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 16, 256)\n",
      "(1, 16, 256)\n",
      "(1, 16, 256)\n",
      "split_heads()\n",
      "(1, 16, 256)\n",
      "(1, 16, 8, 32)\n",
      "split_heads()\n",
      "(1, 16, 256)\n",
      "(1, 16, 8, 32)\n",
      "split_heads()\n",
      "(1, 16, 256)\n",
      "(1, 16, 8, 32)\n",
      "(1, 8, 16, 32)\n",
      "(1, 8, 16, 32)\n",
      "(1, 8, 16, 32)\n",
      "matmul_qk.shape = (1, 8, 16, 16)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.34484416  0.3515555  ...  0.6615005   0.63461095\n",
      "     0.7865152 ]\n",
      "   [-1.5524524   0.57553977  1.342203   ...  3.5891137   2.5784872\n",
      "     2.9062896 ]\n",
      "   [ 0.17656448 -2.1603198  -3.1747456  ...  2.018118   -3.3761356\n",
      "     0.92410535]\n",
      "   ...\n",
      "   [-0.28322992  3.1288378  -0.01700863 ... -3.5476904   2.7394054\n",
      "     2.5613096 ]\n",
      "   [-0.92679524  3.6383579   0.98130333 ...  8.729458    1.1505873\n",
      "     3.6033769 ]\n",
      "   [-1.3367499   2.0600512   2.5556252  ...  1.308313    1.3783121\n",
      "     1.4867456 ]]\n",
      "\n",
      "  [[-0.1123142   0.09086403 -0.2228786  ...  0.08032935 -0.2231797\n",
      "     0.17072286]\n",
      "   [-0.27778682 -7.685327   -0.20150177 ... -2.3570178  -1.3007069\n",
      "     4.5544143 ]\n",
      "   [ 0.30459622 -1.6710656  -3.094567   ... -1.517201    1.251946\n",
      "     0.25938827]\n",
      "   ...\n",
      "   [ 0.07438865 -3.7665203  -3.0217247  ... -3.2898862  -1.5476816\n",
      "     2.1278582 ]\n",
      "   [-0.41796187 -3.853597   -0.58920324 ...  4.2388678   2.4200866\n",
      "     4.729968  ]\n",
      "   [ 0.06541885  3.6411054   2.6309452  ... -5.2802777  -5.7292595\n",
      "    -4.9372163 ]]\n",
      "\n",
      "  [[-0.10918517 -0.21059825  0.3513798  ...  0.6733783   0.436986\n",
      "     0.0950371 ]\n",
      "   [ 0.2860052  -5.063574   -2.1691     ... -0.24466045 -1.3468456\n",
      "    -0.7412987 ]\n",
      "   [ 0.6062982   2.719351   -4.9581633  ... -3.8892925  -2.699599\n",
      "     0.56430966]\n",
      "   ...\n",
      "   [-0.0538262   0.6051362   1.4847     ... -4.348027    1.8271141\n",
      "    -2.4978745 ]\n",
      "   [-0.27212387 -0.8335445  -2.8549159  ... -3.4290743  -6.074763\n",
      "     0.7918832 ]\n",
      "   [-0.08114852  0.001079   -1.5840371  ...  8.464653    4.591612\n",
      "     0.53447896]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.00999826 -0.5171405   0.3842402  ... -0.05267796 -0.25493878\n",
      "    -0.22295314]\n",
      "   [ 0.3875343   2.307618   -2.0652568  ... -1.0692489   1.1628796\n",
      "    -1.388001  ]\n",
      "   [ 0.5494998   0.9012554  -5.717646   ... -2.9159274   0.43149132\n",
      "     1.0211118 ]\n",
      "   ...\n",
      "   [ 0.33041304 -2.5043929  -1.3398037  ... -9.975928    0.45637536\n",
      "     2.0997488 ]\n",
      "   [ 0.41852745  2.3563616  -3.8839002  ...  2.3679385  -2.8890147\n",
      "    -2.0019329 ]\n",
      "   [ 1.6033801  -3.0862079  -1.7501171  ... -0.09048054 -8.952758\n",
      "    -6.454732  ]]\n",
      "\n",
      "  [[-0.20863976  0.0058533   0.141982   ...  0.13776878 -0.21355845\n",
      "    -0.19702168]\n",
      "   [-0.5011563  -5.83805     3.0883057  ...  0.4724806  -1.5432332\n",
      "    -1.1731887 ]\n",
      "   [-0.08541125  2.1106565   0.02025323 ...  2.305009    1.669142\n",
      "     0.02897008]\n",
      "   ...\n",
      "   [ 1.5437251  -6.752349   -2.278639   ... -7.9576335  -1.7474873\n",
      "    -2.1311302 ]\n",
      "   [ 0.29995587  0.06776552  1.1632988  ... -3.3951726  -6.324067\n",
      "    -4.9027705 ]\n",
      "   [-1.3586298   3.6013882  -2.2373238  ... -0.7199012  -2.4544632\n",
      "     0.8931347 ]]\n",
      "\n",
      "  [[-0.15726982  0.34343007 -0.0106815  ...  0.67823213 -1.0174865\n",
      "    -0.0449193 ]\n",
      "   [ 0.42748785 -3.541775   -4.416529   ...  0.66909575  3.8918982\n",
      "     0.18530478]\n",
      "   [ 0.09384713  3.2008502  -4.3988295  ... -2.1684322   1.2025319\n",
      "     1.5590907 ]\n",
      "   ...\n",
      "   [ 0.8418388  -1.6215484  -0.95807385 ... -1.4905474   5.5907626\n",
      "     0.2777496 ]\n",
      "   [-0.19823378  1.7834846  -3.0872905  ... -0.6042437   0.9021961\n",
      "    -0.08039191]\n",
      "   [ 0.03635634 -0.2409001   2.3939605  ... -1.8368378  -0.81020117\n",
      "     1.5436845 ]]]], shape=(1, 8, 16, 16), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 16, 16)\n",
      "output.shape = (1, 8, 16, 32)\n",
      "scaled_attention.shape= (1, 16, 8, 32)\n",
      "concat_attention.shape= (1, 16, 256)\n",
      "outputs.shape= (1, 16, 256)\n",
      "(1, 16, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 16, 256)\n",
      "(1, 16, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 16, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 16, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.6510173   -2.8665452   -1.9706712    0.6258339 ]\n",
      "   [ -1.2761326   -0.6591419   -0.772127    -1.2929033 ]\n",
      "   [  1.0074586   -3.2963974   -3.106445     0.9872349 ]\n",
      "   [  1.2579683   -4.881622    -3.767476     1.2469316 ]\n",
      "   [  1.3234984   -5.9344664   -5.2370906    1.2870476 ]\n",
      "   [  1.7000594   -3.6593008   -2.971424     1.7176253 ]\n",
      "   [  1.2776281   -1.9751713   -1.519901     1.2974825 ]\n",
      "   [  0.58336854  -1.1657993   -0.539427     0.5778812 ]\n",
      "   [  0.7600763   -0.7146235    0.06742804   0.76477766]\n",
      "   [  0.68825406   0.40578374   1.1206802    0.7197399 ]\n",
      "   [  1.5945777   -1.6418726   -0.5148084    1.594585  ]\n",
      "   [  1.7847351   -0.36713108   0.68376136   1.8038828 ]\n",
      "   [  1.5564973    0.14751111   0.38390478   1.6049845 ]\n",
      "   [  1.6079514   -0.09540926   0.7814428    1.6541259 ]\n",
      "   [  0.81602544  -0.8113432    0.67859685   0.80059373]\n",
      "   [  1.6031247    0.62448364   0.6183462    1.6316055 ]]\n",
      "\n",
      "  [[  1.5699102   -4.8338065   -2.8935153    1.5974954 ]\n",
      "   [  1.602976    -6.0585413   -4.3626466    1.6033887 ]\n",
      "   [  0.84273714  -3.3321726   -3.019569     0.8300638 ]\n",
      "   [  1.739568    -5.32123     -4.1527953    1.7057066 ]\n",
      "   [  1.9442569   -4.8756948   -3.3264403    1.9580569 ]\n",
      "   [  3.9294844  -11.821626    -9.099886     3.8940978 ]\n",
      "   [  2.184128    -5.056767    -3.4162076    2.1828146 ]\n",
      "   [  1.9612429   -4.841177    -3.8993757    1.9119158 ]\n",
      "   [  2.1313474   -8.619019    -7.0290985    2.1060083 ]\n",
      "   [  1.7264353   -4.697804    -4.539785     1.657041  ]\n",
      "   [  2.3086565   -7.632138    -5.643827     2.2703083 ]\n",
      "   [  3.4966753   -9.881069    -7.4213333    3.445355  ]\n",
      "   [  2.082511    -6.267626    -4.8032327    2.0366788 ]\n",
      "   [  2.4420376   -7.92061     -6.2677646    2.3764477 ]\n",
      "   [  1.5715002   -2.5319417   -1.8216251    1.5918317 ]\n",
      "   [  1.1295999   -4.316201    -4.391371     1.0374597 ]]\n",
      "\n",
      "  [[ -0.9479617   -1.5584124   -2.3440182   -0.91537315]\n",
      "   [  1.1684959   -6.083402    -5.361126     1.1414931 ]\n",
      "   [  0.8458585   -8.91149     -7.7787905    0.78298134]\n",
      "   [  2.5803494   -5.880795    -5.0921974    2.5411623 ]\n",
      "   [  3.3461082  -11.678779   -10.798813     3.272511  ]\n",
      "   [  3.3453295   -7.0702214   -5.903679     3.2953756 ]\n",
      "   [  1.6342559   -4.6711273   -4.3739734    1.6045712 ]\n",
      "   [  1.7710527   -4.211759    -3.7379825    1.7677531 ]\n",
      "   [  2.3430088   -5.5874043   -4.7693295    2.3498065 ]\n",
      "   [  1.7600528   -4.8194313   -3.658041     1.7406274 ]\n",
      "   [  1.8383347   -4.2881994   -3.964903     1.8433948 ]\n",
      "   [  3.1041694   -6.2948246   -5.899711     3.0869215 ]\n",
      "   [  2.1902268   -8.600747    -8.473362     2.176129  ]\n",
      "   [  2.3792868   -7.585834    -7.168655     2.3181617 ]\n",
      "   [ -0.32808784   3.1995692    2.6395712   -0.32183775]\n",
      "   [  1.6318      -5.232654    -3.7432182    1.642052  ]]\n",
      "\n",
      "  [[ -0.53032047  -2.1487055   -3.3869808   -0.5465881 ]\n",
      "   [ -0.37673792   0.5297353    0.7943683   -0.41485333]\n",
      "   [  1.2794939   -4.2864437   -4.2701783    1.2569122 ]\n",
      "   [  2.427947    -1.600872    -0.90655017   2.481265  ]\n",
      "   [  2.3424263   -7.2077713   -5.824986     2.335523  ]\n",
      "   [  1.7222499   -2.7512228   -2.3087296    1.7864217 ]\n",
      "   [  1.0004832   -7.1413507   -6.550554     0.95411247]\n",
      "   [  1.4302231   -3.8393989   -3.2885814    1.4499868 ]\n",
      "   [  1.734392    -5.0363717   -4.562415     1.7265733 ]\n",
      "   [  1.7437402   -4.61876     -3.9143934    1.71687   ]\n",
      "   [  1.1339672   -3.6319485   -2.8497448    1.1018116 ]\n",
      "   [  1.510794    -1.2551379   -0.05228771   1.5522768 ]\n",
      "   [  1.0197178   -1.2675874   -0.6745764    1.067578  ]\n",
      "   [  0.9115422   -3.4677386   -2.6383572    0.8930571 ]\n",
      "   [  0.81263214  -2.756228    -2.4330876    0.81427544]\n",
      "   [  1.8283294    0.08201662   0.57580274   1.8907442 ]]\n",
      "\n",
      "  [[ -1.8786137    3.8044016    1.7851309   -1.9308672 ]\n",
      "   [ -2.0427704    1.9088362    0.8611902   -2.10597   ]\n",
      "   [  0.3713629   -6.5012736   -5.5806484    0.3149101 ]\n",
      "   [  2.3904674   -0.8018907    0.54941446   2.4430196 ]\n",
      "   [  3.2833803   -5.793328    -3.5530388    3.3556385 ]\n",
      "   [  4.031575    -8.23709     -5.352422     4.1288157 ]\n",
      "   [  1.704372    -4.0030203   -2.8744233    1.746541  ]\n",
      "   [  0.55620587  -0.81578505  -0.7211592    0.5843559 ]\n",
      "   [  1.4359853   -5.1196675   -3.4192846    1.4993091 ]\n",
      "   [  3.066259    -8.724568    -6.3771715    3.138459  ]\n",
      "   [  3.077098    -5.4629254   -3.248803     3.1306965 ]\n",
      "   [  4.6866465   -9.521488    -5.9786706    4.7835746 ]\n",
      "   [  3.749898    -6.577472    -4.5180645    3.8417404 ]\n",
      "   [  5.4376407   -9.574739    -5.771271     5.5562477 ]\n",
      "   [  2.670423    -2.9949458   -1.8302813    2.7224376 ]\n",
      "   [  2.6701472   -3.7526586   -2.0852284    2.7251403 ]]\n",
      "\n",
      "  [[  1.8587611   -2.3004422   -1.999603     1.8540798 ]\n",
      "   [  0.62134135  -1.1685245   -1.0336286    0.63556284]\n",
      "   [  0.93438053  -3.9921112   -3.1974504    0.91454566]\n",
      "   [  2.386284    -8.792911    -6.874719     2.3729997 ]\n",
      "   [  2.8132436  -11.001211    -8.411129     2.7784264 ]\n",
      "   [  2.7941167   -3.5710614   -2.4098222    2.8085887 ]\n",
      "   [  2.5607827   -5.1447167   -4.0192566    2.5128086 ]\n",
      "   [  1.9819213   -4.7589607   -4.3256965    1.9169271 ]\n",
      "   [  2.1862228   -3.8017366   -2.162978     2.196947  ]\n",
      "   [  2.025622    -0.50651217  -0.09534571   2.0341551 ]\n",
      "   [  1.7751181   -5.396942    -3.6041234    1.7526699 ]\n",
      "   [  1.8905165   -2.984126    -2.7361436    1.8944441 ]\n",
      "   [  2.19154     -4.3217793   -3.247157     2.1910028 ]\n",
      "   [  2.6326275   -2.9615233   -1.8003258    2.6587393 ]\n",
      "   [  1.8826354   -1.9406146   -0.6737225    1.887764  ]\n",
      "   [ -0.00523566  -2.99287     -1.9319836   -0.00742362]]\n",
      "\n",
      "  [[  0.263768     1.3027129    0.7912128    0.31364933]\n",
      "   [  0.90733486  -1.2863759   -0.23643312   0.92466456]\n",
      "   [  1.9193958   -5.6561565   -3.624307     1.9176353 ]\n",
      "   [  2.590705    -7.7614427   -5.438415     2.6344485 ]\n",
      "   [  2.0141904   -6.001995    -3.7005918    2.02994   ]\n",
      "   [  2.3413384   -2.5982022   -0.645182     2.369621  ]\n",
      "   [  1.1849254   -0.41514406   0.7534283    1.1835097 ]\n",
      "   [  1.6196365   -2.736624    -1.3294117    1.6487104 ]\n",
      "   [  1.8510432   -2.5615194   -0.7961973    1.8711935 ]\n",
      "   [  2.0705955   -4.3488426   -2.0247169    2.1132114 ]\n",
      "   [  1.9942156   -2.1047823   -0.90358776   2.0466616 ]\n",
      "   [  0.49939027  -1.217362    -0.46714777   0.52691543]\n",
      "   [  1.1221981   -0.761073     0.50912005   1.1667402 ]\n",
      "   [  1.2064869   -1.128277    -0.1816193    1.2461394 ]\n",
      "   [  1.2066578   -2.3339682   -0.79060626   1.220551  ]\n",
      "   [  1.8060701   -2.3906958   -0.6487322    1.8432864 ]]\n",
      "\n",
      "  [[ -0.50981736   4.1422954    3.6196024   -0.43590528]\n",
      "   [  0.4603232   -1.0371166   -0.40937394   0.4394898 ]\n",
      "   [  1.2603666   -3.7127948   -2.6582818    1.2983314 ]\n",
      "   [  1.9421246   -6.2256336   -3.6375484    1.9323288 ]\n",
      "   [  1.3231326   -4.248828    -3.1260052    1.3073777 ]\n",
      "   [  2.442132    -7.4952087   -5.2700887    2.4572377 ]\n",
      "   [  2.4187922   -5.27305     -2.1958756    2.4965396 ]\n",
      "   [  1.0935532   -3.028773    -1.9892881    1.1174632 ]\n",
      "   [  1.07857     -1.6685917   -0.5144004    1.1236442 ]\n",
      "   [  1.9439204   -4.8428583   -2.7191334    2.0016615 ]\n",
      "   [  1.3960849   -4.567839    -2.928855     1.4403856 ]\n",
      "   [  1.6036419   -5.08729     -3.1608024    1.6399496 ]\n",
      "   [  2.1432507   -5.3504367   -3.4768052    2.1538277 ]\n",
      "   [  2.0789034   -8.089037    -6.1146235    2.1081893 ]\n",
      "   [  0.6491808   -1.4989835   -0.8922873    0.6634323 ]\n",
      "   [  0.06960595  -2.5083807   -2.2429636    0.04912842]]]], shape=(1, 8, 16, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 16, 4)\n",
      "output.shape = (1, 8, 16, 32)\n",
      "scaled_attention.shape= (1, 16, 8, 32)\n",
      "concat_attention.shape= (1, 16, 256)\n",
      "outputs.shape= (1, 16, 256)\n",
      "(1, 16, 256)\n",
      "(1, 16, 256)\n",
      "(1, 16, 256)\n",
      "split_heads()\n",
      "(1, 16, 256)\n",
      "(1, 16, 8, 32)\n",
      "split_heads()\n",
      "(1, 16, 256)\n",
      "(1, 16, 8, 32)\n",
      "split_heads()\n",
      "(1, 16, 256)\n",
      "(1, 16, 8, 32)\n",
      "(1, 8, 16, 32)\n",
      "(1, 8, 16, 32)\n",
      "(1, 8, 16, 32)\n",
      "matmul_qk.shape = (1, 8, 16, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7393344  -0.01664181 -0.6147039  ...  1.229891    0.10202239\n",
      "     0.32850313]\n",
      "   [ 3.96568    -0.17723085  1.5590783  ... -6.952222    3.4967818\n",
      "    -7.245028  ]\n",
      "   [ 3.9833748  -2.2539997   0.15140963 ... -1.7606908   0.5195799\n",
      "    -0.7914744 ]\n",
      "   ...\n",
      "   [ 1.616248   -0.41301975  2.1102164  ... -3.1939867   1.1761485\n",
      "    -1.2679567 ]\n",
      "   [ 4.979084   -2.5243933   1.1940266  ... -7.0161867  -2.0503643\n",
      "    -0.09240808]\n",
      "   [-2.7081084  -3.6974242  -3.8014445  ...  5.7798724  -0.5665495\n",
      "    12.004004  ]]\n",
      "\n",
      "  [[-0.16619189 -0.5680489  -0.22415894 ... -1.5257807   0.43077925\n",
      "     0.3584118 ]\n",
      "   [ 0.22428446 -2.402899   -1.7699208  ... -0.9432326  -0.39453703\n",
      "     1.669495  ]\n",
      "   [-0.6712342   0.30168235 -3.0742364  ... -3.5610971  -1.3580014\n",
      "     0.11397357]\n",
      "   ...\n",
      "   [-1.4047577  -1.0068825   0.7469123  ...  2.0891628   0.02782362\n",
      "     2.4152231 ]\n",
      "   [-0.8975079  -0.94379324  1.5547016  ...  1.8971337  -2.2477436\n",
      "     2.8867917 ]\n",
      "   [-0.98076403 -1.2312658   0.5561443  ...  2.0911145   0.03590414\n",
      "    -1.1500138 ]]\n",
      "\n",
      "  [[-3.0174336   0.41961712  1.9252456  ...  2.8171477   0.6974792\n",
      "     0.18334584]\n",
      "   [-1.3549727  -0.09280814  2.5851169  ... -0.67767453 -0.24939446\n",
      "    -1.7602947 ]\n",
      "   [-1.7961591  -1.8925554  -0.41655225 ... -0.21110387 -0.30785158\n",
      "     3.0593605 ]\n",
      "   ...\n",
      "   [-0.9834899  -3.0471325  -5.533581   ...  7.672307    6.376149\n",
      "     5.2377357 ]\n",
      "   [ 0.31654188  1.2054088   4.509779   ... -2.3320508  -2.4442647\n",
      "    -4.720887  ]\n",
      "   [-2.1172123  -1.1903676  -3.6607893  ... -3.5212982   0.89713335\n",
      "    -0.18753654]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.2599558   1.0058122   0.9403136  ... -0.7897071   0.8426995\n",
      "     0.8493423 ]\n",
      "   [ 1.9198732  -0.84344107  2.0494642  ...  0.65555036 -3.7020385\n",
      "    -3.6204383 ]\n",
      "   [ 2.8428354  -1.6724693  -1.4574461  ... -1.5860063  -5.172351\n",
      "    -3.5723574 ]\n",
      "   ...\n",
      "   [ 1.9546982  -0.16918159  4.8530087  ...  1.032651    0.17833906\n",
      "    -2.8946457 ]\n",
      "   [-0.69396204  2.2259943   3.0461872  ...  2.4366105   2.7817082\n",
      "    -0.2656909 ]\n",
      "   [-0.5945187  -1.0852153  -4.3134847  ... -1.4843642   1.9111944\n",
      "     4.291046  ]]\n",
      "\n",
      "  [[-2.1587913   1.7147038   1.544837   ...  1.5435201   1.5954434\n",
      "     2.142618  ]\n",
      "   [-0.00582398 -2.3462925   1.799922   ... -1.2828292  -1.1819105\n",
      "     1.4723727 ]\n",
      "   [-2.1043587  -2.8178024   2.2148285  ...  4.4482775   0.9823559\n",
      "     1.1917044 ]\n",
      "   ...\n",
      "   [ 2.2817585  -0.95722556  3.0716457  ... -2.2234964  -3.2230518\n",
      "    -2.25081   ]\n",
      "   [ 4.4061084   3.3855317  -3.9696262  ... -3.8530157  -0.28425494\n",
      "    -4.5499306 ]\n",
      "   [ 0.55344373  0.814484    0.80500746 ... -1.7593926   0.94361424\n",
      "    -2.2012687 ]]\n",
      "\n",
      "  [[-2.0107505   2.0400176   1.5965837  ...  2.6790552   0.27028453\n",
      "    -0.70449793]\n",
      "   [-1.2814682  -2.6879327   1.4795716  ...  0.5364746  -1.2062161\n",
      "     3.0116005 ]\n",
      "   [ 0.3697977   0.11362547 -0.8682849  ... -0.11516419 -2.3599706\n",
      "    -1.0874932 ]\n",
      "   ...\n",
      "   [-1.0379298  -3.6122782   4.838326   ...  2.0175195   1.247986\n",
      "     1.8743472 ]\n",
      "   [-1.7247764  -2.6890588  -0.49130026 ...  3.6633792  -1.7957431\n",
      "     3.7408998 ]\n",
      "   [-0.1460986   2.392074    0.22694032 ... -0.08168203  3.7521927\n",
      "    -5.572245  ]]]], shape=(1, 8, 16, 16), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 16, 16)\n",
      "output.shape = (1, 8, 16, 32)\n",
      "scaled_attention.shape= (1, 16, 8, 32)\n",
      "concat_attention.shape= (1, 16, 256)\n",
      "outputs.shape= (1, 16, 256)\n",
      "(1, 16, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 16, 256)\n",
      "(1, 16, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 16, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 16, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.51766056   1.96872      2.098977    -0.47190288]\n",
      "   [ -0.49124843   0.08534324   0.2991765   -0.48528743]\n",
      "   [  0.24215932  -4.697081    -4.019181     0.1927161 ]\n",
      "   [  0.7396501   -3.0292149   -2.48159      0.7256521 ]\n",
      "   [  0.7377838   -1.2575825   -1.0456651    0.7396505 ]\n",
      "   [  0.12743154  -2.1013763   -1.7714081    0.15186152]\n",
      "   [  0.57692707  -1.6948001   -1.6039941    0.56792045]\n",
      "   [ -0.04563112  -3.2554927   -2.8839476   -0.07279456]\n",
      "   [  0.541922    -4.2346587   -4.088871     0.5205673 ]\n",
      "   [  0.43123668  -4.4550576   -4.342825     0.38334304]\n",
      "   [  0.2683728   -2.6908388   -3.0562704    0.25416884]\n",
      "   [  0.65151817  -2.3087127   -2.7818563    0.6532232 ]\n",
      "   [  0.7016244   -3.254608    -3.4820378    0.7049006 ]\n",
      "   [  1.2430699   -4.737128    -4.638749     1.2318369 ]\n",
      "   [  0.49773553  -2.68543     -2.2522824    0.4736727 ]\n",
      "   [  1.0115224   -2.5598967   -2.7074509    1.0107176 ]]\n",
      "\n",
      "  [[ -0.5409756    2.819395     2.0520191   -0.5236139 ]\n",
      "   [ -1.0098997    4.226584     3.5640724   -0.9722929 ]\n",
      "   [ -0.31559435  -2.7033126   -2.1731372   -0.34413084]\n",
      "   [  0.67385     -4.3054934   -2.5939085    0.6549963 ]\n",
      "   [  0.7716512   -4.6774607   -3.4361272    0.73228556]\n",
      "   [  0.30699015  -2.0128524   -1.77724      0.3016845 ]\n",
      "   [  0.95172566  -5.123536    -3.7028008    0.9193312 ]\n",
      "   [  0.16256224  -2.8245335   -2.2594569    0.13330695]\n",
      "   [  0.80569774  -2.9156864   -1.9677489    0.81221676]\n",
      "   [  0.5848501   -4.6709986   -3.4369607    0.5637138 ]\n",
      "   [  0.8923553   -4.0675893   -2.657831     0.87549967]\n",
      "   [  1.2629194   -3.7040567   -2.924148     1.2593203 ]\n",
      "   [  1.5493708   -7.248856    -5.396352     1.5176684 ]\n",
      "   [  1.5049651   -6.2729235   -4.4762535    1.4955918 ]\n",
      "   [  0.5231369   -4.513709    -2.8817887    0.49956077]\n",
      "   [  0.974311    -3.9418695   -2.6987228    0.9570487 ]]\n",
      "\n",
      "  [[ -0.42360795   3.468329     3.2733347   -0.39263085]\n",
      "   [ -0.47245097   0.7355631    1.3526613   -0.47366753]\n",
      "   [  0.8764782   -5.054033    -4.6074905    0.832105  ]\n",
      "   [  1.5158468   -5.5036316   -5.460711     1.4665604 ]\n",
      "   [  1.3198522   -4.7758727   -4.838473     1.2815847 ]\n",
      "   [  0.6756115   -5.767558    -5.86507      0.60872716]\n",
      "   [  0.6050416   -6.19143     -6.4390326    0.54084617]\n",
      "   [  0.44500506  -0.68223244  -0.92409056   0.4259432 ]\n",
      "   [  1.465985    -3.2105703   -2.976426     1.4414983 ]\n",
      "   [  1.1924549   -4.355453    -4.300772     1.1541699 ]\n",
      "   [  1.3496063   -1.4938401   -1.1592588    1.3560332 ]\n",
      "   [  0.79303217  -2.173227    -1.8807111    0.78589594]\n",
      "   [  2.1267755   -3.108763    -3.386961     2.123344  ]\n",
      "   [  1.1335939   -4.0095863   -3.878332     1.1041279 ]\n",
      "   [  0.9435391   -4.901097    -4.4360285    0.9156989 ]\n",
      "   [  0.9785473   -4.661531    -4.6272974    0.95410967]]\n",
      "\n",
      "  [[ -1.6453142    4.41468      3.7185767   -1.6388824 ]\n",
      "   [  0.1504475   -1.3770971   -0.84712774   0.10965424]\n",
      "   [  1.5174129   -5.9414515   -4.9204936    1.4784044 ]\n",
      "   [  1.9338356   -4.261786    -3.374979     1.932654  ]\n",
      "   [  1.5312709   -4.5473847   -3.4884474    1.480693  ]\n",
      "   [  2.4686995   -5.566623    -4.674686     2.4672065 ]\n",
      "   [  2.0171244   -5.156834    -4.0178304    1.9925612 ]\n",
      "   [  1.4369009   -3.6673033   -2.722987     1.4106016 ]\n",
      "   [  1.7216051   -4.4552937   -3.1465628    1.6921057 ]\n",
      "   [  1.9970524   -6.690838    -5.398477     1.966113  ]\n",
      "   [  1.44903     -3.9334073   -3.2366369    1.4087826 ]\n",
      "   [  1.3023354   -4.519708    -3.6945662    1.2822565 ]\n",
      "   [  1.6942406   -3.9762886   -3.1610591    1.6870621 ]\n",
      "   [  1.6402903   -3.3537028   -2.8734157    1.6528132 ]\n",
      "   [  1.8542731   -4.373769    -3.7191234    1.8499788 ]\n",
      "   [  3.005128    -7.546983    -6.0248737    2.998044  ]]\n",
      "\n",
      "  [[ -1.29121      2.8940609    1.9502283   -1.3604355 ]\n",
      "   [  0.00294916  -1.2568096   -0.89668256  -0.00931877]\n",
      "   [  0.7530518   -2.5598295   -1.9020394    0.7952453 ]\n",
      "   [  1.7065613   -5.794166    -4.5830455    1.7274754 ]\n",
      "   [  1.828848    -7.046152    -5.713792     1.8388789 ]\n",
      "   [  2.3361118   -9.356349    -7.713717     2.356966  ]\n",
      "   [  2.4712012   -7.777283    -6.1303487    2.5265574 ]\n",
      "   [  2.1786938   -7.176671    -5.9270883    2.2381337 ]\n",
      "   [  2.4477205   -5.1921487   -3.4574718    2.464835  ]\n",
      "   [  2.341771    -5.8448753   -4.0209694    2.4139361 ]\n",
      "   [  1.574666    -4.566143    -4.1088657    1.5959097 ]\n",
      "   [  1.2687012   -5.661861    -4.5558453    1.2785684 ]\n",
      "   [  2.2869492   -8.597368    -7.4518313    2.2985141 ]\n",
      "   [  1.9157221   -8.846792    -8.410634     1.9216846 ]\n",
      "   [  1.6665998   -8.258074    -6.8935685    1.6862932 ]\n",
      "   [  1.7149258   -3.6053843   -3.4942868    1.7738409 ]]\n",
      "\n",
      "  [[ -0.65288585   3.419188     2.6229937   -0.65715   ]\n",
      "   [  0.72896236  -2.1471467   -1.9732635    0.77281165]\n",
      "   [  0.32350188  -1.2035745   -1.7607827    0.34396917]\n",
      "   [  1.0276341    1.058868     1.2416905    1.0519707 ]\n",
      "   [  1.6617185   -1.5399216   -1.5241555    1.7414258 ]\n",
      "   [  2.3202841   -3.4387836   -2.1360145    2.3670495 ]\n",
      "   [  2.2568145   -5.2002926   -3.3399537    2.3179612 ]\n",
      "   [  2.1079552   -4.739115    -2.963286     2.151914  ]\n",
      "   [  1.2009804   -1.6315452   -1.1646751    1.2423136 ]\n",
      "   [  1.612212    -4.7317686   -3.3088357    1.6307019 ]\n",
      "   [  0.8388549   -2.1863418   -1.2897081    0.847988  ]\n",
      "   [  1.036908    -3.0587974   -2.584841     1.0773487 ]\n",
      "   [  1.8100612   -3.9532237   -3.6608818    1.8912138 ]\n",
      "   [  1.5701225    0.33523917   0.4572183    1.6209648 ]\n",
      "   [  0.7436306   -3.9707983   -3.7097435    0.7386792 ]\n",
      "   [ -0.2926402    4.188444     3.4274511   -0.32940495]]\n",
      "\n",
      "  [[ -0.13920635   1.9202758    1.8218281   -0.10183363]\n",
      "   [  0.19900118  -1.852921    -1.364047     0.17529728]\n",
      "   [  0.9201308   -2.3837185   -1.2430844    0.8900646 ]\n",
      "   [  1.0384122   -2.2614129   -0.6760908    1.014745  ]\n",
      "   [  1.3033148   -4.046018    -2.5787446    1.2661915 ]\n",
      "   [  1.1603479   -2.2507179   -1.0619235    1.1338618 ]\n",
      "   [  1.041217    -4.2461724   -2.9057727    1.0158387 ]\n",
      "   [  1.0148015   -1.7259315   -0.37150636   1.0063081 ]\n",
      "   [  0.88598007  -2.206053    -0.84811103   0.8885484 ]\n",
      "   [  1.2307554   -3.4753895   -2.1146924    1.1983802 ]\n",
      "   [  0.5709063   -1.6979381   -1.0712341    0.5441556 ]\n",
      "   [  0.938992    -2.1750786   -1.6815557    0.93939686]\n",
      "   [  0.9054085   -3.5624816   -3.3321881    0.87981725]\n",
      "   [  0.78987503  -2.7473283   -1.3229544    0.7772658 ]\n",
      "   [  0.8431015    1.9753726    2.62057      0.83691   ]\n",
      "   [  0.4921985   -0.82260525  -0.27484074   0.49805784]]\n",
      "\n",
      "  [[ -1.5968649    5.382058     4.5500836   -1.5562288 ]\n",
      "   [  0.86753845  -4.5697775   -4.1758018    0.8370436 ]\n",
      "   [  1.9240823   -9.450348    -8.618066     1.8469068 ]\n",
      "   [  1.4563193   -6.307143    -5.9952025    1.4192016 ]\n",
      "   [  1.9546863   -5.530293    -5.110774     1.9280047 ]\n",
      "   [  1.7568917   -6.1661077   -5.8179903    1.6855166 ]\n",
      "   [  3.4059234  -12.176341   -11.224475     3.3215392 ]\n",
      "   [  1.7747598   -8.078675    -7.3566995    1.7220771 ]\n",
      "   [  2.41111     -7.0833406   -6.4738836    2.3771992 ]\n",
      "   [  2.242727    -8.106706    -7.0536833    2.1916888 ]\n",
      "   [  0.5262304   -3.2455208   -2.7358432    0.4870345 ]\n",
      "   [  0.8685509   -1.3043313   -0.5787912    0.8402956 ]\n",
      "   [  2.91314     -6.7567825   -5.547529     2.8893337 ]\n",
      "   [  1.6566273   -6.555101    -6.1553826    1.6073426 ]\n",
      "   [  2.781743    -9.328011    -8.780681     2.7215238 ]\n",
      "   [  0.9914804   -4.36916     -4.6807566    0.97934586]]]], shape=(1, 8, 16, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 16, 4)\n",
      "output.shape = (1, 8, 16, 32)\n",
      "scaled_attention.shape= (1, 16, 8, 32)\n",
      "concat_attention.shape= (1, 16, 256)\n",
      "outputs.shape= (1, 16, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   2.3561337   1.5915804  -4.1808095 ]\n",
      "   [-5.6385536   4.4107776   0.17257608 -6.9278107 ]\n",
      "   [-5.6089234   0.5278147  -3.2366285  -6.488331  ]\n",
      "   [-2.6520565   3.2424057   4.5023174  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    2.2785926   0.21166308 -5.8514013 ]\n",
      "   [-1.1092676  -0.18028086 -1.7366805  -0.6944961 ]\n",
      "   [-6.485458    0.20626803 -7.206355   -6.201642  ]\n",
      "   [-3.889495    3.7722502   1.8531445  -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  2.6389184   1.8771871  -1.4663465 ]\n",
      "   [-6.229604    3.4806194   4.151268   -5.901417  ]\n",
      "   [-2.453914    2.6220567  -0.153227   -3.2771468 ]\n",
      "   [-3.4774246   5.043724    1.335897   -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183 -0.14562775  1.278508   -1.0099144 ]\n",
      "   [-5.3702993   1.5430654  -1.7044661  -4.7386603 ]\n",
      "   [-5.5109906   1.54136     0.36428282 -5.720249  ]\n",
      "   [-1.8414911   0.912555    1.2957357  -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    2.7316198   0.23491785 -1.7994132 ]\n",
      "   [ 0.23102142 -1.8337682   1.3903457  -0.59883434]\n",
      "   [-4.200921    3.6330864  -2.1295938  -3.8921409 ]\n",
      "   [ 0.2971933  -0.7295645   2.5733593   2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.21287501  0.06264099 -2.5024278 ]\n",
      "   [-1.3814096   1.651177    0.81836665 -2.5697043 ]\n",
      "   [-3.4397511   4.5554137  -3.1124666  -3.6323988 ]\n",
      "   [-1.399356    1.0871059   0.49319047 -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.78072286 -1.1751877  -3.0664551 ]\n",
      "   [-5.346867   -0.39898908 -1.9612279  -6.311541  ]\n",
      "   [-2.4324968   1.637576    0.58495444 -2.7039783 ]\n",
      "   [-2.1759007   2.7337348  -2.1159217  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158   0.35253465 -3.6371818  -2.1313064 ]\n",
      "   [-6.520774    1.4926573   1.5314732  -9.039265  ]\n",
      "   [-4.718839    2.0809498  -4.091497   -5.7374787 ]\n",
      "   [-4.791121    1.4460874  -2.3672192  -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  1.7591333    5.0243936    3.5544546    2.6399336 ]\n",
      "   [ -4.253851     0.84035844  -0.21777306  -4.804449  ]\n",
      "   [ -0.15435873   0.5058999    0.47399068  -0.29995087]\n",
      "   [  0.6791757    3.8417208    2.8333614    1.484463  ]]\n",
      "\n",
      "  [[ -1.2329528    1.6295599   -0.7721219   -2.603114  ]\n",
      "   [ -4.3549094    2.46123      2.5575788   -4.4672627 ]\n",
      "   [ -7.321539     1.1913186    0.96821874  -6.5926433 ]\n",
      "   [ -3.4320796    1.4416587   -1.4664403   -4.591977  ]]\n",
      "\n",
      "  [[ -2.825953     9.073873     5.4353266   -1.9312263 ]\n",
      "   [ -4.668066     1.9637846    0.55493575  -3.1404896 ]\n",
      "   [ -7.8387685    3.1339467    1.06559     -6.1151247 ]\n",
      "   [ -4.6222258    9.124773     5.114823    -3.1526585 ]]\n",
      "\n",
      "  [[  1.819402     0.35315254  -0.21079886   2.736524  ]\n",
      "   [ -9.943851     1.7468896   -1.9401404   -9.102351  ]\n",
      "   [ -7.2699304    2.3089426   -1.2441256   -6.761486  ]\n",
      "   [  0.356722     0.01084253  -0.7567232    1.265906  ]]\n",
      "\n",
      "  [[ 10.832494    -2.3024282   -1.5900779   10.103639  ]\n",
      "   [ -4.9642835    3.0035746   -0.5549055   -5.878555  ]\n",
      "   [ -2.868262     4.303177     0.8180887   -3.5337753 ]\n",
      "   [  4.7653503   -0.3293899   -1.2949134    4.089335  ]]\n",
      "\n",
      "  [[  9.403561    -2.8651876   -0.632884     7.9069276 ]\n",
      "   [ -9.417637     3.5659125    4.728585    -9.294713  ]\n",
      "   [-11.022017     3.1879566    2.7434673  -10.792371  ]\n",
      "   [  6.368421    -1.8694576   -0.20582469   4.6402516 ]]\n",
      "\n",
      "  [[  0.8164711   -0.28761575   0.95695895  -0.36852825]\n",
      "   [ -3.0110312    0.43877918  -1.2730507   -2.6316638 ]\n",
      "   [  0.1665625    1.1675574   -0.15077221   0.2803448 ]\n",
      "   [ -0.01723745   0.6380685    0.90366644  -0.7035069 ]]\n",
      "\n",
      "  [[ 12.470646     3.0128129    7.805017    13.292262  ]\n",
      "   [-11.662636     5.339761     1.5164928  -11.328455  ]\n",
      "   [ -6.6453133    5.8509235    3.0643861   -6.339464  ]\n",
      "   [ 12.59473      3.8915303    8.243019    13.169466  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 17, 256)\n",
      "(1, 17, 256)\n",
      "(1, 17, 256)\n",
      "split_heads()\n",
      "(1, 17, 256)\n",
      "(1, 17, 8, 32)\n",
      "split_heads()\n",
      "(1, 17, 256)\n",
      "(1, 17, 8, 32)\n",
      "split_heads()\n",
      "(1, 17, 256)\n",
      "(1, 17, 8, 32)\n",
      "(1, 8, 17, 32)\n",
      "(1, 8, 17, 32)\n",
      "(1, 8, 17, 32)\n",
      "matmul_qk.shape = (1, 8, 17, 17)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.19671637   0.34482306   0.3514983  ...   0.63462985\n",
      "      0.7864368    1.5851709 ]\n",
      "   [ -1.5522299    0.5752962    1.3420837  ...   2.5783165\n",
      "      2.9059484    4.877658  ]\n",
      "   [  0.17652513  -2.160497    -3.1749377  ...  -3.3766246\n",
      "      0.92398924  -1.3026559 ]\n",
      "   ...\n",
      "   [ -0.9266528    3.6381373    0.9809236  ...   1.1503685\n",
      "      3.603367    14.004007  ]\n",
      "   [ -1.3367797    2.0607495    2.5556931  ...   1.3782963\n",
      "      1.4865342   -1.5133239 ]\n",
      "   [  0.8665007   -4.3527093   -2.13481    ...   1.8332657\n",
      "     -3.9863017   22.027906  ]]\n",
      "\n",
      "  [[ -0.1123208    0.09083703  -0.2228432  ...  -0.22318053\n",
      "      0.17074372  -0.47391328]\n",
      "   [ -0.2778076   -7.6848054   -0.20160444 ...  -1.3000827\n",
      "      4.5543838   -1.9994831 ]\n",
      "   [  0.3045806   -1.6718075   -3.0954444  ...   1.2523042\n",
      "      0.26003852   3.4394028 ]\n",
      "   ...\n",
      "   [ -0.41793326  -3.8543766   -0.59011656 ...   2.4199317\n",
      "      4.730896     0.05887055]\n",
      "   [  0.06543481   3.6409285    2.631873   ...  -5.728239\n",
      "     -4.9387393  -15.372478  ]\n",
      "   [ -0.37056175  -4.6624      -8.662745   ...   7.8117504\n",
      "      8.470594    53.23471   ]]\n",
      "\n",
      "  [[ -0.10918298  -0.21078415   0.35148743 ...   0.43691918\n",
      "      0.09502384   0.32923216]\n",
      "   [  0.286177    -5.063078    -2.1693296  ...  -1.3468394\n",
      "     -0.7408603  -13.540648  ]\n",
      "   [  0.6062929    2.71917     -4.958571   ...  -2.6990194\n",
      "      0.5639981    1.9078724 ]\n",
      "   ...\n",
      "   [ -0.27214354  -0.83239764  -2.8551834  ...  -6.0741205\n",
      "      0.79248315   9.3965845 ]\n",
      "   [ -0.08132678   0.00047862  -1.5826722  ...   4.590625\n",
      "      0.5341264   -3.5352798 ]\n",
      "   [  0.01435987  -2.5880122    5.6674423  ...   0.5182616\n",
      "     -4.172111    -5.1115313 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ -0.00998615  -0.51713806   0.3842013  ...  -0.25496337\n",
      "     -0.22296515   0.41675955]\n",
      "   [  0.3873545    2.3078725   -2.065676   ...   1.1633202\n",
      "     -1.387728     0.37335625]\n",
      "   [  0.5493823    0.90148294  -5.7177277  ...   0.43117315\n",
      "      1.0218422   -5.112863  ]\n",
      "   ...\n",
      "   [  0.41854608   2.3568523   -3.883304   ...  -2.8876584\n",
      "     -2.0006099    2.5869894 ]\n",
      "   [  1.6032448   -3.0856204   -1.7495589  ...  -8.953124\n",
      "     -6.456024    -6.532952  ]\n",
      "   [ -0.45874822   7.199579     3.7425995  ...   5.2210035\n",
      "     -5.4129696   15.51831   ]]\n",
      "\n",
      "  [[ -0.20864005   0.00578878   0.14192149 ...  -0.21370879\n",
      "     -0.19716224   0.43419233]\n",
      "   [ -0.5008973   -5.8371863    3.0893483  ...  -1.5423415\n",
      "     -1.1735877   -3.5173879 ]\n",
      "   [ -0.0856532    2.1096904    0.02049009 ...   1.6681659\n",
      "      0.02906624   7.168967  ]\n",
      "   ...\n",
      "   [  0.2998401    0.06822612   1.1627786  ...  -6.324023\n",
      "     -4.9028425    8.204871  ]\n",
      "   [ -1.3585377    3.6008828   -2.2374058  ...  -2.4548883\n",
      "      0.89259416  -1.1186364 ]\n",
      "   [  0.5167517    0.25727043 -11.06195    ... -10.382974\n",
      "     -4.7903423   38.343204  ]]\n",
      "\n",
      "  [[ -0.15726809   0.3433595   -0.01051011 ...  -1.0173951\n",
      "     -0.04488361   0.4671244 ]\n",
      "   [  0.42739105  -3.5416267   -4.4160037  ...   3.8917248\n",
      "      0.18567929   9.017501  ]\n",
      "   [  0.09385623   3.199941    -4.3982935  ...   1.202934\n",
      "      1.5592904    4.5362062 ]\n",
      "   ...\n",
      "   [ -0.19819802   1.7829542   -3.087674   ...   0.905274\n",
      "     -0.08052139  10.6741495 ]\n",
      "   [  0.0363619   -0.24117607   2.3942986  ...  -0.81050795\n",
      "      1.5444627    5.73031   ]\n",
      "   [  0.56675273  -4.755873     0.81196666 ...   1.6517584\n",
      "     -4.8414054   34.972713  ]]]], shape=(1, 8, 17, 17), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 17, 17)\n",
      "output.shape = (1, 8, 17, 32)\n",
      "scaled_attention.shape= (1, 17, 8, 32)\n",
      "concat_attention.shape= (1, 17, 256)\n",
      "outputs.shape= (1, 17, 256)\n",
      "(1, 17, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 17, 256)\n",
      "(1, 17, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 17, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 17, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.6509341   -2.8663867   -1.9697639    0.62575525]\n",
      "   [ -1.2757046   -0.66076183  -0.7730705   -1.2926762 ]\n",
      "   [  1.007679    -3.2964513   -3.107028     0.9874557 ]\n",
      "   [  1.257767    -4.880912    -3.7672884    1.2468473 ]\n",
      "   [  1.3234034   -5.933906    -5.236538     1.2869945 ]\n",
      "   [  1.6999732   -3.6600497   -2.972112     1.7177101 ]\n",
      "   [  1.2773378   -1.9755442   -1.5194309    1.2972957 ]\n",
      "   [  0.58320224  -1.1661532   -0.5394834    0.5777378 ]\n",
      "   [  0.7598261   -0.7145823    0.06813058   0.76447296]\n",
      "   [  0.68805677   0.4064954    1.121047     0.7195429 ]\n",
      "   [  1.5937982   -1.6412116   -0.51412374   1.5939636 ]\n",
      "   [  1.7841249   -0.3671943    0.68450624   1.8033417 ]\n",
      "   [  1.5564657    0.147776     0.383552     1.6050508 ]\n",
      "   [  1.6078193   -0.09543311   0.78151447   1.6540873 ]\n",
      "   [  0.8161015   -0.8142703    0.6762436    0.80070364]\n",
      "   [  1.6028978    0.6241045    0.6178138    1.6314983 ]\n",
      "   [  2.3350527   -2.0657768   -1.909737     2.3493774 ]]\n",
      "\n",
      "  [[  1.570225    -4.8342686   -2.8939598    1.5975435 ]\n",
      "   [  1.6036134   -6.060514    -4.3644085    1.6040113 ]\n",
      "   [  0.8431237   -3.3325617   -3.0198817    0.8305527 ]\n",
      "   [  1.7396799   -5.322756    -4.1539907    1.7058723 ]\n",
      "   [  1.9448967   -4.8777757   -3.328427     1.958759  ]\n",
      "   [  3.9301667  -11.82411     -9.102355     3.8946667 ]\n",
      "   [  2.184354    -5.057787    -3.417454     2.1830084 ]\n",
      "   [  1.9611686   -4.8410335   -3.8996708    1.9117274 ]\n",
      "   [  2.1313841   -8.618214    -7.0292373    2.1061475 ]\n",
      "   [  1.7266142   -4.6987495   -4.5415616    1.6571113 ]\n",
      "   [  2.3085299   -7.6317096   -5.643262     2.2699904 ]\n",
      "   [  3.4971008   -9.882628    -7.4232836    3.4457142 ]\n",
      "   [  2.0830445   -6.2693915   -4.804928     2.0370524 ]\n",
      "   [  2.4425027   -7.923333    -6.2704573    2.376838  ]\n",
      "   [  1.5717918   -2.5327952   -1.822882     1.5921091 ]\n",
      "   [  1.1297456   -4.317345    -4.392541     1.0376523 ]\n",
      "   [  2.1390867   -7.477202    -5.558408     2.1009784 ]]\n",
      "\n",
      "  [[ -0.9479089   -1.5591385   -2.3436334   -0.91542625]\n",
      "   [  1.1684781   -6.0844707   -5.3619742    1.1413976 ]\n",
      "   [  0.84597003  -8.912725    -7.779404     0.7830915 ]\n",
      "   [  2.5803902   -5.8799777   -5.0920033    2.5409443 ]\n",
      "   [  3.3460186  -11.679185   -10.799364     3.2720766 ]\n",
      "   [  3.3453512   -7.069258    -5.9026856    3.295044  ]\n",
      "   [  1.6344386   -4.6716905   -4.374638     1.604553  ]\n",
      "   [  1.7713587   -4.2145295   -3.7401123    1.7679971 ]\n",
      "   [  2.342803    -5.587083    -4.76949      2.349284  ]\n",
      "   [  1.759695    -4.8193383   -3.6592824    1.7401707 ]\n",
      "   [  1.838409    -4.2896113   -3.9665303    1.8431941 ]\n",
      "   [  3.1046357   -6.2964425   -5.9015503    3.0870106 ]\n",
      "   [  2.1900115   -8.600579    -8.473611     2.1757395 ]\n",
      "   [  2.379572    -7.5867734   -7.169527     2.3181841 ]\n",
      "   [ -0.3277089    3.199168     2.6390367   -0.3214312 ]\n",
      "   [  1.6319063   -5.2350683   -3.7456758    1.6421753 ]\n",
      "   [  2.5742397   -8.027292    -7.208523     2.560471  ]]\n",
      "\n",
      "  [[ -0.5304257   -2.1488113   -3.3874593   -0.546707  ]\n",
      "   [ -0.37675658   0.52872586   0.7934703   -0.41478717]\n",
      "   [  1.2796872   -4.288391    -4.270704     1.257305  ]\n",
      "   [  2.4282255   -1.6019804   -0.90599734   2.4815636 ]\n",
      "   [  2.3426452   -7.2078457   -5.824685     2.3358805 ]\n",
      "   [  1.7222534   -2.7507093   -2.3081126    1.7865363 ]\n",
      "   [  1.0004134   -7.141636    -6.550596     0.9541852 ]\n",
      "   [  1.4302897   -3.839967    -3.2890687    1.4501208 ]\n",
      "   [  1.7340746   -5.036048    -4.5616536    1.7262698 ]\n",
      "   [  1.7433815   -4.618922    -3.9141095    1.7165753 ]\n",
      "   [  1.1336113   -3.6328945   -2.850212     1.1015515 ]\n",
      "   [  1.5105101   -1.2560863   -0.05357185   1.5519286 ]\n",
      "   [  1.0196968   -1.2664933   -0.6737771    1.0675111 ]\n",
      "   [  0.9113641   -3.4672148   -2.6378303    0.8929656 ]\n",
      "   [  0.8128266   -2.7580578   -2.4343321    0.81456846]\n",
      "   [  1.8281659    0.081753     0.57598466   1.8904312 ]\n",
      "   [  1.9026543   -4.325554    -3.7610524    1.9181205 ]]\n",
      "\n",
      "  [[ -1.8784097    3.8037248    1.7850332   -1.9305832 ]\n",
      "   [ -2.0425584    1.9095554    0.8624998   -2.105986  ]\n",
      "   [  0.37148342  -6.5013785   -5.5806413    0.31497884]\n",
      "   [  2.3909469   -0.8024464    0.5490368    2.4436378 ]\n",
      "   [  3.284259    -5.7949805   -3.5545163    3.356937  ]\n",
      "   [  4.031004    -8.23513     -5.3514233    4.1286025 ]\n",
      "   [  1.7040174   -4.0020256   -2.874205     1.7463645 ]\n",
      "   [  0.55629456  -0.81676006  -0.7216789    0.584451  ]\n",
      "   [  1.4354165   -5.118435    -3.419127     1.4990029 ]\n",
      "   [  3.0657308   -8.725056    -6.3781705    3.1381054 ]\n",
      "   [  3.0761838   -5.462584    -3.2489183    3.1300673 ]\n",
      "   [  4.6867423   -9.521186    -5.979313     4.783975  ]\n",
      "   [  3.749679    -6.5771866   -4.5184355    3.8416162 ]\n",
      "   [  5.4381175   -9.575569    -5.7726583    5.556939  ]\n",
      "   [  2.6700032   -2.9949791   -1.8304294    2.7221699 ]\n",
      "   [  2.6695762   -3.7535443   -2.0863273    2.7247272 ]\n",
      "   [  3.359904    -5.115135    -3.3822649    3.4269407 ]]\n",
      "\n",
      "  [[  1.8586155   -2.3001416   -1.999936     1.8540497 ]\n",
      "   [  0.6211309   -1.168661    -1.0340286    0.6352237 ]\n",
      "   [  0.9347803   -3.9932907   -3.1982765    0.9149274 ]\n",
      "   [  2.3863075   -8.794712    -6.875748     2.3731115 ]\n",
      "   [  2.8134053  -11.004837    -8.413596     2.7787576 ]\n",
      "   [  2.7943425   -3.571689    -2.409852     2.8088717 ]\n",
      "   [  2.5605876   -5.144224    -4.018193     2.51266   ]\n",
      "   [  1.981437    -4.758584    -4.325799     1.9165455 ]\n",
      "   [  2.1863608   -3.8024008   -2.163648     2.1971169 ]\n",
      "   [  2.0255754   -0.506863    -0.09594135   2.0340822 ]\n",
      "   [  1.7748458   -5.39665     -3.6042917    1.7524707 ]\n",
      "   [  1.8900745   -2.9841158   -2.7367141    1.894051  ]\n",
      "   [  2.1917036   -4.323266    -3.2479947    2.1912763 ]\n",
      "   [  2.632459    -2.9628072   -1.8018532    2.6585062 ]\n",
      "   [  1.8824352   -1.9412858   -0.673442     1.8875338 ]\n",
      "   [ -0.00533165  -2.9928493   -1.9318286   -0.00753942]\n",
      "   [  2.8046033   -6.9947705   -5.2836866    2.781471  ]]\n",
      "\n",
      "  [[  0.26394355   1.3016005    0.7910604    0.31375277]\n",
      "   [  0.90737504  -1.2866342   -0.23612899   0.92492515]\n",
      "   [  1.9194956   -5.6570635   -3.6252735    1.9178153 ]\n",
      "   [  2.5911002   -7.7615128   -5.4392796    2.6350758 ]\n",
      "   [  2.014172    -6.002149    -3.7011626    2.0300543 ]\n",
      "   [  2.3412988   -2.5975013   -0.6459293    2.3696783 ]\n",
      "   [  1.1848012   -0.41397178   0.7540367    1.1833496 ]\n",
      "   [  1.619462    -2.7361822   -1.3296059    1.6485598 ]\n",
      "   [  1.8506598   -2.5613196   -0.7968157    1.871102  ]\n",
      "   [  2.0705013   -4.3481855   -2.0246627    2.1132836 ]\n",
      "   [  1.9937475   -2.1027558   -0.9025043    2.046349  ]\n",
      "   [  0.49965474  -1.2175167   -0.46714982   0.5272196 ]\n",
      "   [  1.1222295   -0.7609615    0.50896525   1.1668125 ]\n",
      "   [  1.206437    -1.1279833   -0.18234965   1.2462423 ]\n",
      "   [  1.2066667   -2.3342962   -0.79164404   1.2208124 ]\n",
      "   [  1.8061911   -2.3893802   -0.64825076   1.8436785 ]\n",
      "   [  2.5374002   -3.8597028   -1.6043795    2.6163301 ]]\n",
      "\n",
      "  [[ -0.5098043    4.14234      3.618752    -0.43597907]\n",
      "   [  0.46051964  -1.0368892   -0.4086982    0.43977737]\n",
      "   [  1.2603859   -3.7128255   -2.6576352    1.2985009 ]\n",
      "   [  1.9420563   -6.2268205   -3.6372137    1.9324867 ]\n",
      "   [  1.3229225   -4.2489123   -3.1254654    1.3073956 ]\n",
      "   [  2.44232     -7.496988    -5.270363     2.4578779 ]\n",
      "   [  2.4189994   -5.2743196   -2.1956003    2.4972231 ]\n",
      "   [  1.0933077   -3.0281217   -1.9889832    1.1175193 ]\n",
      "   [  1.0782777   -1.6670517   -0.5128577    1.1236109 ]\n",
      "   [  1.9434991   -4.842499    -2.7185435    2.0016136 ]\n",
      "   [  1.3962588   -4.5670314   -2.9270706    1.4408625 ]\n",
      "   [  1.6038973   -5.087473    -3.159885     1.6405536 ]\n",
      "   [  2.1434639   -5.3521223   -3.4776533    2.1544127 ]\n",
      "   [  2.0786924   -8.08981     -6.1141233    2.10833   ]\n",
      "   [  0.6492189   -1.4987669   -0.89203614   0.6635584 ]\n",
      "   [  0.06925935  -2.508758    -2.2429883    0.04873868]\n",
      "   [  1.9322278   -5.712201    -3.439236     1.9518732 ]]]], shape=(1, 8, 17, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 17, 4)\n",
      "output.shape = (1, 8, 17, 32)\n",
      "scaled_attention.shape= (1, 17, 8, 32)\n",
      "concat_attention.shape= (1, 17, 256)\n",
      "outputs.shape= (1, 17, 256)\n",
      "(1, 17, 256)\n",
      "(1, 17, 256)\n",
      "(1, 17, 256)\n",
      "split_heads()\n",
      "(1, 17, 256)\n",
      "(1, 17, 8, 32)\n",
      "split_heads()\n",
      "(1, 17, 256)\n",
      "(1, 17, 8, 32)\n",
      "split_heads()\n",
      "(1, 17, 256)\n",
      "(1, 17, 8, 32)\n",
      "(1, 8, 17, 32)\n",
      "(1, 8, 17, 32)\n",
      "(1, 8, 17, 32)\n",
      "matmul_qk.shape = (1, 8, 17, 17)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.73953164 -0.01649685 -0.6147653  ...  0.10194889  0.32869038\n",
      "    -0.81270885]\n",
      "   [ 3.966023   -0.17683832  1.5592284  ...  3.4982185  -7.2460036\n",
      "    -2.9208345 ]\n",
      "   [ 3.9825613  -2.253869    0.1516434  ...  0.52046317 -0.7912604\n",
      "    -3.4434154 ]\n",
      "   ...\n",
      "   [ 4.979938   -2.5236788   1.1948196  ... -2.050152   -0.0937802\n",
      "     0.87908787]\n",
      "   [-2.7115073  -3.6988733  -3.8031898  ... -0.5643215  12.008472\n",
      "     8.2643795 ]\n",
      "   [ 1.7736529  -1.4358691   0.11265315 ... -0.40124133  2.4295907\n",
      "     1.290543  ]]\n",
      "\n",
      "  [[-0.16615649 -0.56778115 -0.22402702 ...  0.43095836  0.35877115\n",
      "     1.6052579 ]\n",
      "   [ 0.22389287 -2.4020274  -1.7713758  ... -0.3943916   1.6697717\n",
      "     2.5715322 ]\n",
      "   [-0.670969    0.30067873 -3.0741427  ... -1.3581173   0.11301152\n",
      "     0.54858077]\n",
      "   ...\n",
      "   [-0.89694583 -0.9434658   1.5539944  ... -2.2485917   2.8864055\n",
      "     5.817725  ]\n",
      "   [-0.98098075 -1.2310821   0.55733275 ...  0.03587614 -1.148888\n",
      "     2.6698592 ]\n",
      "   [ 0.9393775  -0.68170714  2.0413072  ... -0.42767435 -0.9522181\n",
      "     2.385446  ]]\n",
      "\n",
      "  [[-3.0173438   0.41938055  1.9250286  ...  0.69734395  0.18341348\n",
      "     1.7638128 ]\n",
      "   [-1.3548342  -0.09341108  2.5825107  ... -0.24945371 -1.7589902\n",
      "     2.2673302 ]\n",
      "   [-1.7964395  -1.8909937  -0.41699317 ... -0.3071459   3.0587823\n",
      "     3.467188  ]\n",
      "   ...\n",
      "   [ 0.31588843  1.2057723   4.510305   ... -2.4437263  -4.7190456\n",
      "    -3.8000429 ]\n",
      "   [-2.1184735  -1.1881613  -3.6593199  ...  0.89781064 -0.18667474\n",
      "     3.4597921 ]\n",
      "   [ 1.8741469  -1.1795415  -4.952984   ...  2.6821      2.18865\n",
      "     5.0786157 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.259958    1.00564     0.9402989  ...  0.8426279   0.8489601\n",
      "    -0.10900882]\n",
      "   [ 1.9210991  -0.845608    2.0491595  ... -3.7022736  -3.6209347\n",
      "    -1.6157334 ]\n",
      "   [ 2.843176   -1.6733516  -1.4584364  ... -5.174728   -3.5717819\n",
      "    -2.8171425 ]\n",
      "   ...\n",
      "   [-0.6935651   2.2262654   3.0456855  ...  2.7811723  -0.2661739\n",
      "     2.47465   ]\n",
      "   [-0.5953044  -1.0862712  -4.314466   ...  1.9120532   4.293508\n",
      "     0.0256958 ]\n",
      "   [ 1.1308458  -1.2022599   2.476754   ... -1.465692    0.8943934\n",
      "    -1.704601  ]]\n",
      "\n",
      "  [[-2.1592882   1.7141193   1.5454057  ...  1.5957707   2.142307\n",
      "     2.0096903 ]\n",
      "   [-0.00584073 -2.3464692   1.7997947  ... -1.1821603   1.4722497\n",
      "    -0.44859087]\n",
      "   [-2.104566   -2.8171358   2.213256   ...  0.9827141   1.1911734\n",
      "     5.502022  ]\n",
      "   ...\n",
      "   [ 4.4084864   3.3848937  -3.970095   ... -0.28640327 -4.5506616\n",
      "    -5.1742215 ]\n",
      "   [ 0.55328256  0.8135445   0.8052426  ...  0.9432078  -2.2010915\n",
      "    -1.0764647 ]\n",
      "   [ 3.2146964   1.9936249  -1.5540563  ...  0.30661103 -5.311873\n",
      "    -3.6692865 ]]\n",
      "\n",
      "  [[-2.010828    2.0398128   1.5960796  ...  0.27106848 -0.7035367\n",
      "     1.4634386 ]\n",
      "   [-1.2813582  -2.6896367   1.4788102  ... -1.2070245   3.012765\n",
      "     0.7203002 ]\n",
      "   [ 0.3697054   0.1141834  -0.86882013 ... -2.3611622  -1.0875418\n",
      "    -0.4729183 ]\n",
      "   ...\n",
      "   [-1.7246624  -2.6908042  -0.49430478 ... -1.7967453   3.7391303\n",
      "     5.3083115 ]\n",
      "   [-0.14671737  2.3917654   0.22554515 ...  3.753012   -5.570538\n",
      "     0.23849057]\n",
      "   [ 1.7132952  -3.342785   -3.2370853  ...  0.0361472  -2.315074\n",
      "     1.6207131 ]]]], shape=(1, 8, 17, 17), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 17, 17)\n",
      "output.shape = (1, 8, 17, 32)\n",
      "scaled_attention.shape= (1, 17, 8, 32)\n",
      "concat_attention.shape= (1, 17, 256)\n",
      "outputs.shape= (1, 17, 256)\n",
      "(1, 17, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 17, 256)\n",
      "(1, 17, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 17, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 17, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.51773053   1.9684135    2.0989788   -0.47191814]\n",
      "   [ -0.49109155   0.08540867   0.29941544  -0.48513055]\n",
      "   [  0.24224533  -4.6960793   -4.0174685    0.1929091 ]\n",
      "   [  0.7395457   -3.0274627   -2.4793613    0.7255292 ]\n",
      "   [  0.7378413   -1.2579615   -1.0455656    0.7397156 ]\n",
      "   [  0.12752059  -2.1018803   -1.7717991    0.15204106]\n",
      "   [  0.5766347   -1.6949204   -1.6035248    0.5676175 ]\n",
      "   [ -0.04577393  -3.255752    -2.8843288   -0.07289273]\n",
      "   [  0.5424079   -4.2355266   -4.0886626    0.5209401 ]\n",
      "   [  0.43164253  -4.4558115   -4.3431106    0.38374498]\n",
      "   [  0.26851782  -2.6915414   -3.0564966    0.25433606]\n",
      "   [  0.6517925   -2.3090534   -2.7821772    0.65342623]\n",
      "   [  0.70163107  -3.2546315   -3.4806798    0.7048456 ]\n",
      "   [  1.2431912   -4.738473    -4.6395354    1.2318251 ]\n",
      "   [  0.49777898  -2.6868064   -2.2525272    0.47366527]\n",
      "   [  1.0114245   -2.5588958   -2.7066724    1.0103859 ]\n",
      "   [  1.9425274   -1.5525792   -1.7490975    1.9740599 ]]\n",
      "\n",
      "  [[ -0.5409839    2.820436     2.0525382   -0.5237764 ]\n",
      "   [ -1.0096651    4.2267857    3.5646887   -0.9722638 ]\n",
      "   [ -0.31569007  -2.7028654   -2.1727977   -0.34412804]\n",
      "   [  0.6734595   -4.3046517   -2.5929103    0.654736  ]\n",
      "   [  0.7717248   -4.6770797   -3.4358013    0.73251957]\n",
      "   [  0.3074527   -2.013344    -1.7778394    0.3021317 ]\n",
      "   [  0.95138294  -5.1220384   -3.7013133    0.9192492 ]\n",
      "   [  0.16254915  -2.823455    -2.2590072    0.1333201 ]\n",
      "   [  0.8057602   -2.915716    -1.9673549    0.8124104 ]\n",
      "   [  0.58534837  -4.6730847   -3.4382315    0.5643268 ]\n",
      "   [  0.8921864   -4.067369    -2.6574163    0.87548065]\n",
      "   [  1.263035    -3.7051916   -2.9246576    1.259721  ]\n",
      "   [  1.5495721   -7.249092    -5.396203     1.5181053 ]\n",
      "   [  1.5050123   -6.274107    -4.4773808    1.4957819 ]\n",
      "   [  0.52326226  -4.514884    -2.882339     0.49977398]\n",
      "   [  0.9739213   -3.9392607   -2.6965396    0.95672405]\n",
      "   [  1.6701567   -8.105629    -5.6700974    1.6506586 ]]\n",
      "\n",
      "  [[ -0.42357546   3.4679852    3.2737176   -0.3926961 ]\n",
      "   [ -0.47237855   0.73508      1.3522946   -0.47363722]\n",
      "   [  0.87635446  -5.053873    -4.6070757    0.8318843 ]\n",
      "   [  1.5159085   -5.503112    -5.4603167    1.4666736 ]\n",
      "   [  1.319828    -4.7747602   -4.837744     1.2815806 ]\n",
      "   [  0.67584485  -5.7660384   -5.863941     0.6088723 ]\n",
      "   [  0.6045776   -6.189817    -6.437412     0.54042745]\n",
      "   [  0.44505593  -0.6814554   -0.9241394    0.42602405]\n",
      "   [  1.4660449   -3.211466    -2.9774787    1.4418215 ]\n",
      "   [  1.1929647   -4.357862    -4.3038273    1.1547726 ]\n",
      "   [  1.3491231   -1.4927049   -1.1584446    1.3557673 ]\n",
      "   [  0.7931738   -2.172222    -1.8795286    0.7862224 ]\n",
      "   [  2.1265166   -3.1071985   -3.385888     2.1233609 ]\n",
      "   [  1.1331393   -4.009203    -3.8779974    1.1037004 ]\n",
      "   [  0.94336456  -4.9021435   -4.4373703    0.9157444 ]\n",
      "   [  0.9782694   -4.6611958   -4.6269164    0.9539806 ]\n",
      "   [  2.2280862   -6.051944    -5.9109716    2.1952019 ]]\n",
      "\n",
      "  [[ -1.6452051    4.4152617    3.7192976   -1.6389836 ]\n",
      "   [  0.15054701  -1.376494    -0.84676856   0.10986754]\n",
      "   [  1.5166471   -5.9398055   -4.920059     1.4778708 ]\n",
      "   [  1.9333277   -4.260203    -3.3740385    1.9323695 ]\n",
      "   [  1.5309781   -4.547489    -3.489066     1.480607  ]\n",
      "   [  2.4673605   -5.5631537   -4.672514     2.4660254 ]\n",
      "   [  2.016837    -5.1566005   -4.018026     1.9924809 ]\n",
      "   [  1.4370192   -3.6683967   -2.7240903    1.4107778 ]\n",
      "   [  1.7217991   -4.4562564   -3.147753     1.6924825 ]\n",
      "   [  1.9967728   -6.69053     -5.3990774    1.966135  ]\n",
      "   [  1.448594    -3.9328735   -3.2366052    1.4085102 ]\n",
      "   [  1.3022422   -4.5195117   -3.695314     1.2822536 ]\n",
      "   [  1.6938584   -3.9746835   -3.1606958    1.6867723 ]\n",
      "   [  1.6408745   -3.3546185   -2.8749492    1.6534002 ]\n",
      "   [  1.8543684   -4.374298    -3.720038     1.8502384 ]\n",
      "   [  3.0048428   -7.5485997   -6.0270286    2.9982078 ]\n",
      "   [  3.0472713   -6.414274    -4.8705397    3.048514  ]]\n",
      "\n",
      "  [[ -1.2910825    2.8939807    1.9498776   -1.3604476 ]\n",
      "   [  0.00260484  -1.2573309   -0.8968588   -0.00958439]\n",
      "   [  0.75316143  -2.5608296   -1.9023268    0.7952951 ]\n",
      "   [  1.7065487   -5.793466    -4.5818887    1.7274722 ]\n",
      "   [  1.8291211   -7.0474367   -5.71421      1.8391759 ]\n",
      "   [  2.3368053   -9.357328    -7.7134566    2.3576763 ]\n",
      "   [  2.470301    -7.7757945   -6.127875     2.525731  ]\n",
      "   [  2.179172    -7.179293    -5.9285226    2.2387433 ]\n",
      "   [  2.447483    -5.1934614   -3.4582183    2.4647417 ]\n",
      "   [  2.341822    -5.8447704   -4.020074     2.4142365 ]\n",
      "   [  1.5742371   -4.565001    -4.107082     1.5955894 ]\n",
      "   [  1.2685965   -5.6633954   -4.5562716    1.2786512 ]\n",
      "   [  2.2874243   -8.599766    -7.4527707    2.2990892 ]\n",
      "   [  1.9153122   -8.844961    -8.40816      1.9211934 ]\n",
      "   [  1.6667337   -8.259502    -6.8934407    1.68654   ]\n",
      "   [  1.7146856   -3.6042047   -3.4926262    1.7736028 ]\n",
      "   [  1.5083216   -4.626235    -4.255062     1.5293872 ]]\n",
      "\n",
      "  [[ -0.65307564   3.4191115    2.6231182   -0.6571952 ]\n",
      "   [  0.72905433  -2.145474    -1.97241      0.77274555]\n",
      "   [  0.32349634  -1.2042887   -1.761193     0.3439586 ]\n",
      "   [  1.0276749    1.0591685    1.2420549    1.0516766 ]\n",
      "   [  1.6621069   -1.5393435   -1.5238453    1.7415565 ]\n",
      "   [  2.320585    -3.4410572   -2.1379056    2.3669145 ]\n",
      "   [  2.2574694   -5.198576    -3.338084     2.3181233 ]\n",
      "   [  2.1080887   -4.739885    -2.964387     2.1516023 ]\n",
      "   [  1.2008439   -1.6303428   -1.1642933    1.2419888 ]\n",
      "   [  1.6120139   -4.7312937   -3.3091938    1.6301892 ]\n",
      "   [  0.8391877   -2.1865473   -1.2894361    0.8481004 ]\n",
      "   [  1.0372262   -3.0614028   -2.5879543    1.0775429 ]\n",
      "   [  1.8101386   -3.9512072   -3.660099     1.8910354 ]\n",
      "   [  1.569953     0.33562177   0.45753404   1.6205871 ]\n",
      "   [  0.7444925   -3.9702365   -3.7100112    0.7392991 ]\n",
      "   [ -0.2933775    4.186637     3.426312    -0.33012983]\n",
      "   [  1.2968972   -0.68574536  -0.69700754   1.3469435 ]]\n",
      "\n",
      "  [[ -0.13926476   1.9205028    1.8216934   -0.10185678]\n",
      "   [  0.19879982  -1.8530471   -1.3639555    0.17513399]\n",
      "   [  0.91999245  -2.3831973   -1.2426841    0.89008635]\n",
      "   [  1.0381927   -2.2600927   -0.6750837    1.0146084 ]\n",
      "   [  1.3032984   -4.0459757   -2.579303     1.2662694 ]\n",
      "   [  1.1602114   -2.250634    -1.0625684    1.1338593 ]\n",
      "   [  1.0409786   -4.245473    -2.9055738    1.0156531 ]\n",
      "   [  1.014791    -1.7258795   -0.37235314   1.006404  ]\n",
      "   [  0.8859152   -2.206238    -0.84860724   0.888578  ]\n",
      "   [  1.2310557   -3.4763055   -2.1156893    1.1987462 ]\n",
      "   [  0.5707287   -1.6970289   -1.0702107    0.5439681 ]\n",
      "   [  0.9391156   -2.175531    -1.6824609    0.9395181 ]\n",
      "   [  0.90507394  -3.5607924   -3.33109      0.87961745]\n",
      "   [  0.78980726  -2.7478735   -1.3239654    0.7771642 ]\n",
      "   [  0.84316844   1.976685     2.620523     0.8371755 ]\n",
      "   [  0.49205273  -0.8215834   -0.27460206   0.49792984]\n",
      "   [  0.5736791   -2.6253998   -1.6513782    0.523841  ]]\n",
      "\n",
      "  [[ -1.5971173    5.382769     4.550599    -1.5563691 ]\n",
      "   [  0.86756885  -4.567832    -4.1745157    0.837092  ]\n",
      "   [  1.9241165   -9.449777    -8.618061     1.846838  ]\n",
      "   [  1.4564842   -6.3062134   -5.994499     1.4193963 ]\n",
      "   [  1.9547297   -5.528871    -5.109731     1.9279364 ]\n",
      "   [  1.7569231   -6.1655936   -5.818126     1.6856258 ]\n",
      "   [  3.4062142  -12.175182   -11.223284     3.321623  ]\n",
      "   [  1.7754023   -8.079217    -7.357292     1.722711  ]\n",
      "   [  2.411096    -7.082067    -6.472629     2.3767996 ]\n",
      "   [  2.2427962   -8.10574     -7.052744     2.1916656 ]\n",
      "   [  0.5261236   -3.244605    -2.7344992    0.48679253]\n",
      "   [  0.8691856   -1.3054293   -0.57848895   0.84080577]\n",
      "   [  2.913751    -6.7577157   -5.548678     2.8895385 ]\n",
      "   [  1.656866    -6.5547523   -6.1547184    1.6073687 ]\n",
      "   [  2.7824957   -9.328528    -8.781585     2.7219822 ]\n",
      "   [  0.99199706  -4.3707194   -4.6825504    0.97969323]\n",
      "   [  2.4821875   -6.9345264   -6.251833     2.450629  ]]]], shape=(1, 8, 17, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 17, 4)\n",
      "output.shape = (1, 8, 17, 32)\n",
      "scaled_attention.shape= (1, 17, 8, 32)\n",
      "concat_attention.shape= (1, 17, 256)\n",
      "outputs.shape= (1, 17, 256)\n",
      "Input: 고민이 있어\n",
      "Output: 생각을 종이에 끄젹여여 보는게 도움이 될 수도 있어요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"고민이 있어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724   -0.7859426    1.8895346   -4.1808095 ]\n",
      "   [  3.1438417   -2.3516042   -2.567732     3.3101664 ]\n",
      "   [ -6.821511     5.132966    -1.515522    -8.113494  ]\n",
      "   [ -2.6520565    0.29321504   2.1502676   -3.7211149 ]]\n",
      "\n",
      "  [[ -5.830585    -3.796887     3.4242606   -5.8514013 ]\n",
      "   [ -4.592556     0.16063029   2.767142    -4.5720806 ]\n",
      "   [ -3.28553     -1.1929119    0.7792256   -3.4044573 ]\n",
      "   [ -3.889495    -2.520736     2.122136    -3.5668743 ]]\n",
      "\n",
      "  [[  0.21261294  -2.0974307    1.9989094   -1.4663465 ]\n",
      "   [ -8.633069    -8.325801    -1.5773704  -10.09647   ]\n",
      "   [ -8.450605    -4.192688     1.4179147   -7.872258  ]\n",
      "   [ -3.4774246   -1.0960379    2.3552895   -3.6499627 ]]\n",
      "\n",
      "  [[ -0.62310183   0.02354282   0.13477258  -1.0099144 ]\n",
      "   [ -2.3111663   -0.6952288    2.469388    -1.3457242 ]\n",
      "   [ -3.9942987   -1.6525842   -1.1253656   -4.210669  ]\n",
      "   [ -1.8414911    0.44378278   1.8752314   -2.3266878 ]]\n",
      "\n",
      "  [[ -3.365542    -1.0635742   -0.04164821  -1.7994132 ]\n",
      "   [ -6.9826465   -2.3183198    2.294877    -4.308402  ]\n",
      "   [ -5.801219    -3.403646    -4.5645704   -6.675435  ]\n",
      "   [  0.2971933    0.63473487  -0.6389538    2.6413    ]]\n",
      "\n",
      "  [[ -2.1686854   -1.023346     0.65018183  -2.5024278 ]\n",
      "   [ -4.622171    -0.8858723    5.574534    -4.401349  ]\n",
      "   [ -3.8713155    1.6690211    3.75436     -4.813896  ]\n",
      "   [ -1.399356    -0.4949215    2.2085276   -2.2406054 ]]\n",
      "\n",
      "  [[ -0.3049526    2.224203    -1.2487973   -3.0664551 ]\n",
      "   [ -4.3723803    1.9350511    0.1110547   -4.0373297 ]\n",
      "   [ -8.824966    -2.4120855    4.2221565   -6.5589924 ]\n",
      "   [ -2.1759007    2.9709213    0.01304059  -3.8550992 ]]\n",
      "\n",
      "  [[ -2.8292158    0.7908498   -2.1495862   -2.1313064 ]\n",
      "   [ -4.523009    -2.7817159   -3.0650115   -5.429205  ]\n",
      "   [ -5.458025     1.8018078   -4.9602304   -6.058091  ]\n",
      "   [ -4.791121     1.7835791   -2.0525951   -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.3626041   -3.6032155    4.161095     0.0656366 ]\n",
      "   [ -6.182477    -2.5122466    7.009508    -3.584936  ]\n",
      "   [ -3.4696164    1.1780642    5.1291347   -2.1059198 ]\n",
      "   [ -3.483398    -4.1618204    4.4601355   -0.07843098]]\n",
      "\n",
      "  [[  0.510499     1.036337    -1.0375209    0.28767318]\n",
      "   [ -3.3273916   -0.11347406   3.4049916   -3.419848  ]\n",
      "   [ -4.9530396    0.3851128    4.3673553   -5.12493   ]\n",
      "   [ -2.7946234    0.38115668  -0.20627333  -2.9363682 ]]\n",
      "\n",
      "  [[ -9.042869    -3.423595    -4.7580905   -7.272155  ]\n",
      "   [ -5.3064766   -1.7224908   -1.1489992   -3.3164253 ]\n",
      "   [ -5.864008     1.9313622    5.1383986   -4.273402  ]\n",
      "   [-10.078705    -3.3595207   -3.9119687   -7.9476895 ]]\n",
      "\n",
      "  [[  0.704116     0.5427282   -1.371693     1.7133871 ]\n",
      "   [ -7.3349566   -2.3262897    2.0722024   -6.863343  ]\n",
      "   [ -7.6698356   -5.039354     0.01469475  -8.557696  ]\n",
      "   [  0.14490138   1.3311632   -0.6794642    1.1242119 ]]\n",
      "\n",
      "  [[  5.0507197    3.1588564    1.3940698    5.1164274 ]\n",
      "   [-13.740164    -2.4651692    8.651231   -13.1501875 ]\n",
      "   [-13.236485    -2.9835963    8.292635   -12.939023  ]\n",
      "   [  1.3312078    2.3829906    2.8037574    1.0776967 ]]\n",
      "\n",
      "  [[ 12.874086     2.85469     -6.3369064   11.067723  ]\n",
      "   [ -1.5080522   -3.8416598    0.31655705  -2.3005078 ]\n",
      "   [ -5.6672664   -1.453503     2.7044997   -4.9066    ]\n",
      "   [ 10.062546     2.1319842   -5.8750806    8.317264  ]]\n",
      "\n",
      "  [[ -8.929089    -8.014712    -1.0568765   -8.620834  ]\n",
      "   [-10.908268   -10.764266    -0.8827691  -10.867963  ]\n",
      "   [-10.751021    -8.130157     2.3616416   -9.656115  ]\n",
      "   [ -9.412735    -8.36169     -1.0328311   -8.912132  ]]\n",
      "\n",
      "  [[ 10.875551     2.7801728   -1.4670037   12.899784  ]\n",
      "   [  2.4307857    2.7539134    2.4796565    3.4081655 ]\n",
      "   [ -7.5609837   -0.33894548   3.7714336   -8.186357  ]\n",
      "   [  8.785441     2.427588    -1.0477616   10.493459  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 1)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106 ]]\n",
      "\n",
      "  [[-0.1123142 ]]\n",
      "\n",
      "  [[-0.10918517]]\n",
      "\n",
      "  [[-0.22114104]]\n",
      "\n",
      "  [[ 0.00076126]]\n",
      "\n",
      "  [[-0.00999826]]\n",
      "\n",
      "  [[-0.20863976]]\n",
      "\n",
      "  [[-0.15726982]]]], shape=(1, 8, 1, 1), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 1)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.43004218 -2.1264591  -4.20363     0.41727006]]\n",
      "\n",
      "  [[ 1.3539228  -1.1119139  -3.6765318   1.3366559 ]]\n",
      "\n",
      "  [[-0.6358987   0.6857092   3.934223   -0.6082751 ]]\n",
      "\n",
      "  [[-0.5390432   0.7702814   1.6837941  -0.4367591 ]]\n",
      "\n",
      "  [[-1.7341876   0.2025832   1.12208    -1.7460632 ]]\n",
      "\n",
      "  [[ 1.9698906   1.0078789  -1.2122052   2.0134254 ]]\n",
      "\n",
      "  [[ 0.1258963  -0.71917385 -2.2927022   0.03860371]]\n",
      "\n",
      "  [[-0.31415597  2.5866678   6.2770805  -0.21654116]]]], shape=(1, 8, 1, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 4)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 1)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7765327 ]]\n",
      "\n",
      "  [[ 0.02616234]]\n",
      "\n",
      "  [[-2.9237103 ]]\n",
      "\n",
      "  [[-2.809221  ]]\n",
      "\n",
      "  [[-0.4573305 ]]\n",
      "\n",
      "  [[-1.4886317 ]]\n",
      "\n",
      "  [[-2.3999312 ]]\n",
      "\n",
      "  [[-1.9244509 ]]]], shape=(1, 8, 1, 1), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 1)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.43907133 -0.33947113  4.013497   -0.29627562]]\n",
      "\n",
      "  [[-0.4324026   0.4282575   2.6263597  -0.40622276]]\n",
      "\n",
      "  [[-0.44929326  0.54526     4.611433   -0.29631194]]\n",
      "\n",
      "  [[-1.5575596  -0.13660048  4.819441   -1.527465  ]]\n",
      "\n",
      "  [[-1.0361646   1.0310884   2.9110556  -1.0666991 ]]\n",
      "\n",
      "  [[-0.79085    -1.3115892   0.67641354 -0.8404352 ]]\n",
      "\n",
      "  [[-0.21380028 -0.35505432  2.4520872  -0.14886369]]\n",
      "\n",
      "  [[-1.6940509  -1.1217054   4.9840674  -1.5811129 ]]]], shape=(1, 8, 1, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 4)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724   -0.7859426    1.8895346   -4.1808095 ]\n",
      "   [  3.1438417   -2.3516042   -2.567732     3.3101664 ]\n",
      "   [ -6.821511     5.132966    -1.515522    -8.113494  ]\n",
      "   [ -2.6520565    0.29321504   2.1502676   -3.7211149 ]]\n",
      "\n",
      "  [[ -5.830585    -3.796887     3.4242606   -5.8514013 ]\n",
      "   [ -4.592556     0.16063029   2.767142    -4.5720806 ]\n",
      "   [ -3.28553     -1.1929119    0.7792256   -3.4044573 ]\n",
      "   [ -3.889495    -2.520736     2.122136    -3.5668743 ]]\n",
      "\n",
      "  [[  0.21261294  -2.0974307    1.9989094   -1.4663465 ]\n",
      "   [ -8.633069    -8.325801    -1.5773704  -10.09647   ]\n",
      "   [ -8.450605    -4.192688     1.4179147   -7.872258  ]\n",
      "   [ -3.4774246   -1.0960379    2.3552895   -3.6499627 ]]\n",
      "\n",
      "  [[ -0.62310183   0.02354282   0.13477258  -1.0099144 ]\n",
      "   [ -2.3111663   -0.6952288    2.469388    -1.3457242 ]\n",
      "   [ -3.9942987   -1.6525842   -1.1253656   -4.210669  ]\n",
      "   [ -1.8414911    0.44378278   1.8752314   -2.3266878 ]]\n",
      "\n",
      "  [[ -3.365542    -1.0635742   -0.04164821  -1.7994132 ]\n",
      "   [ -6.9826465   -2.3183198    2.294877    -4.308402  ]\n",
      "   [ -5.801219    -3.403646    -4.5645704   -6.675435  ]\n",
      "   [  0.2971933    0.63473487  -0.6389538    2.6413    ]]\n",
      "\n",
      "  [[ -2.1686854   -1.023346     0.65018183  -2.5024278 ]\n",
      "   [ -4.622171    -0.8858723    5.574534    -4.401349  ]\n",
      "   [ -3.8713155    1.6690211    3.75436     -4.813896  ]\n",
      "   [ -1.399356    -0.4949215    2.2085276   -2.2406054 ]]\n",
      "\n",
      "  [[ -0.3049526    2.224203    -1.2487973   -3.0664551 ]\n",
      "   [ -4.3723803    1.9350511    0.1110547   -4.0373297 ]\n",
      "   [ -8.824966    -2.4120855    4.2221565   -6.5589924 ]\n",
      "   [ -2.1759007    2.9709213    0.01304059  -3.8550992 ]]\n",
      "\n",
      "  [[ -2.8292158    0.7908498   -2.1495862   -2.1313064 ]\n",
      "   [ -4.523009    -2.7817159   -3.0650115   -5.429205  ]\n",
      "   [ -5.458025     1.8018078   -4.9602304   -6.058091  ]\n",
      "   [ -4.791121     1.7835791   -2.0525951   -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.3626041   -3.6032155    4.161095     0.0656366 ]\n",
      "   [ -6.182477    -2.5122466    7.009508    -3.584936  ]\n",
      "   [ -3.4696164    1.1780642    5.1291347   -2.1059198 ]\n",
      "   [ -3.483398    -4.1618204    4.4601355   -0.07843098]]\n",
      "\n",
      "  [[  0.510499     1.036337    -1.0375209    0.28767318]\n",
      "   [ -3.3273916   -0.11347406   3.4049916   -3.419848  ]\n",
      "   [ -4.9530396    0.3851128    4.3673553   -5.12493   ]\n",
      "   [ -2.7946234    0.38115668  -0.20627333  -2.9363682 ]]\n",
      "\n",
      "  [[ -9.042869    -3.423595    -4.7580905   -7.272155  ]\n",
      "   [ -5.3064766   -1.7224908   -1.1489992   -3.3164253 ]\n",
      "   [ -5.864008     1.9313622    5.1383986   -4.273402  ]\n",
      "   [-10.078705    -3.3595207   -3.9119687   -7.9476895 ]]\n",
      "\n",
      "  [[  0.704116     0.5427282   -1.371693     1.7133871 ]\n",
      "   [ -7.3349566   -2.3262897    2.0722024   -6.863343  ]\n",
      "   [ -7.6698356   -5.039354     0.01469475  -8.557696  ]\n",
      "   [  0.14490138   1.3311632   -0.6794642    1.1242119 ]]\n",
      "\n",
      "  [[  5.0507197    3.1588564    1.3940698    5.1164274 ]\n",
      "   [-13.740164    -2.4651692    8.651231   -13.1501875 ]\n",
      "   [-13.236485    -2.9835963    8.292635   -12.939023  ]\n",
      "   [  1.3312078    2.3829906    2.8037574    1.0776967 ]]\n",
      "\n",
      "  [[ 12.874086     2.85469     -6.3369064   11.067723  ]\n",
      "   [ -1.5080522   -3.8416598    0.31655705  -2.3005078 ]\n",
      "   [ -5.6672664   -1.453503     2.7044997   -4.9066    ]\n",
      "   [ 10.062546     2.1319842   -5.8750806    8.317264  ]]\n",
      "\n",
      "  [[ -8.929089    -8.014712    -1.0568765   -8.620834  ]\n",
      "   [-10.908268   -10.764266    -0.8827691  -10.867963  ]\n",
      "   [-10.751021    -8.130157     2.3616416   -9.656115  ]\n",
      "   [ -9.412735    -8.36169     -1.0328311   -8.912132  ]]\n",
      "\n",
      "  [[ 10.875551     2.7801728   -1.4670037   12.899784  ]\n",
      "   [  2.4307857    2.7539134    2.4796565    3.4081655 ]\n",
      "   [ -7.5609837   -0.33894548   3.7714336   -8.186357  ]\n",
      "   [  8.785441     2.427588    -1.0477616   10.493459  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 2)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.86808515]\n",
      "   [-0.8052174   1.2325178 ]]\n",
      "\n",
      "  [[-0.1123142   0.22450152]\n",
      "   [ 0.31872883 -4.25332   ]]\n",
      "\n",
      "  [[-0.10918517  0.20129828]\n",
      "   [ 0.3942903  -3.7951708 ]]\n",
      "\n",
      "  [[-0.22114104  0.78687924]\n",
      "   [-0.32672447  1.066411  ]]\n",
      "\n",
      "  [[ 0.00076126 -0.47878256]\n",
      "   [ 0.7206705  -4.158416  ]]\n",
      "\n",
      "  [[-0.00999826 -0.17336224]\n",
      "   [ 1.4367385  -9.06984   ]]\n",
      "\n",
      "  [[-0.20863976  0.569198  ]\n",
      "   [-0.44067878 -1.3154124 ]]\n",
      "\n",
      "  [[-0.15726982  0.34657618]\n",
      "   [-0.20781471 -3.9169958 ]]]], shape=(1, 8, 2, 2), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 2)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.43004218 -2.1264591  -4.20363     0.41727006]\n",
      "   [ 0.36214504 -0.9224591  -3.69376     0.32345214]]\n",
      "\n",
      "  [[ 1.3539228  -1.1119139  -3.6765318   1.3366559 ]\n",
      "   [ 0.49521226  0.8556765   1.0242827   0.54091555]]\n",
      "\n",
      "  [[-0.6358987   0.6857092   3.934223   -0.6082751 ]\n",
      "   [ 2.1257792  -0.75171655 -5.1641245   2.0634513 ]]\n",
      "\n",
      "  [[-0.5390432   0.7702814   1.6837941  -0.4367591 ]\n",
      "   [ 1.0922868  -2.2901568  -6.70844     0.99209934]]\n",
      "\n",
      "  [[-1.7341876   0.2025832   1.12208    -1.7460632 ]\n",
      "   [-0.38023633 -0.88305074 -1.6188183  -0.4605108 ]]\n",
      "\n",
      "  [[ 1.9698906   1.0078789  -1.2122052   2.0134254 ]\n",
      "   [ 1.8958745  -0.45892912 -5.3896065   1.8568405 ]]\n",
      "\n",
      "  [[ 0.1258963  -0.71917385 -2.2927022   0.03860371]\n",
      "   [ 1.3117977  -1.6638893  -4.465428    1.2304243 ]]\n",
      "\n",
      "  [[-0.31415597  2.5866678   6.2770805  -0.21654116]\n",
      "   [ 1.1086718  -2.3592036  -5.937715    1.05256   ]]]], shape=(1, 8, 2, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 4)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 2)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.77654105 -1.1600666 ]\n",
      "   [ 4.2638865  -4.9808917 ]]\n",
      "\n",
      "  [[ 0.02615874  1.1034771 ]\n",
      "   [-1.1199212  -1.2406197 ]]\n",
      "\n",
      "  [[-2.9237115   3.363596  ]\n",
      "   [-0.80274636  4.677598  ]]\n",
      "\n",
      "  [[-2.8092232   1.49841   ]\n",
      "   [-2.5165534   2.19107   ]]\n",
      "\n",
      "  [[-0.4573368  -0.41149053]\n",
      "   [ 3.5183     -1.5149683 ]]\n",
      "\n",
      "  [[-1.4886304  -0.8160711 ]\n",
      "   [ 0.90435994 -2.5749598 ]]\n",
      "\n",
      "  [[-2.3999348   1.3400928 ]\n",
      "   [-1.6690627   0.10749228]]\n",
      "\n",
      "  [[-1.9244474   2.1994686 ]\n",
      "   [-0.34355617 -2.1330175 ]]]], shape=(1, 8, 2, 2), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 2)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.4390437   -0.33947098   4.013443    -0.29624832]\n",
      "   [ -0.2583174    0.33025625  -0.39602375  -0.30866668]]\n",
      "\n",
      "  [[ -0.43239835   0.42818624   2.6262326   -0.4062209 ]\n",
      "   [ -0.22419338   1.074776     1.1630725   -0.19326892]]\n",
      "\n",
      "  [[ -0.44927993   0.54527193   4.6114116   -0.29629943]\n",
      "   [  0.67831874  -0.902597    -5.0636787    0.530373  ]]\n",
      "\n",
      "  [[ -1.5574945   -0.13661464   4.8192244   -1.5274014 ]\n",
      "   [  1.8428049    0.10988725  -3.7634213    1.8194834 ]]\n",
      "\n",
      "  [[ -1.0361291    1.0311096    2.9110353   -1.0666645 ]\n",
      "   [  0.17529838  -0.76200086  -1.3703144    0.18987674]]\n",
      "\n",
      "  [[ -0.7908757   -1.3115877    0.67647886  -0.8404614 ]\n",
      "   [ -0.19861834  -1.3611583   -2.9659412   -0.28904545]]\n",
      "\n",
      "  [[ -0.21379697  -0.3550262    2.452176    -0.14885877]\n",
      "   [  1.1190844   -0.06087807  -3.8031468    1.0202737 ]]\n",
      "\n",
      "  [[ -1.6940544   -1.1217345    4.9840846   -1.5811146 ]\n",
      "   [  2.1202106   -1.363047   -10.718351     1.9247364 ]]]], shape=(1, 8, 2, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 4)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724   -0.7859426    1.8895346   -4.1808095 ]\n",
      "   [  3.1438417   -2.3516042   -2.567732     3.3101664 ]\n",
      "   [ -6.821511     5.132966    -1.515522    -8.113494  ]\n",
      "   [ -2.6520565    0.29321504   2.1502676   -3.7211149 ]]\n",
      "\n",
      "  [[ -5.830585    -3.796887     3.4242606   -5.8514013 ]\n",
      "   [ -4.592556     0.16063029   2.767142    -4.5720806 ]\n",
      "   [ -3.28553     -1.1929119    0.7792256   -3.4044573 ]\n",
      "   [ -3.889495    -2.520736     2.122136    -3.5668743 ]]\n",
      "\n",
      "  [[  0.21261294  -2.0974307    1.9989094   -1.4663465 ]\n",
      "   [ -8.633069    -8.325801    -1.5773704  -10.09647   ]\n",
      "   [ -8.450605    -4.192688     1.4179147   -7.872258  ]\n",
      "   [ -3.4774246   -1.0960379    2.3552895   -3.6499627 ]]\n",
      "\n",
      "  [[ -0.62310183   0.02354282   0.13477258  -1.0099144 ]\n",
      "   [ -2.3111663   -0.6952288    2.469388    -1.3457242 ]\n",
      "   [ -3.9942987   -1.6525842   -1.1253656   -4.210669  ]\n",
      "   [ -1.8414911    0.44378278   1.8752314   -2.3266878 ]]\n",
      "\n",
      "  [[ -3.365542    -1.0635742   -0.04164821  -1.7994132 ]\n",
      "   [ -6.9826465   -2.3183198    2.294877    -4.308402  ]\n",
      "   [ -5.801219    -3.403646    -4.5645704   -6.675435  ]\n",
      "   [  0.2971933    0.63473487  -0.6389538    2.6413    ]]\n",
      "\n",
      "  [[ -2.1686854   -1.023346     0.65018183  -2.5024278 ]\n",
      "   [ -4.622171    -0.8858723    5.574534    -4.401349  ]\n",
      "   [ -3.8713155    1.6690211    3.75436     -4.813896  ]\n",
      "   [ -1.399356    -0.4949215    2.2085276   -2.2406054 ]]\n",
      "\n",
      "  [[ -0.3049526    2.224203    -1.2487973   -3.0664551 ]\n",
      "   [ -4.3723803    1.9350511    0.1110547   -4.0373297 ]\n",
      "   [ -8.824966    -2.4120855    4.2221565   -6.5589924 ]\n",
      "   [ -2.1759007    2.9709213    0.01304059  -3.8550992 ]]\n",
      "\n",
      "  [[ -2.8292158    0.7908498   -2.1495862   -2.1313064 ]\n",
      "   [ -4.523009    -2.7817159   -3.0650115   -5.429205  ]\n",
      "   [ -5.458025     1.8018078   -4.9602304   -6.058091  ]\n",
      "   [ -4.791121     1.7835791   -2.0525951   -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.3626041   -3.6032155    4.161095     0.0656366 ]\n",
      "   [ -6.182477    -2.5122466    7.009508    -3.584936  ]\n",
      "   [ -3.4696164    1.1780642    5.1291347   -2.1059198 ]\n",
      "   [ -3.483398    -4.1618204    4.4601355   -0.07843098]]\n",
      "\n",
      "  [[  0.510499     1.036337    -1.0375209    0.28767318]\n",
      "   [ -3.3273916   -0.11347406   3.4049916   -3.419848  ]\n",
      "   [ -4.9530396    0.3851128    4.3673553   -5.12493   ]\n",
      "   [ -2.7946234    0.38115668  -0.20627333  -2.9363682 ]]\n",
      "\n",
      "  [[ -9.042869    -3.423595    -4.7580905   -7.272155  ]\n",
      "   [ -5.3064766   -1.7224908   -1.1489992   -3.3164253 ]\n",
      "   [ -5.864008     1.9313622    5.1383986   -4.273402  ]\n",
      "   [-10.078705    -3.3595207   -3.9119687   -7.9476895 ]]\n",
      "\n",
      "  [[  0.704116     0.5427282   -1.371693     1.7133871 ]\n",
      "   [ -7.3349566   -2.3262897    2.0722024   -6.863343  ]\n",
      "   [ -7.6698356   -5.039354     0.01469475  -8.557696  ]\n",
      "   [  0.14490138   1.3311632   -0.6794642    1.1242119 ]]\n",
      "\n",
      "  [[  5.0507197    3.1588564    1.3940698    5.1164274 ]\n",
      "   [-13.740164    -2.4651692    8.651231   -13.1501875 ]\n",
      "   [-13.236485    -2.9835963    8.292635   -12.939023  ]\n",
      "   [  1.3312078    2.3829906    2.8037574    1.0776967 ]]\n",
      "\n",
      "  [[ 12.874086     2.85469     -6.3369064   11.067723  ]\n",
      "   [ -1.5080522   -3.8416598    0.31655705  -2.3005078 ]\n",
      "   [ -5.6672664   -1.453503     2.7044997   -4.9066    ]\n",
      "   [ 10.062546     2.1319842   -5.8750806    8.317264  ]]\n",
      "\n",
      "  [[ -8.929089    -8.014712    -1.0568765   -8.620834  ]\n",
      "   [-10.908268   -10.764266    -0.8827691  -10.867963  ]\n",
      "   [-10.751021    -8.130157     2.3616416   -9.656115  ]\n",
      "   [ -9.412735    -8.36169     -1.0328311   -8.912132  ]]\n",
      "\n",
      "  [[ 10.875551     2.7801728   -1.4670037   12.899784  ]\n",
      "   [  2.4307857    2.7539134    2.4796565    3.4081655 ]\n",
      "   [ -7.5609837   -0.33894548   3.7714336   -8.186357  ]\n",
      "   [  8.785441     2.427588    -1.0477616   10.493459  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 3)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.86808515  0.68926543]\n",
      "   [-0.8052174   1.2325178   1.3822585 ]\n",
      "   [-0.29754275  1.5583472  -0.8161052 ]]\n",
      "\n",
      "  [[-0.1123142   0.22450152  0.1997487 ]\n",
      "   [ 0.31872883 -4.25332    -2.458986  ]\n",
      "   [ 0.06266014 -2.8750339  -5.2976384 ]]\n",
      "\n",
      "  [[-0.10918517  0.20129828 -0.1020415 ]\n",
      "   [ 0.3942903  -3.7951708  -3.841178  ]\n",
      "   [-0.45792332 -2.8219523  -3.4908211 ]]\n",
      "\n",
      "  [[-0.22114104  0.78687924  0.38468125]\n",
      "   [-0.32672447  1.066411    0.20903866]\n",
      "   [ 0.17269178 -1.3044931  -4.883713  ]]\n",
      "\n",
      "  [[ 0.00076126 -0.47878256  0.09911879]\n",
      "   [ 0.7206705  -4.158416   -1.74024   ]\n",
      "   [ 0.12035121  5.0561695  -2.9415672 ]]\n",
      "\n",
      "  [[-0.00999826 -0.17336224 -0.04885251]\n",
      "   [ 1.4367385  -9.06984    -3.8178802 ]\n",
      "   [ 0.92183876 -4.7562103  -6.4032483 ]]\n",
      "\n",
      "  [[-0.20863976  0.569198    0.44341293]\n",
      "   [-0.44067878 -1.3154124   2.2516541 ]\n",
      "   [-0.73116076  0.78557914  1.7380664 ]]\n",
      "\n",
      "  [[-0.15726982  0.34657618  0.09771852]\n",
      "   [-0.20781471 -3.9169958  -2.6317358 ]\n",
      "   [ 0.49460515 -2.8374786  -6.512395  ]]]], shape=(1, 8, 3, 3), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 3)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.43004218 -2.1264591  -4.20363     0.41727006]\n",
      "   [ 0.36214504 -0.9224591  -3.69376     0.32345214]\n",
      "   [-0.11888524  0.7827182   1.6091667  -0.07043942]]\n",
      "\n",
      "  [[ 1.3539228  -1.1119139  -3.6765318   1.3366559 ]\n",
      "   [ 0.49521226  0.8556765   1.0242827   0.54091555]\n",
      "   [ 1.8845222   0.80958265 -1.4450098   1.8755151 ]]\n",
      "\n",
      "  [[-0.6358987   0.6857092   3.934223   -0.6082751 ]\n",
      "   [ 2.1257792  -0.75171655 -5.1641245   2.0634513 ]\n",
      "   [ 1.9908828  -2.4718666  -8.323398    1.8416339 ]]\n",
      "\n",
      "  [[-0.5390432   0.7702814   1.6837941  -0.4367591 ]\n",
      "   [ 1.0922868  -2.2901568  -6.70844     0.99209934]\n",
      "   [ 0.63555646 -0.9029067  -2.4676714   0.5942919 ]]\n",
      "\n",
      "  [[-1.7341876   0.2025832   1.12208    -1.7460632 ]\n",
      "   [-0.38023633 -0.88305074 -1.6188183  -0.4605108 ]\n",
      "   [ 0.7600631  -0.39524692 -3.455463    0.6971765 ]]\n",
      "\n",
      "  [[ 1.9698906   1.0078789  -1.2122052   2.0134254 ]\n",
      "   [ 1.8958745  -0.45892912 -5.3896065   1.8568405 ]\n",
      "   [ 1.4736686   0.538687   -1.965722    1.469285  ]]\n",
      "\n",
      "  [[ 0.1258963  -0.71917385 -2.2927022   0.03860371]\n",
      "   [ 1.3117977  -1.6638893  -4.465428    1.2304243 ]\n",
      "   [ 2.5624385  -3.8971152  -9.53067     2.4499042 ]]\n",
      "\n",
      "  [[-0.31415597  2.5866678   6.2770805  -0.21654116]\n",
      "   [ 1.1086718  -2.3592036  -5.937715    1.05256   ]\n",
      "   [ 1.0852163  -4.0399203  -9.892566    0.9899011 ]]]], shape=(1, 8, 3, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 4)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 3)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.77654105 -1.1600666  -0.7377553 ]\n",
      "   [ 4.2638865  -4.9808917  -4.7282324 ]\n",
      "   [ 3.7290585  -3.6264493  -4.6852508 ]]\n",
      "\n",
      "  [[ 0.02615874  1.1034771  -0.24971536]\n",
      "   [-1.1199212  -1.2406197  -0.3052636 ]\n",
      "   [-1.1390502   0.8624328   2.084101  ]]\n",
      "\n",
      "  [[-2.9237115   3.363596    2.3430805 ]\n",
      "   [-0.80274636  4.677598    1.8153228 ]\n",
      "   [-2.9179065   2.4044318   0.6520111 ]]\n",
      "\n",
      "  [[-2.8092232   1.49841     1.9324561 ]\n",
      "   [-2.5165534   2.19107     0.6257863 ]\n",
      "   [-2.6379817   2.7387853  -1.0823029 ]]\n",
      "\n",
      "  [[-0.4573368  -0.41149053  0.57131124]\n",
      "   [ 3.5183     -1.5149683  -4.673192  ]\n",
      "   [-0.12923025  1.2998483  -1.2514737 ]]\n",
      "\n",
      "  [[-1.4886304  -0.8160711   1.272815  ]\n",
      "   [ 0.90435994 -2.5749598  -1.9510299 ]\n",
      "   [-0.34667596 -1.3035538  -2.9048364 ]]\n",
      "\n",
      "  [[-2.3999348   1.3400928   0.6617681 ]\n",
      "   [-1.6690627   0.10749228  0.34268987]\n",
      "   [-1.4865489  -3.3957736  -3.0061002 ]]\n",
      "\n",
      "  [[-1.9244474   2.1994686   1.347579  ]\n",
      "   [-0.34355617 -2.1330175   0.8624636 ]\n",
      "   [ 0.41986907 -3.7553241  -1.9478018 ]]]], shape=(1, 8, 3, 3), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 3)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.4390437   -0.33947098   4.013443    -0.29624832]\n",
      "   [ -0.2583174    0.33025625  -0.39602375  -0.30866668]\n",
      "   [  0.32099545  -0.95505685  -4.5404463    0.24053822]]\n",
      "\n",
      "  [[ -0.43239835   0.42818624   2.6262326   -0.4062209 ]\n",
      "   [ -0.22419338   1.074776     1.1630725   -0.19326892]\n",
      "   [  0.51628953   0.88442105  -0.02622377   0.5438406 ]]\n",
      "\n",
      "  [[ -0.44927993   0.54527193   4.6114116   -0.29629943]\n",
      "   [  0.67831874  -0.902597    -5.0636787    0.530373  ]\n",
      "   [  1.1608257   -0.9582575   -6.0131745    1.0148909 ]]\n",
      "\n",
      "  [[ -1.5574945   -0.13661464   4.8192244   -1.5274014 ]\n",
      "   [  1.8428049    0.10988725  -3.7634213    1.8194834 ]\n",
      "   [  1.8790634    0.16765016  -5.2636895    1.8923659 ]]\n",
      "\n",
      "  [[ -1.0361291    1.0311096    2.9110353   -1.0666645 ]\n",
      "   [  0.17529838  -0.76200086  -1.3703144    0.18987674]\n",
      "   [  0.9855322   -1.2880604   -2.9897346    0.9731935 ]]\n",
      "\n",
      "  [[ -0.7908757   -1.3115877    0.67647886  -0.8404614 ]\n",
      "   [ -0.19861834  -1.3611583   -2.9659412   -0.28904545]\n",
      "   [ -0.5602697   -0.27112013   0.11186516  -0.6005517 ]]\n",
      "\n",
      "  [[ -0.21379697  -0.3550262    2.452176    -0.14885877]\n",
      "   [  1.1190844   -0.06087807  -3.8031468    1.0202737 ]\n",
      "   [  0.3503374   -1.1730963   -4.796474     0.2665696 ]]\n",
      "\n",
      "  [[ -1.6940544   -1.1217345    4.9840846   -1.5811146 ]\n",
      "   [  2.1202106   -1.363047   -10.718351     1.9247364 ]\n",
      "   [  0.94408834   1.107803    -3.365614     0.8552538 ]]]], shape=(1, 8, 3, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 4)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724   -0.7859426    1.8895346   -4.1808095 ]\n",
      "   [  3.1438417   -2.3516042   -2.567732     3.3101664 ]\n",
      "   [ -6.821511     5.132966    -1.515522    -8.113494  ]\n",
      "   [ -2.6520565    0.29321504   2.1502676   -3.7211149 ]]\n",
      "\n",
      "  [[ -5.830585    -3.796887     3.4242606   -5.8514013 ]\n",
      "   [ -4.592556     0.16063029   2.767142    -4.5720806 ]\n",
      "   [ -3.28553     -1.1929119    0.7792256   -3.4044573 ]\n",
      "   [ -3.889495    -2.520736     2.122136    -3.5668743 ]]\n",
      "\n",
      "  [[  0.21261294  -2.0974307    1.9989094   -1.4663465 ]\n",
      "   [ -8.633069    -8.325801    -1.5773704  -10.09647   ]\n",
      "   [ -8.450605    -4.192688     1.4179147   -7.872258  ]\n",
      "   [ -3.4774246   -1.0960379    2.3552895   -3.6499627 ]]\n",
      "\n",
      "  [[ -0.62310183   0.02354282   0.13477258  -1.0099144 ]\n",
      "   [ -2.3111663   -0.6952288    2.469388    -1.3457242 ]\n",
      "   [ -3.9942987   -1.6525842   -1.1253656   -4.210669  ]\n",
      "   [ -1.8414911    0.44378278   1.8752314   -2.3266878 ]]\n",
      "\n",
      "  [[ -3.365542    -1.0635742   -0.04164821  -1.7994132 ]\n",
      "   [ -6.9826465   -2.3183198    2.294877    -4.308402  ]\n",
      "   [ -5.801219    -3.403646    -4.5645704   -6.675435  ]\n",
      "   [  0.2971933    0.63473487  -0.6389538    2.6413    ]]\n",
      "\n",
      "  [[ -2.1686854   -1.023346     0.65018183  -2.5024278 ]\n",
      "   [ -4.622171    -0.8858723    5.574534    -4.401349  ]\n",
      "   [ -3.8713155    1.6690211    3.75436     -4.813896  ]\n",
      "   [ -1.399356    -0.4949215    2.2085276   -2.2406054 ]]\n",
      "\n",
      "  [[ -0.3049526    2.224203    -1.2487973   -3.0664551 ]\n",
      "   [ -4.3723803    1.9350511    0.1110547   -4.0373297 ]\n",
      "   [ -8.824966    -2.4120855    4.2221565   -6.5589924 ]\n",
      "   [ -2.1759007    2.9709213    0.01304059  -3.8550992 ]]\n",
      "\n",
      "  [[ -2.8292158    0.7908498   -2.1495862   -2.1313064 ]\n",
      "   [ -4.523009    -2.7817159   -3.0650115   -5.429205  ]\n",
      "   [ -5.458025     1.8018078   -4.9602304   -6.058091  ]\n",
      "   [ -4.791121     1.7835791   -2.0525951   -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.3626041   -3.6032155    4.161095     0.0656366 ]\n",
      "   [ -6.182477    -2.5122466    7.009508    -3.584936  ]\n",
      "   [ -3.4696164    1.1780642    5.1291347   -2.1059198 ]\n",
      "   [ -3.483398    -4.1618204    4.4601355   -0.07843098]]\n",
      "\n",
      "  [[  0.510499     1.036337    -1.0375209    0.28767318]\n",
      "   [ -3.3273916   -0.11347406   3.4049916   -3.419848  ]\n",
      "   [ -4.9530396    0.3851128    4.3673553   -5.12493   ]\n",
      "   [ -2.7946234    0.38115668  -0.20627333  -2.9363682 ]]\n",
      "\n",
      "  [[ -9.042869    -3.423595    -4.7580905   -7.272155  ]\n",
      "   [ -5.3064766   -1.7224908   -1.1489992   -3.3164253 ]\n",
      "   [ -5.864008     1.9313622    5.1383986   -4.273402  ]\n",
      "   [-10.078705    -3.3595207   -3.9119687   -7.9476895 ]]\n",
      "\n",
      "  [[  0.704116     0.5427282   -1.371693     1.7133871 ]\n",
      "   [ -7.3349566   -2.3262897    2.0722024   -6.863343  ]\n",
      "   [ -7.6698356   -5.039354     0.01469475  -8.557696  ]\n",
      "   [  0.14490138   1.3311632   -0.6794642    1.1242119 ]]\n",
      "\n",
      "  [[  5.0507197    3.1588564    1.3940698    5.1164274 ]\n",
      "   [-13.740164    -2.4651692    8.651231   -13.1501875 ]\n",
      "   [-13.236485    -2.9835963    8.292635   -12.939023  ]\n",
      "   [  1.3312078    2.3829906    2.8037574    1.0776967 ]]\n",
      "\n",
      "  [[ 12.874086     2.85469     -6.3369064   11.067723  ]\n",
      "   [ -1.5080522   -3.8416598    0.31655705  -2.3005078 ]\n",
      "   [ -5.6672664   -1.453503     2.7044997   -4.9066    ]\n",
      "   [ 10.062546     2.1319842   -5.8750806    8.317264  ]]\n",
      "\n",
      "  [[ -8.929089    -8.014712    -1.0568765   -8.620834  ]\n",
      "   [-10.908268   -10.764266    -0.8827691  -10.867963  ]\n",
      "   [-10.751021    -8.130157     2.3616416   -9.656115  ]\n",
      "   [ -9.412735    -8.36169     -1.0328311   -8.912132  ]]\n",
      "\n",
      "  [[ 10.875551     2.7801728   -1.4670037   12.899784  ]\n",
      "   [  2.4307857    2.7539134    2.4796565    3.4081655 ]\n",
      "   [ -7.5609837   -0.33894548   3.7714336   -8.186357  ]\n",
      "   [  8.785441     2.427588    -1.0477616   10.493459  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.86808515  0.68926543 -0.07710785]\n",
      "   [-0.8052174   1.2325178   1.3822585  -0.6516272 ]\n",
      "   [-0.29754275  1.5583472  -0.8161052   1.4953052 ]\n",
      "   [-0.5443256   2.2166584   6.8072824  -9.876017  ]]\n",
      "\n",
      "  [[-0.1123142   0.22450152  0.1997487  -0.5255737 ]\n",
      "   [ 0.31872883 -4.25332    -2.458986    4.503875  ]\n",
      "   [ 0.06266014 -2.8750339  -5.2976384   1.4493271 ]\n",
      "   [-0.54926753  4.0720015   2.403759   -3.7070231 ]]\n",
      "\n",
      "  [[-0.10918517  0.20129828 -0.1020415   0.657444  ]\n",
      "   [ 0.3942903  -3.7951708  -3.841178   -0.39951256]\n",
      "   [-0.45792332 -2.8219523  -3.4908211  -0.8908195 ]\n",
      "   [ 0.4032216   1.4782436   8.961608   -4.7050643 ]]\n",
      "\n",
      "  [[-0.22114104  0.78687924  0.38468125 -0.35858485]\n",
      "   [-0.32672447  1.066411    0.20903866 -0.35502467]\n",
      "   [ 0.17269178 -1.3044931  -4.883713   -0.30554432]\n",
      "   [ 0.08333597  1.9566892   3.4552226  -3.034405  ]]\n",
      "\n",
      "  [[ 0.00076126 -0.47878256  0.09911879  0.00893964]\n",
      "   [ 0.7206705  -4.158416   -1.74024     4.0903745 ]\n",
      "   [ 0.12035121  5.0561695  -2.9415672  -0.26743913]\n",
      "   [ 0.03012125  5.2640715   3.1126494  -4.628424  ]]\n",
      "\n",
      "  [[-0.00999826 -0.17336224 -0.04885251 -0.06919655]\n",
      "   [ 1.4367385  -9.06984    -3.8178802  -0.75747144]\n",
      "   [ 0.92183876 -4.7562103  -6.4032483  -2.5681021 ]\n",
      "   [-0.15983817  5.33236     4.7559114   0.13260181]]\n",
      "\n",
      "  [[-0.20863976  0.569198    0.44341293  0.16857311]\n",
      "   [-0.44067878 -1.3154124   2.2516541   5.957117  ]\n",
      "   [-0.73116076  0.78557914  1.7380664   4.084935  ]\n",
      "   [ 2.1423054  -2.7513258  -6.220788   -6.2902803 ]]\n",
      "\n",
      "  [[-0.15726982  0.34657618  0.09771852 -0.06033853]\n",
      "   [-0.20781471 -3.9169958  -2.6317358   2.4233148 ]\n",
      "   [ 0.49460515 -2.8374786  -6.512395   -2.2088082 ]\n",
      "   [ 0.40351048  5.496288    7.4884925  -5.00473   ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.43004218 -2.1264591  -4.20363     0.41727006]\n",
      "   [ 0.36214504 -0.9224591  -3.69376     0.32345214]\n",
      "   [-0.11888524  0.7827182   1.6091667  -0.07043942]\n",
      "   [ 1.5769911  -1.5107999  -4.705859    1.5822129 ]]\n",
      "\n",
      "  [[ 1.3539228  -1.1119139  -3.6765318   1.3366559 ]\n",
      "   [ 0.49521226  0.8556765   1.0242827   0.54091555]\n",
      "   [ 1.8845222   0.80958265 -1.4450098   1.8755151 ]\n",
      "   [ 3.1395586  -0.3902885  -4.6706777   3.0263221 ]]\n",
      "\n",
      "  [[-0.6358987   0.6857092   3.934223   -0.6082751 ]\n",
      "   [ 2.1257792  -0.75171655 -5.1641245   2.0634513 ]\n",
      "   [ 1.9908828  -2.4718666  -8.323398    1.8416339 ]\n",
      "   [ 1.4525386  -1.9171642  -6.0657043   1.3277422 ]]\n",
      "\n",
      "  [[-0.5390432   0.7702814   1.6837941  -0.4367591 ]\n",
      "   [ 1.0922868  -2.2901568  -6.70844     0.99209934]\n",
      "   [ 0.63555646 -0.9029067  -2.4676714   0.5942919 ]\n",
      "   [ 1.3171933  -1.6731497  -4.4172173   1.2324891 ]]\n",
      "\n",
      "  [[-1.7341876   0.2025832   1.12208    -1.7460632 ]\n",
      "   [-0.38023633 -0.88305074 -1.6188183  -0.4605108 ]\n",
      "   [ 0.7600631  -0.39524692 -3.455463    0.6971765 ]\n",
      "   [ 3.004541   -3.5289714  -9.588761    2.9130647 ]]\n",
      "\n",
      "  [[ 1.9698906   1.0078789  -1.2122052   2.0134254 ]\n",
      "   [ 1.8958745  -0.45892912 -5.3896065   1.8568405 ]\n",
      "   [ 1.4736686   0.538687   -1.965722    1.469285  ]\n",
      "   [ 3.1977854   0.05167901 -5.276443    3.1679316 ]]\n",
      "\n",
      "  [[ 0.1258963  -0.71917385 -2.2927022   0.03860371]\n",
      "   [ 1.3117977  -1.6638893  -4.465428    1.2304243 ]\n",
      "   [ 2.5624385  -3.8971152  -9.53067     2.4499042 ]\n",
      "   [ 2.0999384  -1.6019284  -5.034336    2.0732462 ]]\n",
      "\n",
      "  [[-0.31415597  2.5866678   6.2770805  -0.21654116]\n",
      "   [ 1.1086718  -2.3592036  -5.937715    1.05256   ]\n",
      "   [ 1.0852163  -4.0399203  -9.892566    0.9899011 ]\n",
      "   [ 1.6877527  -2.2720673  -3.6519856   1.6843686 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.77654105 -1.1600666  -0.7377553   0.15321049]\n",
      "   [ 4.2638865  -4.9808917  -4.7282324  -7.588514  ]\n",
      "   [ 3.7290585  -3.6264493  -4.6852508  -2.8495634 ]\n",
      "   [ 3.8470237   1.1639067  -2.604357   -4.39899   ]]\n",
      "\n",
      "  [[ 0.02615874  1.1034771  -0.24971536 -0.62090695]\n",
      "   [-1.1199212  -1.2406197  -0.3052636   2.3498487 ]\n",
      "   [-1.1390502   0.8624328   2.084101    1.3884075 ]\n",
      "   [ 1.2047955  -2.8474126  -2.2027948   1.087519  ]]\n",
      "\n",
      "  [[-2.9237115   3.363596    2.3430805   0.21954887]\n",
      "   [-0.80274636  4.677598    1.8153228   2.9806826 ]\n",
      "   [-2.9179065   2.4044318   0.6520111   6.7438183 ]\n",
      "   [-3.690457    0.7928321   8.471516   -1.8322024 ]]\n",
      "\n",
      "  [[-2.8092232   1.49841     1.9324561   0.96108407]\n",
      "   [-2.5165534   2.19107     0.6257863   7.272254  ]\n",
      "   [-2.6379817   2.7387853  -1.0823029   6.826854  ]\n",
      "   [-2.4187973  -3.4014497   3.5254593   0.5569787 ]]\n",
      "\n",
      "  [[-0.4573368  -0.41149053  0.57131124  0.99204594]\n",
      "   [ 3.5183     -1.5149683  -4.673192   -3.8204358 ]\n",
      "   [-0.12923025  1.2998483  -1.2514737  -1.2474041 ]\n",
      "   [ 1.3779988   4.212641   -3.3267968  -0.59099406]]\n",
      "\n",
      "  [[-1.4886304  -0.8160711   1.272815    1.069158  ]\n",
      "   [ 0.90435994 -2.5749598  -1.9510299  -1.5345374 ]\n",
      "   [-0.34667596 -1.3035538  -2.9048364  -0.24566376]\n",
      "   [ 1.2138464   1.9463997   1.1304235   4.3117375 ]]\n",
      "\n",
      "  [[-2.3999348   1.3400928   0.6617681  -1.8778961 ]\n",
      "   [-1.6690627   0.10749228  0.34268987  1.8814775 ]\n",
      "   [-1.4865489  -3.3957736  -3.0061002   3.6486413 ]\n",
      "   [ 2.238486    0.6319354  -1.8617017  -5.3909755 ]]\n",
      "\n",
      "  [[-1.9244474   2.1994686   1.347579    0.2057227 ]\n",
      "   [-0.34355617 -2.1330175   0.8624636  -2.9745345 ]\n",
      "   [ 0.41986907 -3.7553241  -1.9478018  -1.2957213 ]\n",
      "   [ 0.42432842  0.5639865   4.050384   -1.9052758 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.4390437   -0.33947098   4.013443    -0.29624832]\n",
      "   [ -0.2583174    0.33025625  -0.39602375  -0.30866668]\n",
      "   [  0.32099545  -0.95505685  -4.5404463    0.24053822]\n",
      "   [  0.63142234  -1.2339483   -6.734192     0.53280115]]\n",
      "\n",
      "  [[ -0.43239835   0.42818624   2.6262326   -0.4062209 ]\n",
      "   [ -0.22419338   1.074776     1.1630725   -0.19326892]\n",
      "   [  0.51628953   0.88442105  -0.02622377   0.5438406 ]\n",
      "   [  0.7893162   -3.560948    -7.5051656    0.6630447 ]]\n",
      "\n",
      "  [[ -0.44927993   0.54527193   4.6114116   -0.29629943]\n",
      "   [  0.67831874  -0.902597    -5.0636787    0.530373  ]\n",
      "   [  1.1608257   -0.9582575   -6.0131745    1.0148909 ]\n",
      "   [  1.8622837   -0.54619     -6.1305943    1.7046378 ]]\n",
      "\n",
      "  [[ -1.5574945   -0.13661464   4.8192244   -1.5274014 ]\n",
      "   [  1.8428049    0.10988725  -3.7634213    1.8194834 ]\n",
      "   [  1.8790634    0.16765016  -5.2636895    1.8923659 ]\n",
      "   [  1.8488076    0.38219947  -2.467583     1.8204844 ]]\n",
      "\n",
      "  [[ -1.0361291    1.0311096    2.9110353   -1.0666645 ]\n",
      "   [  0.17529838  -0.76200086  -1.3703144    0.18987674]\n",
      "   [  0.9855322   -1.2880604   -2.9897346    0.9731935 ]\n",
      "   [  0.7058304   -0.60330254  -3.2749028    0.66278076]]\n",
      "\n",
      "  [[ -0.7908757   -1.3115877    0.67647886  -0.8404614 ]\n",
      "   [ -0.19861834  -1.3611583   -2.9659412   -0.28904545]\n",
      "   [ -0.5602697   -0.27112013   0.11186516  -0.6005517 ]\n",
      "   [  0.77641386  -0.26496348  -3.3041744    0.8275802 ]]\n",
      "\n",
      "  [[ -0.21379697  -0.3550262    2.452176    -0.14885877]\n",
      "   [  1.1190844   -0.06087807  -3.8031468    1.0202737 ]\n",
      "   [  0.3503374   -1.1730963   -4.796474     0.2665696 ]\n",
      "   [  0.7476409    0.9311516   -1.8346077    0.6924049 ]]\n",
      "\n",
      "  [[ -1.6940544   -1.1217345    4.9840846   -1.5811146 ]\n",
      "   [  2.1202106   -1.363047   -10.718351     1.9247364 ]\n",
      "   [  0.94408834   1.107803    -3.365614     0.8552538 ]\n",
      "   [  2.3723536   -2.03031    -11.663365     2.1812994 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724   -0.7859426    1.8895346   -4.1808095 ]\n",
      "   [  3.1438417   -2.3516042   -2.567732     3.3101664 ]\n",
      "   [ -6.821511     5.132966    -1.515522    -8.113494  ]\n",
      "   [ -2.6520565    0.29321504   2.1502676   -3.7211149 ]]\n",
      "\n",
      "  [[ -5.830585    -3.796887     3.4242606   -5.8514013 ]\n",
      "   [ -4.592556     0.16063029   2.767142    -4.5720806 ]\n",
      "   [ -3.28553     -1.1929119    0.7792256   -3.4044573 ]\n",
      "   [ -3.889495    -2.520736     2.122136    -3.5668743 ]]\n",
      "\n",
      "  [[  0.21261294  -2.0974307    1.9989094   -1.4663465 ]\n",
      "   [ -8.633069    -8.325801    -1.5773704  -10.09647   ]\n",
      "   [ -8.450605    -4.192688     1.4179147   -7.872258  ]\n",
      "   [ -3.4774246   -1.0960379    2.3552895   -3.6499627 ]]\n",
      "\n",
      "  [[ -0.62310183   0.02354282   0.13477258  -1.0099144 ]\n",
      "   [ -2.3111663   -0.6952288    2.469388    -1.3457242 ]\n",
      "   [ -3.9942987   -1.6525842   -1.1253656   -4.210669  ]\n",
      "   [ -1.8414911    0.44378278   1.8752314   -2.3266878 ]]\n",
      "\n",
      "  [[ -3.365542    -1.0635742   -0.04164821  -1.7994132 ]\n",
      "   [ -6.9826465   -2.3183198    2.294877    -4.308402  ]\n",
      "   [ -5.801219    -3.403646    -4.5645704   -6.675435  ]\n",
      "   [  0.2971933    0.63473487  -0.6389538    2.6413    ]]\n",
      "\n",
      "  [[ -2.1686854   -1.023346     0.65018183  -2.5024278 ]\n",
      "   [ -4.622171    -0.8858723    5.574534    -4.401349  ]\n",
      "   [ -3.8713155    1.6690211    3.75436     -4.813896  ]\n",
      "   [ -1.399356    -0.4949215    2.2085276   -2.2406054 ]]\n",
      "\n",
      "  [[ -0.3049526    2.224203    -1.2487973   -3.0664551 ]\n",
      "   [ -4.3723803    1.9350511    0.1110547   -4.0373297 ]\n",
      "   [ -8.824966    -2.4120855    4.2221565   -6.5589924 ]\n",
      "   [ -2.1759007    2.9709213    0.01304059  -3.8550992 ]]\n",
      "\n",
      "  [[ -2.8292158    0.7908498   -2.1495862   -2.1313064 ]\n",
      "   [ -4.523009    -2.7817159   -3.0650115   -5.429205  ]\n",
      "   [ -5.458025     1.8018078   -4.9602304   -6.058091  ]\n",
      "   [ -4.791121     1.7835791   -2.0525951   -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.3626041   -3.6032155    4.161095     0.0656366 ]\n",
      "   [ -6.182477    -2.5122466    7.009508    -3.584936  ]\n",
      "   [ -3.4696164    1.1780642    5.1291347   -2.1059198 ]\n",
      "   [ -3.483398    -4.1618204    4.4601355   -0.07843098]]\n",
      "\n",
      "  [[  0.510499     1.036337    -1.0375209    0.28767318]\n",
      "   [ -3.3273916   -0.11347406   3.4049916   -3.419848  ]\n",
      "   [ -4.9530396    0.3851128    4.3673553   -5.12493   ]\n",
      "   [ -2.7946234    0.38115668  -0.20627333  -2.9363682 ]]\n",
      "\n",
      "  [[ -9.042869    -3.423595    -4.7580905   -7.272155  ]\n",
      "   [ -5.3064766   -1.7224908   -1.1489992   -3.3164253 ]\n",
      "   [ -5.864008     1.9313622    5.1383986   -4.273402  ]\n",
      "   [-10.078705    -3.3595207   -3.9119687   -7.9476895 ]]\n",
      "\n",
      "  [[  0.704116     0.5427282   -1.371693     1.7133871 ]\n",
      "   [ -7.3349566   -2.3262897    2.0722024   -6.863343  ]\n",
      "   [ -7.6698356   -5.039354     0.01469475  -8.557696  ]\n",
      "   [  0.14490138   1.3311632   -0.6794642    1.1242119 ]]\n",
      "\n",
      "  [[  5.0507197    3.1588564    1.3940698    5.1164274 ]\n",
      "   [-13.740164    -2.4651692    8.651231   -13.1501875 ]\n",
      "   [-13.236485    -2.9835963    8.292635   -12.939023  ]\n",
      "   [  1.3312078    2.3829906    2.8037574    1.0776967 ]]\n",
      "\n",
      "  [[ 12.874086     2.85469     -6.3369064   11.067723  ]\n",
      "   [ -1.5080522   -3.8416598    0.31655705  -2.3005078 ]\n",
      "   [ -5.6672664   -1.453503     2.7044997   -4.9066    ]\n",
      "   [ 10.062546     2.1319842   -5.8750806    8.317264  ]]\n",
      "\n",
      "  [[ -8.929089    -8.014712    -1.0568765   -8.620834  ]\n",
      "   [-10.908268   -10.764266    -0.8827691  -10.867963  ]\n",
      "   [-10.751021    -8.130157     2.3616416   -9.656115  ]\n",
      "   [ -9.412735    -8.36169     -1.0328311   -8.912132  ]]\n",
      "\n",
      "  [[ 10.875551     2.7801728   -1.4670037   12.899784  ]\n",
      "   [  2.4307857    2.7539134    2.4796565    3.4081655 ]\n",
      "   [ -7.5609837   -0.33894548   3.7714336   -8.186357  ]\n",
      "   [  8.785441     2.427588    -1.0477616   10.493459  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    0.86808515   0.68926543  -0.07710785   0.5841838 ]\n",
      "   [ -0.8052174    1.2325178    1.3822585   -0.6516272    0.6315028 ]\n",
      "   [ -0.29754275   1.5583472   -0.8161052    1.4953052    0.10321569]\n",
      "   [ -0.5443256    2.2166584    6.8072824   -9.876017     2.3808532 ]\n",
      "   [  0.7472351   -4.5735664    2.1001763   -0.68935      0.6213042 ]]\n",
      "\n",
      "  [[ -0.1123142    0.22450152   0.1997487   -0.5255737    0.31211773]\n",
      "   [  0.31872883  -4.25332     -2.458986     4.503875    -2.5093522 ]\n",
      "   [  0.06266014  -2.8750339   -5.2976384    1.4493271    2.1512098 ]\n",
      "   [ -0.54926753   4.0720015    2.403759    -3.7070231    0.68023175]\n",
      "   [  0.8732976   -1.8000302    3.722933     1.1266485   -2.602924  ]]\n",
      "\n",
      "  [[ -0.10918517   0.20129828  -0.1020415    0.657444     0.22135806]\n",
      "   [  0.3942903   -3.7951708   -3.841178    -0.39951256   0.29694498]\n",
      "   [ -0.45792332  -2.8219523   -3.4908211   -0.8908195    1.6258212 ]\n",
      "   [  0.4032216    1.4782436    8.961608    -4.7050643    4.6159906 ]\n",
      "   [ -0.39189684   4.1333766    9.57504      1.0154871    0.6772467 ]]\n",
      "\n",
      "  [[ -0.22114104   0.78687924   0.38468125  -0.35858485   0.4814524 ]\n",
      "   [ -0.32672447   1.066411     0.20903866  -0.35502467   1.9553363 ]\n",
      "   [  0.17269178  -1.3044931   -4.883713    -0.30554432   0.34996486]\n",
      "   [  0.08333597   1.9566892    3.4552226   -3.034405     1.9709632 ]\n",
      "   [  0.19127834  -0.42828786   4.3851147    0.4973215    2.414637  ]]\n",
      "\n",
      "  [[  0.00076126  -0.47878256   0.09911879   0.00893964   0.24509   ]\n",
      "   [  0.7206705   -4.158416    -1.74024      4.0903745   -0.7901846 ]\n",
      "   [  0.12035121   5.0561695   -2.9415672   -0.26743913  -4.871599  ]\n",
      "   [  0.03012125   5.2640715    3.1126494   -4.628424    -2.5972195 ]\n",
      "   [  0.44786143   4.194443     3.1239393   -2.8355985   -5.2169623 ]]\n",
      "\n",
      "  [[ -0.00999826  -0.17336224  -0.04885251  -0.06919655  -0.32633722]\n",
      "   [  1.4367385   -9.06984     -3.8178802   -0.75747144  -3.7752612 ]\n",
      "   [  0.92183876  -4.7562103   -6.4032483   -2.5681021   -2.3384533 ]\n",
      "   [ -0.15983817   5.33236      4.7559114    0.13260181   1.7207963 ]\n",
      "   [  0.10523379   1.6092048    6.520943    -0.82788485  -2.471699  ]]\n",
      "\n",
      "  [[ -0.20863976   0.569198     0.44341293   0.16857311   0.1010042 ]\n",
      "   [ -0.44067878  -1.3154124    2.2516541    5.957117     3.327574  ]\n",
      "   [ -0.73116076   0.78557914   1.7380664    4.084935     3.872323  ]\n",
      "   [  2.1423054   -2.7513258   -6.220788    -6.2902803   -5.0086665 ]\n",
      "   [  2.4718273   -3.7755775   -3.795776    -5.034314   -10.499384  ]]\n",
      "\n",
      "  [[ -0.15726982   0.34657618   0.09771852  -0.06033853   0.13718088]\n",
      "   [ -0.20781471  -3.9169958   -2.6317358    2.4233148    1.5008358 ]\n",
      "   [  0.49460515  -2.8374786   -6.512395    -2.2088082   -3.047029  ]\n",
      "   [  0.40351048   5.496288     7.4884925   -5.00473     -0.34395367]\n",
      "   [  0.20931137   1.581105     5.1500797   -1.7881161   -0.8540002 ]]]], shape=(1, 8, 5, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 5)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.43004218 -2.1264591  -4.20363     0.41727006]\n",
      "   [ 0.36214504 -0.9224591  -3.69376     0.32345214]\n",
      "   [-0.11888524  0.7827182   1.6091667  -0.07043942]\n",
      "   [ 1.5769911  -1.5107999  -4.705859    1.5822129 ]\n",
      "   [ 1.433958    0.35687694 -0.44859353  1.4714174 ]]\n",
      "\n",
      "  [[ 1.3539228  -1.1119139  -3.6765318   1.3366559 ]\n",
      "   [ 0.49521226  0.8556765   1.0242827   0.54091555]\n",
      "   [ 1.8845222   0.80958265 -1.4450098   1.8755151 ]\n",
      "   [ 3.1395586  -0.3902885  -4.6706777   3.0263221 ]\n",
      "   [ 2.4448514  -0.79036796 -4.79417     2.363135  ]]\n",
      "\n",
      "  [[-0.6358987   0.6857092   3.934223   -0.6082751 ]\n",
      "   [ 2.1257792  -0.75171655 -5.1641245   2.0634513 ]\n",
      "   [ 1.9908828  -2.4718666  -8.323398    1.8416339 ]\n",
      "   [ 1.4525386  -1.9171642  -6.0657043   1.3277422 ]\n",
      "   [ 2.0609357  -2.3901558  -8.3400135   1.8740809 ]]\n",
      "\n",
      "  [[-0.5390432   0.7702814   1.6837941  -0.4367591 ]\n",
      "   [ 1.0922868  -2.2901568  -6.70844     0.99209934]\n",
      "   [ 0.63555646 -0.9029067  -2.4676714   0.5942919 ]\n",
      "   [ 1.3171933  -1.6731497  -4.4172173   1.2324891 ]\n",
      "   [ 1.0370547  -2.4209025  -5.395325    0.9418529 ]]\n",
      "\n",
      "  [[-1.7341876   0.2025832   1.12208    -1.7460632 ]\n",
      "   [-0.38023633 -0.88305074 -1.6188183  -0.4605108 ]\n",
      "   [ 0.7600631  -0.39524692 -3.455463    0.6971765 ]\n",
      "   [ 3.004541   -3.5289714  -9.588761    2.9130647 ]\n",
      "   [ 2.5986595  -3.3891108  -8.484004    2.4783466 ]]\n",
      "\n",
      "  [[ 1.9698906   1.0078789  -1.2122052   2.0134254 ]\n",
      "   [ 1.8958745  -0.45892912 -5.3896065   1.8568405 ]\n",
      "   [ 1.4736686   0.538687   -1.965722    1.469285  ]\n",
      "   [ 3.1977854   0.05167901 -5.276443    3.1679316 ]\n",
      "   [ 2.3644834   0.91031945 -2.6867223   2.3744936 ]]\n",
      "\n",
      "  [[ 0.1258963  -0.71917385 -2.2927022   0.03860371]\n",
      "   [ 1.3117977  -1.6638893  -4.465428    1.2304243 ]\n",
      "   [ 2.5624385  -3.8971152  -9.53067     2.4499042 ]\n",
      "   [ 2.0999384  -1.6019284  -5.034336    2.0732462 ]\n",
      "   [ 0.7817824  -1.5657676  -3.6030664   0.7704573 ]]\n",
      "\n",
      "  [[-0.31415597  2.5866678   6.2770805  -0.21654116]\n",
      "   [ 1.1086718  -2.3592036  -5.937715    1.05256   ]\n",
      "   [ 1.0852163  -4.0399203  -9.892566    0.9899011 ]\n",
      "   [ 1.6877527  -2.2720673  -3.6519856   1.6843686 ]\n",
      "   [ 1.2909392  -2.185475   -4.1043363   1.2519479 ]]]], shape=(1, 8, 5, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 4)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.77654105 -1.1600666  -0.7377553   0.15321049 -0.33445883]\n",
      "   [ 4.2638865  -4.9808917  -4.7282324  -7.588514   -5.1648345 ]\n",
      "   [ 3.7290585  -3.6264493  -4.6852508  -2.8495634  -3.194244  ]\n",
      "   [ 3.8470237   1.1639067  -2.604357   -4.39899    -3.9771621 ]\n",
      "   [ 5.5331926  -2.1469176  -7.7634535  -5.166938   -6.2337794 ]]\n",
      "\n",
      "  [[ 0.02615874  1.1034771  -0.24971536 -0.62090695  0.00287016]\n",
      "   [-1.1199212  -1.2406197  -0.3052636   2.3498487   1.2994226 ]\n",
      "   [-1.1390502   0.8624328   2.084101    1.3884075  -0.980912  ]\n",
      "   [ 1.2047955  -2.8474126  -2.2027948   1.087519    1.6147972 ]\n",
      "   [ 3.020494   -0.39609548 -2.2673187  -2.7705114  -3.26368   ]]\n",
      "\n",
      "  [[-2.9237115   3.363596    2.3430805   0.21954887 -0.1109451 ]\n",
      "   [-0.80274636  4.677598    1.8153228   2.9806826   2.4021566 ]\n",
      "   [-2.9179065   2.4044318   0.6520111   6.7438183   2.0902429 ]\n",
      "   [-3.690457    0.7928321   8.471516   -1.8322024   4.8284726 ]\n",
      "   [ 1.9101584   1.9778312   1.4799114   3.4420455   4.4670367 ]]\n",
      "\n",
      "  [[-2.8092232   1.49841     1.9324561   0.96108407  1.7129885 ]\n",
      "   [-2.5165534   2.19107     0.6257863   7.272254    2.259812  ]\n",
      "   [-2.6379817   2.7387853  -1.0823029   6.826854    1.8786801 ]\n",
      "   [-2.4187973  -3.4014497   3.5254593   0.5569787   5.067711  ]\n",
      "   [-1.945622   -0.65303457  2.4640982   3.3566499   4.1663265 ]]\n",
      "\n",
      "  [[-0.4573368  -0.41149053  0.57131124  0.99204594  0.7350709 ]\n",
      "   [ 3.5183     -1.5149683  -4.673192   -3.8204358  -2.732508  ]\n",
      "   [-0.12923025  1.2998483  -1.2514737  -1.2474041  -0.5723316 ]\n",
      "   [ 1.3779988   4.212641   -3.3267968  -0.59099406  3.2183704 ]\n",
      "   [ 1.6397702   0.8696879  -2.9280584  -0.5760767  -0.3494962 ]]\n",
      "\n",
      "  [[-1.4886304  -0.8160711   1.272815    1.069158   -1.2259004 ]\n",
      "   [ 0.90435994 -2.5749598  -1.9510299  -1.5345374  -0.7728811 ]\n",
      "   [-0.34667596 -1.3035538  -2.9048364  -0.24566376 -0.21664956]\n",
      "   [ 1.2138464   1.9463997   1.1304235   4.3117375  -0.93706936]\n",
      "   [ 2.2285655   0.59783006  2.3479755   1.71059    -4.110444  ]]\n",
      "\n",
      "  [[-2.3999348   1.3400928   0.6617681  -1.8778961  -0.45578986]\n",
      "   [-1.6690627   0.10749228  0.34268987  1.8814775   2.1918917 ]\n",
      "   [-1.4865489  -3.3957736  -3.0061002   3.6486413   3.2406335 ]\n",
      "   [ 2.238486    0.6319354  -1.8617017  -5.3909755  -1.2285255 ]\n",
      "   [ 2.793881   -1.9779224  -3.7091935  -1.6687323  -2.2475994 ]]\n",
      "\n",
      "  [[-1.9244474   2.1994686   1.347579    0.2057227  -0.04820146]\n",
      "   [-0.34355617 -2.1330175   0.8624636  -2.9745345  -1.522738  ]\n",
      "   [ 0.41986907 -3.7553241  -1.9478018  -1.2957213  -3.4820578 ]\n",
      "   [ 0.42432842  0.5639865   4.050384   -1.9052758   1.185033  ]\n",
      "   [ 0.7143441  -2.7222555   1.654944   -0.4344939  -3.5097253 ]]]], shape=(1, 8, 5, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 5)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.4390437   -0.33947098   4.013443    -0.29624832]\n",
      "   [ -0.2583174    0.33025625  -0.39602375  -0.30866668]\n",
      "   [  0.32099545  -0.95505685  -4.5404463    0.24053822]\n",
      "   [  0.63142234  -1.2339483   -6.734192     0.53280115]\n",
      "   [  0.54130554   0.21318378  -0.52515334   0.56435126]]\n",
      "\n",
      "  [[ -0.43239835   0.42818624   2.6262326   -0.4062209 ]\n",
      "   [ -0.22419338   1.074776     1.1630725   -0.19326892]\n",
      "   [  0.51628953   0.88442105  -0.02622377   0.5438406 ]\n",
      "   [  0.7893162   -3.560948    -7.5051656    0.6630447 ]\n",
      "   [  0.57238746  -1.7088602   -4.2579656    0.5265007 ]]\n",
      "\n",
      "  [[ -0.44927993   0.54527193   4.6114116   -0.29629943]\n",
      "   [  0.67831874  -0.902597    -5.0636787    0.530373  ]\n",
      "   [  1.1608257   -0.9582575   -6.0131745    1.0148909 ]\n",
      "   [  1.8622837   -0.54619     -6.1305943    1.7046378 ]\n",
      "   [  0.5365511   -0.7164705   -4.3196692    0.4233776 ]]\n",
      "\n",
      "  [[ -1.5574945   -0.13661464   4.8192244   -1.5274014 ]\n",
      "   [  1.8428049    0.10988725  -3.7634213    1.8194834 ]\n",
      "   [  1.8790634    0.16765016  -5.2636895    1.8923659 ]\n",
      "   [  1.8488076    0.38219947  -2.467583     1.8204844 ]\n",
      "   [  1.9478984    0.24887773  -4.0790386    1.8869959 ]]\n",
      "\n",
      "  [[ -1.0361291    1.0311096    2.9110353   -1.0666645 ]\n",
      "   [  0.17529838  -0.76200086  -1.3703144    0.18987674]\n",
      "   [  0.9855322   -1.2880604   -2.9897346    0.9731935 ]\n",
      "   [  0.7058304   -0.60330254  -3.2749028    0.66278076]\n",
      "   [  0.73217136   0.31563684  -1.2379584    0.7082298 ]]\n",
      "\n",
      "  [[ -0.7908757   -1.3115877    0.67647886  -0.8404614 ]\n",
      "   [ -0.19861834  -1.3611583   -2.9659412   -0.28904545]\n",
      "   [ -0.5602697   -0.27112013   0.11186516  -0.6005517 ]\n",
      "   [  0.77641386  -0.26496348  -3.3041744    0.8275802 ]\n",
      "   [  1.4017886    1.7031087   -1.5688373    1.5331511 ]]\n",
      "\n",
      "  [[ -0.21379697  -0.3550262    2.452176    -0.14885877]\n",
      "   [  1.1190844   -0.06087807  -3.8031468    1.0202737 ]\n",
      "   [  0.3503374   -1.1730963   -4.796474     0.2665696 ]\n",
      "   [  0.7476409    0.9311516   -1.8346077    0.6924049 ]\n",
      "   [  0.8267951    0.640895    -1.8839159    0.7792115 ]]\n",
      "\n",
      "  [[ -1.6940544   -1.1217345    4.9840846   -1.5811146 ]\n",
      "   [  2.1202106   -1.363047   -10.718351     1.9247364 ]\n",
      "   [  0.94408834   1.107803    -3.365614     0.8552538 ]\n",
      "   [  2.3723536   -2.03031    -11.663365     2.1812994 ]\n",
      "   [  1.1664867   -1.2401501   -6.0198855    1.0814999 ]]]], shape=(1, 8, 5, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 4)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724   -0.7859426    1.8895346   -4.1808095 ]\n",
      "   [  3.1438417   -2.3516042   -2.567732     3.3101664 ]\n",
      "   [ -6.821511     5.132966    -1.515522    -8.113494  ]\n",
      "   [ -2.6520565    0.29321504   2.1502676   -3.7211149 ]]\n",
      "\n",
      "  [[ -5.830585    -3.796887     3.4242606   -5.8514013 ]\n",
      "   [ -4.592556     0.16063029   2.767142    -4.5720806 ]\n",
      "   [ -3.28553     -1.1929119    0.7792256   -3.4044573 ]\n",
      "   [ -3.889495    -2.520736     2.122136    -3.5668743 ]]\n",
      "\n",
      "  [[  0.21261294  -2.0974307    1.9989094   -1.4663465 ]\n",
      "   [ -8.633069    -8.325801    -1.5773704  -10.09647   ]\n",
      "   [ -8.450605    -4.192688     1.4179147   -7.872258  ]\n",
      "   [ -3.4774246   -1.0960379    2.3552895   -3.6499627 ]]\n",
      "\n",
      "  [[ -0.62310183   0.02354282   0.13477258  -1.0099144 ]\n",
      "   [ -2.3111663   -0.6952288    2.469388    -1.3457242 ]\n",
      "   [ -3.9942987   -1.6525842   -1.1253656   -4.210669  ]\n",
      "   [ -1.8414911    0.44378278   1.8752314   -2.3266878 ]]\n",
      "\n",
      "  [[ -3.365542    -1.0635742   -0.04164821  -1.7994132 ]\n",
      "   [ -6.9826465   -2.3183198    2.294877    -4.308402  ]\n",
      "   [ -5.801219    -3.403646    -4.5645704   -6.675435  ]\n",
      "   [  0.2971933    0.63473487  -0.6389538    2.6413    ]]\n",
      "\n",
      "  [[ -2.1686854   -1.023346     0.65018183  -2.5024278 ]\n",
      "   [ -4.622171    -0.8858723    5.574534    -4.401349  ]\n",
      "   [ -3.8713155    1.6690211    3.75436     -4.813896  ]\n",
      "   [ -1.399356    -0.4949215    2.2085276   -2.2406054 ]]\n",
      "\n",
      "  [[ -0.3049526    2.224203    -1.2487973   -3.0664551 ]\n",
      "   [ -4.3723803    1.9350511    0.1110547   -4.0373297 ]\n",
      "   [ -8.824966    -2.4120855    4.2221565   -6.5589924 ]\n",
      "   [ -2.1759007    2.9709213    0.01304059  -3.8550992 ]]\n",
      "\n",
      "  [[ -2.8292158    0.7908498   -2.1495862   -2.1313064 ]\n",
      "   [ -4.523009    -2.7817159   -3.0650115   -5.429205  ]\n",
      "   [ -5.458025     1.8018078   -4.9602304   -6.058091  ]\n",
      "   [ -4.791121     1.7835791   -2.0525951   -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.3626041   -3.6032155    4.161095     0.0656366 ]\n",
      "   [ -6.182477    -2.5122466    7.009508    -3.584936  ]\n",
      "   [ -3.4696164    1.1780642    5.1291347   -2.1059198 ]\n",
      "   [ -3.483398    -4.1618204    4.4601355   -0.07843098]]\n",
      "\n",
      "  [[  0.510499     1.036337    -1.0375209    0.28767318]\n",
      "   [ -3.3273916   -0.11347406   3.4049916   -3.419848  ]\n",
      "   [ -4.9530396    0.3851128    4.3673553   -5.12493   ]\n",
      "   [ -2.7946234    0.38115668  -0.20627333  -2.9363682 ]]\n",
      "\n",
      "  [[ -9.042869    -3.423595    -4.7580905   -7.272155  ]\n",
      "   [ -5.3064766   -1.7224908   -1.1489992   -3.3164253 ]\n",
      "   [ -5.864008     1.9313622    5.1383986   -4.273402  ]\n",
      "   [-10.078705    -3.3595207   -3.9119687   -7.9476895 ]]\n",
      "\n",
      "  [[  0.704116     0.5427282   -1.371693     1.7133871 ]\n",
      "   [ -7.3349566   -2.3262897    2.0722024   -6.863343  ]\n",
      "   [ -7.6698356   -5.039354     0.01469475  -8.557696  ]\n",
      "   [  0.14490138   1.3311632   -0.6794642    1.1242119 ]]\n",
      "\n",
      "  [[  5.0507197    3.1588564    1.3940698    5.1164274 ]\n",
      "   [-13.740164    -2.4651692    8.651231   -13.1501875 ]\n",
      "   [-13.236485    -2.9835963    8.292635   -12.939023  ]\n",
      "   [  1.3312078    2.3829906    2.8037574    1.0776967 ]]\n",
      "\n",
      "  [[ 12.874086     2.85469     -6.3369064   11.067723  ]\n",
      "   [ -1.5080522   -3.8416598    0.31655705  -2.3005078 ]\n",
      "   [ -5.6672664   -1.453503     2.7044997   -4.9066    ]\n",
      "   [ 10.062546     2.1319842   -5.8750806    8.317264  ]]\n",
      "\n",
      "  [[ -8.929089    -8.014712    -1.0568765   -8.620834  ]\n",
      "   [-10.908268   -10.764266    -0.8827691  -10.867963  ]\n",
      "   [-10.751021    -8.130157     2.3616416   -9.656115  ]\n",
      "   [ -9.412735    -8.36169     -1.0328311   -8.912132  ]]\n",
      "\n",
      "  [[ 10.875551     2.7801728   -1.4670037   12.899784  ]\n",
      "   [  2.4307857    2.7539134    2.4796565    3.4081655 ]\n",
      "   [ -7.5609837   -0.33894548   3.7714336   -8.186357  ]\n",
      "   [  8.785441     2.427588    -1.0477616   10.493459  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    0.86808515   0.68926543  -0.07710785   0.5841838\n",
      "      0.57039815]\n",
      "   [ -0.8052174    1.2325178    1.3822585   -0.6516272    0.6315028\n",
      "     -0.14370728]\n",
      "   [ -0.29754275   1.5583472   -0.8161052    1.4953052    0.10321569\n",
      "     -3.3889081 ]\n",
      "   [ -0.5443256    2.2166584    6.8072824   -9.876017     2.3808532\n",
      "      8.322778  ]\n",
      "   [  0.7472351   -4.5735664    2.1001763   -0.68935      0.6213042\n",
      "      6.4843163 ]\n",
      "   [ -0.29372013   1.6946385   -2.4606276   -1.9047768    3.2469358\n",
      "      2.6467237 ]]\n",
      "\n",
      "  [[ -0.1123142    0.22450152   0.1997487   -0.5255737    0.31211773\n",
      "     -0.39361402]\n",
      "   [  0.31872883  -4.25332     -2.458986     4.503875    -2.5093522\n",
      "      0.16508146]\n",
      "   [  0.06266014  -2.8750339   -5.2976384    1.4493271    2.1512098\n",
      "     -1.3659165 ]\n",
      "   [ -0.54926753   4.0720015    2.403759    -3.7070231    0.68023175\n",
      "      0.05022379]\n",
      "   [  0.8732976   -1.8000302    3.722933     1.1266485   -2.602924\n",
      "      3.7650833 ]\n",
      "   [  0.33890194   2.1156766    0.45062694   1.4144641   -3.823428\n",
      "     -5.970876  ]]\n",
      "\n",
      "  [[ -0.10918517   0.20129828  -0.1020415    0.657444     0.22135806\n",
      "      0.15187913]\n",
      "   [  0.3942903   -3.7951708   -3.841178    -0.39951256   0.29694498\n",
      "     -0.4107963 ]\n",
      "   [ -0.45792332  -2.8219523   -3.4908211   -0.8908195    1.6258212\n",
      "     -2.935929  ]\n",
      "   [  0.4032216    1.4782436    8.961608    -4.7050643    4.6159906\n",
      "      4.017976  ]\n",
      "   [ -0.39189684   4.1333766    9.57504      1.0154871    0.6772467\n",
      "      0.85469484]\n",
      "   [ -0.6209384   -0.1761216    3.239656     0.448386     2.965093\n",
      "      0.04321224]]\n",
      "\n",
      "  [[ -0.22114104   0.78687924   0.38468125  -0.35858485   0.4814524\n",
      "      0.7716349 ]\n",
      "   [ -0.32672447   1.066411     0.20903866  -0.35502467   1.9553363\n",
      "      2.3845599 ]\n",
      "   [  0.17269178  -1.3044931   -4.883713    -0.30554432   0.34996486\n",
      "     -3.483534  ]\n",
      "   [  0.08333597   1.9566892    3.4552226   -3.034405     1.9709632\n",
      "     -0.00770508]\n",
      "   [  0.19127834  -0.42828786   4.3851147    0.4973215    2.414637\n",
      "      2.6963992 ]\n",
      "   [ -0.02093794   3.6430805    8.713464    -0.52347636   9.823708\n",
      "     -0.37148795]]\n",
      "\n",
      "  [[  0.00076126  -0.47878256   0.09911879   0.00893964   0.24509\n",
      "      0.14304979]\n",
      "   [  0.7206705   -4.158416    -1.74024      4.0903745   -0.7901846\n",
      "     -2.9663303 ]\n",
      "   [  0.12035121   5.0561695   -2.9415672   -0.26743913  -4.871599\n",
      "      1.4902955 ]\n",
      "   [  0.03012125   5.2640715    3.1126494   -4.628424    -2.5972195\n",
      "      1.9151638 ]\n",
      "   [  0.44786143   4.194443     3.1239393   -2.8355985   -5.2169623\n",
      "      3.1980612 ]\n",
      "   [  0.61835045  -3.2027795    1.4719862    3.313971     2.5021975\n",
      "     -1.5920473 ]]\n",
      "\n",
      "  [[ -0.00999826  -0.17336224  -0.04885251  -0.06919655  -0.32633722\n",
      "      0.0366222 ]\n",
      "   [  1.4367385   -9.06984     -3.8178802   -0.75747144  -3.7752612\n",
      "     -7.119363  ]\n",
      "   [  0.92183876  -4.7562103   -6.4032483   -2.5681021   -2.3384533\n",
      "     -3.918842  ]\n",
      "   [ -0.15983817   5.33236      4.7559114    0.13260181   1.7207963\n",
      "      4.4075403 ]\n",
      "   [  0.10523379   1.6092048    6.520943    -0.82788485  -2.471699\n",
      "      2.2942595 ]\n",
      "   [  0.71476805  -5.8466797   -1.9964423   -1.6363546   -0.9318489\n",
      "     -4.4617267 ]]\n",
      "\n",
      "  [[ -0.20863976   0.569198     0.44341293   0.16857311   0.1010042\n",
      "      0.3144073 ]\n",
      "   [ -0.44067878  -1.3154124    2.2516541    5.957117     3.327574\n",
      "      2.1122663 ]\n",
      "   [ -0.73116076   0.78557914   1.7380664    4.084935     3.872323\n",
      "      3.7347608 ]\n",
      "   [  2.1423054   -2.7513258   -6.220788    -6.2902803   -5.0086665\n",
      "     -6.650022  ]\n",
      "   [  2.4718273   -3.7755775   -3.795776    -5.034314   -10.499384\n",
      "     -6.276488  ]\n",
      "   [  1.5774866   -2.8405964   -6.4159265   -3.2861729   -4.428243\n",
      "     -9.614207  ]]\n",
      "\n",
      "  [[ -0.15726982   0.34657618   0.09771852  -0.06033853   0.13718088\n",
      "      0.4450618 ]\n",
      "   [ -0.20781471  -3.9169958   -2.6317358    2.4233148    1.5008358\n",
      "      2.266367  ]\n",
      "   [  0.49460515  -2.8374786   -6.512395    -2.2088082   -3.047029\n",
      "     -2.7998128 ]\n",
      "   [  0.40351048   5.496288     7.4884925   -5.00473     -0.34395367\n",
      "      0.5531984 ]\n",
      "   [  0.20931137   1.581105     5.1500797   -1.7881161   -0.8540002\n",
      "     -1.366854  ]\n",
      "   [  0.27719426   3.7286282    1.4238677   -2.3924153    6.4014854\n",
      "     -0.2659451 ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.43004218 -2.1264591  -4.20363     0.41727006]\n",
      "   [ 0.36214504 -0.9224591  -3.69376     0.32345214]\n",
      "   [-0.11888524  0.7827182   1.6091667  -0.07043942]\n",
      "   [ 1.5769911  -1.5107999  -4.705859    1.5822129 ]\n",
      "   [ 1.433958    0.35687694 -0.44859353  1.4714174 ]\n",
      "   [ 0.6759765  -2.6841862  -5.7552285   0.64645445]]\n",
      "\n",
      "  [[ 1.3539228  -1.1119139  -3.6765318   1.3366559 ]\n",
      "   [ 0.49521226  0.8556765   1.0242827   0.54091555]\n",
      "   [ 1.8845222   0.80958265 -1.4450098   1.8755151 ]\n",
      "   [ 3.1395586  -0.3902885  -4.6706777   3.0263221 ]\n",
      "   [ 2.4448514  -0.79036796 -4.79417     2.363135  ]\n",
      "   [ 1.9017469  -1.5351089  -6.007563    1.8183545 ]]\n",
      "\n",
      "  [[-0.6358987   0.6857092   3.934223   -0.6082751 ]\n",
      "   [ 2.1257792  -0.75171655 -5.1641245   2.0634513 ]\n",
      "   [ 1.9908828  -2.4718666  -8.323398    1.8416339 ]\n",
      "   [ 1.4525386  -1.9171642  -6.0657043   1.3277422 ]\n",
      "   [ 2.0609357  -2.3901558  -8.3400135   1.8740809 ]\n",
      "   [ 2.1479387  -0.04788223 -3.5392594   2.0927868 ]]\n",
      "\n",
      "  [[-0.5390432   0.7702814   1.6837941  -0.4367591 ]\n",
      "   [ 1.0922868  -2.2901568  -6.70844     0.99209934]\n",
      "   [ 0.63555646 -0.9029067  -2.4676714   0.5942919 ]\n",
      "   [ 1.3171933  -1.6731497  -4.4172173   1.2324891 ]\n",
      "   [ 1.0370547  -2.4209025  -5.395325    0.9418529 ]\n",
      "   [ 0.70518225 -1.6686244  -4.040526    0.64739156]]\n",
      "\n",
      "  [[-1.7341876   0.2025832   1.12208    -1.7460632 ]\n",
      "   [-0.38023633 -0.88305074 -1.6188183  -0.4605108 ]\n",
      "   [ 0.7600631  -0.39524692 -3.455463    0.6971765 ]\n",
      "   [ 3.004541   -3.5289714  -9.588761    2.9130647 ]\n",
      "   [ 2.5986595  -3.3891108  -8.484004    2.4783466 ]\n",
      "   [ 2.6414726  -3.1978264  -6.90967     2.5648355 ]]\n",
      "\n",
      "  [[ 1.9698906   1.0078789  -1.2122052   2.0134254 ]\n",
      "   [ 1.8958745  -0.45892912 -5.3896065   1.8568405 ]\n",
      "   [ 1.4736686   0.538687   -1.965722    1.469285  ]\n",
      "   [ 3.1977854   0.05167901 -5.276443    3.1679316 ]\n",
      "   [ 2.3644834   0.91031945 -2.6867223   2.3744936 ]\n",
      "   [ 1.4999217  -3.062734   -6.6033297   1.5126845 ]]\n",
      "\n",
      "  [[ 0.1258963  -0.71917385 -2.2927022   0.03860371]\n",
      "   [ 1.3117977  -1.6638893  -4.465428    1.2304243 ]\n",
      "   [ 2.5624385  -3.8971152  -9.53067     2.4499042 ]\n",
      "   [ 2.0999384  -1.6019284  -5.034336    2.0732462 ]\n",
      "   [ 0.7817824  -1.5657676  -3.6030664   0.7704573 ]\n",
      "   [ 1.6116695  -3.2206306  -7.0788507   1.4964689 ]]\n",
      "\n",
      "  [[-0.31415597  2.5866678   6.2770805  -0.21654116]\n",
      "   [ 1.1086718  -2.3592036  -5.937715    1.05256   ]\n",
      "   [ 1.0852163  -4.0399203  -9.892566    0.9899011 ]\n",
      "   [ 1.6877527  -2.2720673  -3.6519856   1.6843686 ]\n",
      "   [ 1.2909392  -2.185475   -4.1043363   1.2519479 ]\n",
      "   [ 2.045505   -2.9272645  -7.5192847   2.0481453 ]]]], shape=(1, 8, 6, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 4)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.77654105 -1.1600666  -0.7377553   0.15321049 -0.33445883\n",
      "     0.9638917 ]\n",
      "   [ 4.2638865  -4.9808917  -4.7282324  -7.588514   -5.1648345\n",
      "     0.752576  ]\n",
      "   [ 3.7290585  -3.6264493  -4.6852508  -2.8495634  -3.194244\n",
      "     2.0674891 ]\n",
      "   [ 3.8470237   1.1639067  -2.604357   -4.39899    -3.9771621\n",
      "    -0.7163415 ]\n",
      "   [ 5.5331926  -2.1469176  -7.7634535  -5.166938   -6.2337794\n",
      "     1.4473847 ]\n",
      "   [ 4.7488627   0.8060199  -2.7915359   0.2651618   2.8968983\n",
      "    -4.7307568 ]]\n",
      "\n",
      "  [[ 0.02615874  1.1034771  -0.24971536 -0.62090695  0.00287016\n",
      "     0.03166713]\n",
      "   [-1.1199212  -1.2406197  -0.3052636   2.3498487   1.2994226\n",
      "    -0.1909589 ]\n",
      "   [-1.1390502   0.8624328   2.084101    1.3884075  -0.980912\n",
      "     1.0793152 ]\n",
      "   [ 1.2047955  -2.8474126  -2.2027948   1.087519    1.6147972\n",
      "     0.44935256]\n",
      "   [ 3.020494   -0.39609548 -2.2673187  -2.7705114  -3.26368\n",
      "    -0.7973336 ]\n",
      "   [-1.8445143  -3.3527074   1.6643316   5.427364    4.1477027\n",
      "     1.6791626 ]]\n",
      "\n",
      "  [[-2.9237115   3.363596    2.3430805   0.21954887 -0.1109451\n",
      "     0.492053  ]\n",
      "   [-0.80274636  4.677598    1.8153228   2.9806826   2.4021566\n",
      "    -1.9523367 ]\n",
      "   [-2.9179065   2.4044318   0.6520111   6.7438183   2.0902429\n",
      "    -2.2136843 ]\n",
      "   [-3.690457    0.7928321   8.471516   -1.8322024   4.8284726\n",
      "     2.881544  ]\n",
      "   [ 1.9101584   1.9778312   1.4799114   3.4420455   4.4670367\n",
      "    -1.8857652 ]\n",
      "   [-1.2988942  -0.97658354 -0.9146665   2.3100488  -1.0963827\n",
      "     0.26074374]]\n",
      "\n",
      "  [[-2.8092232   1.49841     1.9324561   0.96108407  1.7129885\n",
      "     1.6189119 ]\n",
      "   [-2.5165534   2.19107     0.6257863   7.272254    2.259812\n",
      "     3.410876  ]\n",
      "   [-2.6379817   2.7387853  -1.0823029   6.826854    1.8786801\n",
      "     2.3327372 ]\n",
      "   [-2.4187973  -3.4014497   3.5254593   0.5569787   5.067711\n",
      "     0.5633194 ]\n",
      "   [-1.945622   -0.65303457  2.4640982   3.3566499   4.1663265\n",
      "    -1.1195359 ]\n",
      "   [-4.171936   -0.01147364  3.4513283   3.152668    4.512732\n",
      "     9.864028  ]]\n",
      "\n",
      "  [[-0.4573368  -0.41149053  0.57131124  0.99204594  0.7350709\n",
      "     0.20620687]\n",
      "   [ 3.5183     -1.5149683  -4.673192   -3.8204358  -2.732508\n",
      "     0.3246554 ]\n",
      "   [-0.12923025  1.2998483  -1.2514737  -1.2474041  -0.5723316\n",
      "    -2.1154923 ]\n",
      "   [ 1.3779988   4.212641   -3.3267968  -0.59099406  3.2183704\n",
      "    -5.157247  ]\n",
      "   [ 1.6397702   0.8696879  -2.9280584  -0.5760767  -0.3494962\n",
      "    -2.4273357 ]\n",
      "   [ 0.33756587  2.3127203  -0.16594982  2.7338834   6.459333\n",
      "    -3.8244364 ]]\n",
      "\n",
      "  [[-1.4886304  -0.8160711   1.272815    1.069158   -1.2259004\n",
      "     0.27663958]\n",
      "   [ 0.90435994 -2.5749598  -1.9510299  -1.5345374  -0.7728811\n",
      "    -1.8463182 ]\n",
      "   [-0.34667596 -1.3035538  -2.9048364  -0.24566376 -0.21664956\n",
      "    -2.8083339 ]\n",
      "   [ 1.2138464   1.9463997   1.1304235   4.3117375  -0.93706936\n",
      "    -2.5150895 ]\n",
      "   [ 2.2285655   0.59783006  2.3479755   1.71059    -4.110444\n",
      "    -4.411427  ]\n",
      "   [ 0.04869156  4.3527355   3.3943183   3.6977544   2.4533684\n",
      "    -0.86546636]]\n",
      "\n",
      "  [[-2.3999348   1.3400928   0.6617681  -1.8778961  -0.45578986\n",
      "     2.2384698 ]\n",
      "   [-1.6690627   0.10749228  0.34268987  1.8814775   2.1918917\n",
      "     0.12542067]\n",
      "   [-1.4865489  -3.3957736  -3.0061002   3.6486413   3.2406335\n",
      "     1.885369  ]\n",
      "   [ 2.238486    0.6319354  -1.8617017  -5.3909755  -1.2285255\n",
      "    -2.478067  ]\n",
      "   [ 2.793881   -1.9779224  -3.7091935  -1.6687323  -2.2475994\n",
      "    -2.747856  ]\n",
      "   [ 2.2604463   0.89827144 -2.7981474   0.4819163   1.2817676\n",
      "    -2.3887415 ]]\n",
      "\n",
      "  [[-1.9244474   2.1994686   1.347579    0.2057227  -0.04820146\n",
      "     2.141403  ]\n",
      "   [-0.34355617 -2.1330175   0.8624636  -2.9745345  -1.522738\n",
      "     0.9593218 ]\n",
      "   [ 0.41986907 -3.7553241  -1.9478018  -1.2957213  -3.4820578\n",
      "    -2.2611165 ]\n",
      "   [ 0.42432842  0.5639865   4.050384   -1.9052758   1.185033\n",
      "     1.0191929 ]\n",
      "   [ 0.7143441  -2.7222555   1.654944   -0.4344939  -3.5097253\n",
      "     0.19337063]\n",
      "   [-0.24068627  0.5629927   3.9413342   2.948522    6.2797832\n",
      "     1.3845129 ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.4390437   -0.33947098   4.013443    -0.29624832]\n",
      "   [ -0.2583174    0.33025625  -0.39602375  -0.30866668]\n",
      "   [  0.32099545  -0.95505685  -4.5404463    0.24053822]\n",
      "   [  0.63142234  -1.2339483   -6.734192     0.53280115]\n",
      "   [  0.54130554   0.21318378  -0.52515334   0.56435126]\n",
      "   [  0.36491862  -0.681736    -3.822705     0.32760125]]\n",
      "\n",
      "  [[ -0.43239835   0.42818624   2.6262326   -0.4062209 ]\n",
      "   [ -0.22419338   1.074776     1.1630725   -0.19326892]\n",
      "   [  0.51628953   0.88442105  -0.02622377   0.5438406 ]\n",
      "   [  0.7893162   -3.560948    -7.5051656    0.6630447 ]\n",
      "   [  0.57238746  -1.7088602   -4.2579656    0.5265007 ]\n",
      "   [  0.4162348   -1.7032979   -3.9784098    0.36342958]]\n",
      "\n",
      "  [[ -0.44927993   0.54527193   4.6114116   -0.29629943]\n",
      "   [  0.67831874  -0.902597    -5.0636787    0.530373  ]\n",
      "   [  1.1608257   -0.9582575   -6.0131745    1.0148909 ]\n",
      "   [  1.8622837   -0.54619     -6.1305943    1.7046378 ]\n",
      "   [  0.5365511   -0.7164705   -4.3196692    0.4233776 ]\n",
      "   [  0.9778634    0.05141426  -1.4833766    0.9106759 ]]\n",
      "\n",
      "  [[ -1.5574945   -0.13661464   4.8192244   -1.5274014 ]\n",
      "   [  1.8428049    0.10988725  -3.7634213    1.8194834 ]\n",
      "   [  1.8790634    0.16765016  -5.2636895    1.8923659 ]\n",
      "   [  1.8488076    0.38219947  -2.467583     1.8204844 ]\n",
      "   [  1.9478984    0.24887773  -4.0790386    1.8869959 ]\n",
      "   [  2.0514908    0.8322106   -3.4669142    2.0144742 ]]\n",
      "\n",
      "  [[ -1.0361291    1.0311096    2.9110353   -1.0666645 ]\n",
      "   [  0.17529838  -0.76200086  -1.3703144    0.18987674]\n",
      "   [  0.9855322   -1.2880604   -2.9897346    0.9731935 ]\n",
      "   [  0.7058304   -0.60330254  -3.2749028    0.66278076]\n",
      "   [  0.73217136   0.31563684  -1.2379584    0.7082298 ]\n",
      "   [  1.0366942   -0.6526082   -2.5682065    1.0441903 ]]\n",
      "\n",
      "  [[ -0.7908757   -1.3115877    0.67647886  -0.8404614 ]\n",
      "   [ -0.19861834  -1.3611583   -2.9659412   -0.28904545]\n",
      "   [ -0.5602697   -0.27112013   0.11186516  -0.6005517 ]\n",
      "   [  0.77641386  -0.26496348  -3.3041744    0.8275802 ]\n",
      "   [  1.4017886    1.7031087   -1.5688373    1.5331511 ]\n",
      "   [  1.1998662    0.4657598   -1.240734     1.2614617 ]]\n",
      "\n",
      "  [[ -0.21379697  -0.3550262    2.452176    -0.14885877]\n",
      "   [  1.1190844   -0.06087807  -3.8031468    1.0202737 ]\n",
      "   [  0.3503374   -1.1730963   -4.796474     0.2665696 ]\n",
      "   [  0.7476409    0.9311516   -1.8346077    0.6924049 ]\n",
      "   [  0.8267951    0.640895    -1.8839159    0.7792115 ]\n",
      "   [  0.15174165  -0.8480215   -3.930585     0.0703403 ]]\n",
      "\n",
      "  [[ -1.6940544   -1.1217345    4.9840846   -1.5811146 ]\n",
      "   [  2.1202106   -1.363047   -10.718351     1.9247364 ]\n",
      "   [  0.94408834   1.107803    -3.365614     0.8552538 ]\n",
      "   [  2.3723536   -2.03031    -11.663365     2.1812994 ]\n",
      "   [  1.1664867   -1.2401501   -6.0198855    1.0814999 ]\n",
      "   [  2.8200953   -0.9444836   -9.885286     2.6860921 ]]]], shape=(1, 8, 6, 4), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights.shape = (1, 8, 6, 4)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724   -0.7859426    1.8895346   -4.1808095 ]\n",
      "   [  3.1438417   -2.3516042   -2.567732     3.3101664 ]\n",
      "   [ -6.821511     5.132966    -1.515522    -8.113494  ]\n",
      "   [ -2.6520565    0.29321504   2.1502676   -3.7211149 ]]\n",
      "\n",
      "  [[ -5.830585    -3.796887     3.4242606   -5.8514013 ]\n",
      "   [ -4.592556     0.16063029   2.767142    -4.5720806 ]\n",
      "   [ -3.28553     -1.1929119    0.7792256   -3.4044573 ]\n",
      "   [ -3.889495    -2.520736     2.122136    -3.5668743 ]]\n",
      "\n",
      "  [[  0.21261294  -2.0974307    1.9989094   -1.4663465 ]\n",
      "   [ -8.633069    -8.325801    -1.5773704  -10.09647   ]\n",
      "   [ -8.450605    -4.192688     1.4179147   -7.872258  ]\n",
      "   [ -3.4774246   -1.0960379    2.3552895   -3.6499627 ]]\n",
      "\n",
      "  [[ -0.62310183   0.02354282   0.13477258  -1.0099144 ]\n",
      "   [ -2.3111663   -0.6952288    2.469388    -1.3457242 ]\n",
      "   [ -3.9942987   -1.6525842   -1.1253656   -4.210669  ]\n",
      "   [ -1.8414911    0.44378278   1.8752314   -2.3266878 ]]\n",
      "\n",
      "  [[ -3.365542    -1.0635742   -0.04164821  -1.7994132 ]\n",
      "   [ -6.9826465   -2.3183198    2.294877    -4.308402  ]\n",
      "   [ -5.801219    -3.403646    -4.5645704   -6.675435  ]\n",
      "   [  0.2971933    0.63473487  -0.6389538    2.6413    ]]\n",
      "\n",
      "  [[ -2.1686854   -1.023346     0.65018183  -2.5024278 ]\n",
      "   [ -4.622171    -0.8858723    5.574534    -4.401349  ]\n",
      "   [ -3.8713155    1.6690211    3.75436     -4.813896  ]\n",
      "   [ -1.399356    -0.4949215    2.2085276   -2.2406054 ]]\n",
      "\n",
      "  [[ -0.3049526    2.224203    -1.2487973   -3.0664551 ]\n",
      "   [ -4.3723803    1.9350511    0.1110547   -4.0373297 ]\n",
      "   [ -8.824966    -2.4120855    4.2221565   -6.5589924 ]\n",
      "   [ -2.1759007    2.9709213    0.01304059  -3.8550992 ]]\n",
      "\n",
      "  [[ -2.8292158    0.7908498   -2.1495862   -2.1313064 ]\n",
      "   [ -4.523009    -2.7817159   -3.0650115   -5.429205  ]\n",
      "   [ -5.458025     1.8018078   -4.9602304   -6.058091  ]\n",
      "   [ -4.791121     1.7835791   -2.0525951   -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.3626041   -3.6032155    4.161095     0.0656366 ]\n",
      "   [ -6.182477    -2.5122466    7.009508    -3.584936  ]\n",
      "   [ -3.4696164    1.1780642    5.1291347   -2.1059198 ]\n",
      "   [ -3.483398    -4.1618204    4.4601355   -0.07843098]]\n",
      "\n",
      "  [[  0.510499     1.036337    -1.0375209    0.28767318]\n",
      "   [ -3.3273916   -0.11347406   3.4049916   -3.419848  ]\n",
      "   [ -4.9530396    0.3851128    4.3673553   -5.12493   ]\n",
      "   [ -2.7946234    0.38115668  -0.20627333  -2.9363682 ]]\n",
      "\n",
      "  [[ -9.042869    -3.423595    -4.7580905   -7.272155  ]\n",
      "   [ -5.3064766   -1.7224908   -1.1489992   -3.3164253 ]\n",
      "   [ -5.864008     1.9313622    5.1383986   -4.273402  ]\n",
      "   [-10.078705    -3.3595207   -3.9119687   -7.9476895 ]]\n",
      "\n",
      "  [[  0.704116     0.5427282   -1.371693     1.7133871 ]\n",
      "   [ -7.3349566   -2.3262897    2.0722024   -6.863343  ]\n",
      "   [ -7.6698356   -5.039354     0.01469475  -8.557696  ]\n",
      "   [  0.14490138   1.3311632   -0.6794642    1.1242119 ]]\n",
      "\n",
      "  [[  5.0507197    3.1588564    1.3940698    5.1164274 ]\n",
      "   [-13.740164    -2.4651692    8.651231   -13.1501875 ]\n",
      "   [-13.236485    -2.9835963    8.292635   -12.939023  ]\n",
      "   [  1.3312078    2.3829906    2.8037574    1.0776967 ]]\n",
      "\n",
      "  [[ 12.874086     2.85469     -6.3369064   11.067723  ]\n",
      "   [ -1.5080522   -3.8416598    0.31655705  -2.3005078 ]\n",
      "   [ -5.6672664   -1.453503     2.7044997   -4.9066    ]\n",
      "   [ 10.062546     2.1319842   -5.8750806    8.317264  ]]\n",
      "\n",
      "  [[ -8.929089    -8.014712    -1.0568765   -8.620834  ]\n",
      "   [-10.908268   -10.764266    -0.8827691  -10.867963  ]\n",
      "   [-10.751021    -8.130157     2.3616416   -9.656115  ]\n",
      "   [ -9.412735    -8.36169     -1.0328311   -8.912132  ]]\n",
      "\n",
      "  [[ 10.875551     2.7801728   -1.4670037   12.899784  ]\n",
      "   [  2.4307857    2.7539134    2.4796565    3.4081655 ]\n",
      "   [ -7.5609837   -0.33894548   3.7714336   -8.186357  ]\n",
      "   [  8.785441     2.427588    -1.0477616   10.493459  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 7, 256)\n",
      "(1, 7, 256)\n",
      "(1, 7, 256)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "(1, 8, 7, 32)\n",
      "(1, 8, 7, 32)\n",
      "(1, 8, 7, 32)\n",
      "matmul_qk.shape = (1, 8, 7, 7)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    0.86808515   0.68926543  -0.07710785   0.5841838\n",
      "      0.57039815   0.14776815]\n",
      "   [ -0.8052174    1.2325178    1.3822585   -0.6516272    0.6315028\n",
      "     -0.14370728   3.4813554 ]\n",
      "   [ -0.29754275   1.5583472   -0.8161052    1.4953052    0.10321569\n",
      "     -3.3889081   -0.38736   ]\n",
      "   [ -0.5443256    2.2166584    6.8072824   -9.876017     2.3808532\n",
      "      8.322778     7.1853633 ]\n",
      "   [  0.7472351   -4.5735664    2.1001763   -0.68935      0.6213042\n",
      "      6.4843163    0.10306584]\n",
      "   [ -0.29372013   1.6946385   -2.4606276   -1.9047768    3.2469358\n",
      "      2.6467237    0.40995654]\n",
      "   [ -0.9051418    3.3132124   -0.7918448    7.3934846   -0.4331388\n",
      "     -9.205105    -4.1221614 ]]\n",
      "\n",
      "  [[ -0.1123142    0.22450152   0.1997487   -0.5255737    0.31211773\n",
      "     -0.39361402  -0.09552613]\n",
      "   [  0.31872883  -4.25332     -2.458986     4.503875    -2.5093522\n",
      "      0.16508146  -1.1448668 ]\n",
      "   [  0.06266014  -2.8750339   -5.2976384    1.4493271    2.1512098\n",
      "     -1.3659165   -1.542527  ]\n",
      "   [ -0.54926753   4.0720015    2.403759    -3.7070231    0.68023175\n",
      "      0.05022379  -2.1575356 ]\n",
      "   [  0.8732976   -1.8000302    3.722933     1.1266485   -2.602924\n",
      "      3.7650833    1.260653  ]\n",
      "   [  0.33890194   2.1156766    0.45062694   1.4144641   -3.823428\n",
      "     -5.970876    -1.2156229 ]\n",
      "   [ -0.16106324   2.994075     0.45485264  -0.35378376  -4.149223\n",
      "     -3.9140332   -4.860044  ]]\n",
      "\n",
      "  [[ -0.10918517   0.20129828  -0.1020415    0.657444     0.22135806\n",
      "      0.15187913  -0.11050548]\n",
      "   [  0.3942903   -3.7951708   -3.841178    -0.39951256   0.29694498\n",
      "     -0.4107963    0.40402666]\n",
      "   [ -0.45792332  -2.8219523   -3.4908211   -0.8908195    1.6258212\n",
      "     -2.935929    -4.5810666 ]\n",
      "   [  0.4032216    1.4782436    8.961608    -4.7050643    4.6159906\n",
      "      4.017976     7.0261846 ]\n",
      "   [ -0.39189684   4.1333766    9.57504      1.0154871    0.6772467\n",
      "      0.85469484   5.3734183 ]\n",
      "   [ -0.6209384   -0.1761216    3.239656     0.448386     2.965093\n",
      "      0.04321224  -0.55968136]\n",
      "   [ -0.13727854  -2.334021    -0.12732092   2.6658416   -0.16601619\n",
      "      5.0760036   -8.124224  ]]\n",
      "\n",
      "  [[ -0.22114104   0.78687924   0.38468125  -0.35858485   0.4814524\n",
      "      0.7716349    0.18577488]\n",
      "   [ -0.32672447   1.066411     0.20903866  -0.35502467   1.9553363\n",
      "      2.3845599    4.0547504 ]\n",
      "   [  0.17269178  -1.3044931   -4.883713    -0.30554432   0.34996486\n",
      "     -3.483534    -0.35805675]\n",
      "   [  0.08333597   1.9566892    3.4552226   -3.034405     1.9709632\n",
      "     -0.00770508   1.6790633 ]\n",
      "   [  0.19127834  -0.42828786   4.3851147    0.4973215    2.414637\n",
      "      2.6963992    4.829734  ]\n",
      "   [ -0.02093794   3.6430805    8.713464    -0.52347636   9.823708\n",
      "     -0.37148795   4.448587  ]\n",
      "   [ -1.3862007    9.094722     6.5496054   -1.6104932    9.99708\n",
      "      6.1065807    0.33539578]]\n",
      "\n",
      "  [[  0.00076126  -0.47878256   0.09911879   0.00893964   0.24509\n",
      "      0.14304979  -0.08038879]\n",
      "   [  0.7206705   -4.158416    -1.74024      4.0903745   -0.7901846\n",
      "     -2.9663303    0.31932393]\n",
      "   [  0.12035121   5.0561695   -2.9415672   -0.26743913  -4.871599\n",
      "      1.4902955   -1.8848196 ]\n",
      "   [  0.03012125   5.2640715    3.1126494   -4.628424    -2.5972195\n",
      "      1.9151638   -0.22672556]\n",
      "   [  0.44786143   4.194443     3.1239393   -2.8355985   -5.2169623\n",
      "      3.1980612    2.4337146 ]\n",
      "   [  0.61835045  -3.2027795    1.4719862    3.313971     2.5021975\n",
      "     -1.5920473    1.3572292 ]\n",
      "   [  0.2457593   -7.236913     0.20529102   8.781249     3.4424264\n",
      "     -5.4454756   -6.8731003 ]]\n",
      "\n",
      "  [[ -0.00999826  -0.17336224  -0.04885251  -0.06919655  -0.32633722\n",
      "      0.0366222    0.16041176]\n",
      "   [  1.4367385   -9.06984     -3.8178802   -0.75747144  -3.7752612\n",
      "     -7.119363    -0.808656  ]\n",
      "   [  0.92183876  -4.7562103   -6.4032483   -2.5681021   -2.3384533\n",
      "     -3.918842     3.1888914 ]\n",
      "   [ -0.15983817   5.33236      4.7559114    0.13260181   1.7207963\n",
      "      4.4075403    5.9188757 ]\n",
      "   [  0.10523379   1.6092048    6.520943    -0.82788485  -2.471699\n",
      "      2.2942595    0.73225766]\n",
      "   [  0.71476805  -5.8466797   -1.9964423   -1.6363546   -0.9318489\n",
      "     -4.4617267   -0.7425307 ]\n",
      "   [  1.4523759   -9.992252    -9.585231    -3.4427912   -8.658167\n",
      "    -10.796175   -10.854523  ]]\n",
      "\n",
      "  [[ -0.20863976   0.569198     0.44341293   0.16857311   0.1010042\n",
      "      0.3144073   -0.57303137]\n",
      "   [ -0.44067878  -1.3154124    2.2516541    5.957117     3.327574\n",
      "      2.1122663    4.2094145 ]\n",
      "   [ -0.73116076   0.78557914   1.7380664    4.084935     3.872323\n",
      "      3.7347608    1.9456888 ]\n",
      "   [  2.1423054   -2.7513258   -6.220788    -6.2902803   -5.0086665\n",
      "     -6.650022     0.06290687]\n",
      "   [  2.4718273   -3.7755775   -3.795776    -5.034314   -10.499384\n",
      "     -6.276488    -3.7034833 ]\n",
      "   [  1.5774866   -2.8405964   -6.4159265   -3.2861729   -4.428243\n",
      "     -9.614207    -3.1512659 ]\n",
      "   [ -2.476524     1.4815274    2.88176      8.238331     9.522714\n",
      "     11.843423    -2.1702423 ]]\n",
      "\n",
      "  [[ -0.15726982   0.34657618   0.09771852  -0.06033853   0.13718088\n",
      "      0.4450618   -0.11978562]\n",
      "   [ -0.20781471  -3.9169958   -2.6317358    2.4233148    1.5008358\n",
      "      2.266367    -2.2837386 ]\n",
      "   [  0.49460515  -2.8374786   -6.512395    -2.2088082   -3.047029\n",
      "     -2.7998128   -4.838161  ]\n",
      "   [  0.40351048   5.496288     7.4884925   -5.00473     -0.34395367\n",
      "      0.5531984    3.1644454 ]\n",
      "   [  0.20931137   1.581105     5.1500797   -1.7881161   -0.8540002\n",
      "     -1.366854     0.834874  ]\n",
      "   [  0.27719426   3.7286282    1.4238677   -2.3924153    6.4014854\n",
      "     -0.2659451    2.9778736 ]\n",
      "   [ -0.38605455   4.430161     5.1124682   -4.818994     2.8128343\n",
      "      3.5536995   -1.2899668 ]]]], shape=(1, 8, 7, 7), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 7, 7)\n",
      "output.shape = (1, 8, 7, 32)\n",
      "scaled_attention.shape= (1, 7, 8, 32)\n",
      "concat_attention.shape= (1, 7, 256)\n",
      "outputs.shape= (1, 7, 256)\n",
      "(1, 7, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 7, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 7, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.43004218 -2.1264591  -4.20363     0.41727006]\n",
      "   [ 0.36214504 -0.9224591  -3.69376     0.32345214]\n",
      "   [-0.11888524  0.7827182   1.6091667  -0.07043942]\n",
      "   [ 1.5769911  -1.5107999  -4.705859    1.5822129 ]\n",
      "   [ 1.433958    0.35687694 -0.44859353  1.4714174 ]\n",
      "   [ 0.6759765  -2.6841862  -5.7552285   0.64645445]\n",
      "   [ 1.4815818  -2.0108624  -5.7773795   1.438906  ]]\n",
      "\n",
      "  [[ 1.3539228  -1.1119139  -3.6765318   1.3366559 ]\n",
      "   [ 0.49521226  0.8556765   1.0242827   0.54091555]\n",
      "   [ 1.8845222   0.80958265 -1.4450098   1.8755151 ]\n",
      "   [ 3.1395586  -0.3902885  -4.6706777   3.0263221 ]\n",
      "   [ 2.4448514  -0.79036796 -4.79417     2.363135  ]\n",
      "   [ 1.9017469  -1.5351089  -6.007563    1.8183545 ]\n",
      "   [ 1.7505506  -0.7003997  -4.214069    1.6834856 ]]\n",
      "\n",
      "  [[-0.6358987   0.6857092   3.934223   -0.6082751 ]\n",
      "   [ 2.1257792  -0.75171655 -5.1641245   2.0634513 ]\n",
      "   [ 1.9908828  -2.4718666  -8.323398    1.8416339 ]\n",
      "   [ 1.4525386  -1.9171642  -6.0657043   1.3277422 ]\n",
      "   [ 2.0609357  -2.3901558  -8.3400135   1.8740809 ]\n",
      "   [ 2.1479387  -0.04788223 -3.5392594   2.0927868 ]\n",
      "   [ 0.8613854  -1.3274925  -4.003621    0.7955877 ]]\n",
      "\n",
      "  [[-0.5390432   0.7702814   1.6837941  -0.4367591 ]\n",
      "   [ 1.0922868  -2.2901568  -6.70844     0.99209934]\n",
      "   [ 0.63555646 -0.9029067  -2.4676714   0.5942919 ]\n",
      "   [ 1.3171933  -1.6731497  -4.4172173   1.2324891 ]\n",
      "   [ 1.0370547  -2.4209025  -5.395325    0.9418529 ]\n",
      "   [ 0.70518225 -1.6686244  -4.040526    0.64739156]\n",
      "   [ 1.5462278  -2.7973738  -7.242962    1.4810474 ]]\n",
      "\n",
      "  [[-1.7341876   0.2025832   1.12208    -1.7460632 ]\n",
      "   [-0.38023633 -0.88305074 -1.6188183  -0.4605108 ]\n",
      "   [ 0.7600631  -0.39524692 -3.455463    0.6971765 ]\n",
      "   [ 3.004541   -3.5289714  -9.588761    2.9130647 ]\n",
      "   [ 2.5986595  -3.3891108  -8.484004    2.4783466 ]\n",
      "   [ 2.6414726  -3.1978264  -6.90967     2.5648355 ]\n",
      "   [ 2.475332   -2.112042   -3.9433625   2.4765618 ]]\n",
      "\n",
      "  [[ 1.9698906   1.0078789  -1.2122052   2.0134254 ]\n",
      "   [ 1.8958745  -0.45892912 -5.3896065   1.8568405 ]\n",
      "   [ 1.4736686   0.538687   -1.965722    1.469285  ]\n",
      "   [ 3.1977854   0.05167901 -5.276443    3.1679316 ]\n",
      "   [ 2.3644834   0.91031945 -2.6867223   2.3744936 ]\n",
      "   [ 1.4999217  -3.062734   -6.6033297   1.5126845 ]\n",
      "   [ 1.4788055  -2.0994508  -6.462512    1.3902283 ]]\n",
      "\n",
      "  [[ 0.1258963  -0.71917385 -2.2927022   0.03860371]\n",
      "   [ 1.3117977  -1.6638893  -4.465428    1.2304243 ]\n",
      "   [ 2.5624385  -3.8971152  -9.53067     2.4499042 ]\n",
      "   [ 2.0999384  -1.6019284  -5.034336    2.0732462 ]\n",
      "   [ 0.7817824  -1.5657676  -3.6030664   0.7704573 ]\n",
      "   [ 1.6116695  -3.2206306  -7.0788507   1.4964689 ]\n",
      "   [ 1.9198471  -0.6208935  -2.765539    1.9039177 ]]\n",
      "\n",
      "  [[-0.31415597  2.5866678   6.2770805  -0.21654116]\n",
      "   [ 1.1086718  -2.3592036  -5.937715    1.05256   ]\n",
      "   [ 1.0852163  -4.0399203  -9.892566    0.9899011 ]\n",
      "   [ 1.6877527  -2.2720673  -3.6519856   1.6843686 ]\n",
      "   [ 1.2909392  -2.185475   -4.1043363   1.2519479 ]\n",
      "   [ 2.045505   -2.9272645  -7.5192847   2.0481453 ]\n",
      "   [ 1.0800407  -1.0022451  -3.029588    1.0860733 ]]]], shape=(1, 8, 7, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 7, 4)\n",
      "output.shape = (1, 8, 7, 32)\n",
      "scaled_attention.shape= (1, 7, 8, 32)\n",
      "concat_attention.shape= (1, 7, 256)\n",
      "outputs.shape= (1, 7, 256)\n",
      "(1, 7, 256)\n",
      "(1, 7, 256)\n",
      "(1, 7, 256)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "(1, 8, 7, 32)\n",
      "(1, 8, 7, 32)\n",
      "(1, 8, 7, 32)\n",
      "matmul_qk.shape = (1, 8, 7, 7)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.77654105  -1.1600666   -0.7377553    0.15321049  -0.33445883\n",
      "      0.9638917    1.0282744 ]\n",
      "   [  4.2638865   -4.9808917   -4.7282324   -7.588514    -5.1648345\n",
      "      0.752576     1.5854373 ]\n",
      "   [  3.7290585   -3.6264493   -4.6852508   -2.8495634   -3.194244\n",
      "      2.0674891    0.16462609]\n",
      "   [  3.8470237    1.1639067   -2.604357    -4.39899     -3.9771621\n",
      "     -0.7163415   -2.6310756 ]\n",
      "   [  5.5331926   -2.1469176   -7.7634535   -5.166938    -6.2337794\n",
      "      1.4473847   -0.10470335]\n",
      "   [  4.7488627    0.8060199   -2.7915359    0.2651618    2.8968983\n",
      "     -4.7307568   -3.8274188 ]\n",
      "   [ -3.646525    -2.417211     0.9628969    3.695615     4.510241\n",
      "      0.65492576  14.975302  ]]\n",
      "\n",
      "  [[  0.02615874   1.1034771   -0.24971536  -0.62090695   0.00287016\n",
      "      0.03166713   0.67804277]\n",
      "   [ -1.1199212   -1.2406197   -0.3052636    2.3498487    1.2994226\n",
      "     -0.1909589    1.7674519 ]\n",
      "   [ -1.1390502    0.8624328    2.084101     1.3884075   -0.980912\n",
      "      1.0793152    0.07685544]\n",
      "   [  1.2047955   -2.8474126   -2.2027948    1.087519     1.6147972\n",
      "      0.44935256   0.00483328]\n",
      "   [  3.020494    -0.39609548  -2.2673187   -2.7705114   -3.26368\n",
      "     -0.7973336   -1.0194429 ]\n",
      "   [ -1.8445143   -3.3527074    1.6643316    5.427364     4.1477027\n",
      "      1.6791626    1.010268  ]\n",
      "   [ -0.23157938   3.4557557    2.9004824   -0.5162866   -0.8175649\n",
      "      1.2562578   -1.8160521 ]]\n",
      "\n",
      "  [[ -2.9237115    3.363596     2.3430805    0.21954887  -0.1109451\n",
      "      0.492053    -0.16566317]\n",
      "   [ -0.80274636   4.677598     1.8153228    2.9806826    2.4021566\n",
      "     -1.9523367    0.08145932]\n",
      "   [ -2.9179065    2.4044318    0.6520111    6.7438183    2.0902429\n",
      "     -2.2136843    2.1637242 ]\n",
      "   [ -3.690457     0.7928321    8.471516    -1.8322024    4.8284726\n",
      "      2.881544     4.1723924 ]\n",
      "   [  1.9101584    1.9778312    1.4799114    3.4420455    4.4670367\n",
      "     -1.8857652    4.515708  ]\n",
      "   [ -1.2988942   -0.97658354  -0.9146665    2.3100488   -1.0963827\n",
      "      0.26074374   0.40357405]\n",
      "   [ -1.8591923    3.461019    -2.158382     5.977408     0.09147047\n",
      "     -7.350376     0.48368448]]\n",
      "\n",
      "  [[ -2.8092232    1.49841      1.9324561    0.96108407   1.7129885\n",
      "      1.6189119    0.54601675]\n",
      "   [ -2.5165534    2.19107      0.6257863    7.272254     2.259812\n",
      "      3.410876     0.34877956]\n",
      "   [ -2.6379817    2.7387853   -1.0823029    6.826854     1.8786801\n",
      "      2.3327372   -2.5237718 ]\n",
      "   [ -2.4187973   -3.4014497    3.5254593    0.5569787    5.067711\n",
      "      0.5633194    7.8339043 ]\n",
      "   [ -1.945622    -0.65303457   2.4640982    3.3566499    4.1663265\n",
      "     -1.1195359    0.9511546 ]\n",
      "   [ -4.171936    -0.01147364   3.4513283    3.152668     4.512732\n",
      "      9.864028     4.1050034 ]\n",
      "   [  1.0633516    8.69028     -0.31681222   2.8985076    0.9200999\n",
      "      0.9111165  -13.286961  ]]\n",
      "\n",
      "  [[ -0.4573368   -0.41149053   0.57131124   0.99204594   0.7350709\n",
      "      0.20620687  -0.9990892 ]\n",
      "   [  3.5183      -1.5149683   -4.673192    -3.8204358   -2.732508\n",
      "      0.3246554   -3.8637986 ]\n",
      "   [ -0.12923025   1.2998483   -1.2514737   -1.2474041   -0.5723316\n",
      "     -2.1154923   -0.44402006]\n",
      "   [  1.3779988    4.212641    -3.3267968   -0.59099406   3.2183704\n",
      "     -5.157247    -2.7787948 ]\n",
      "   [  1.6397702    0.8696879   -2.9280584   -0.5760767   -0.3494962\n",
      "     -2.4273357   -1.5823427 ]\n",
      "   [  0.33756587   2.3127203   -0.16594982   2.7338834    6.459333\n",
      "     -3.8244364    0.3468593 ]\n",
      "   [ -0.4835711   -2.4353967   -2.2305977   -2.4151573   -5.3643565\n",
      "      0.38799766   0.41155824]]\n",
      "\n",
      "  [[ -1.4886304   -0.8160711    1.272815     1.069158    -1.2259004\n",
      "      0.27663958   0.5322572 ]\n",
      "   [  0.90435994  -2.5749598   -1.9510299   -1.5345374   -0.7728811\n",
      "     -1.8463182    0.02996135]\n",
      "   [ -0.34667596  -1.3035538   -2.9048364   -0.24566376  -0.21664956\n",
      "     -2.8083339    1.8281504 ]\n",
      "   [  1.2138464    1.9463997    1.1304235    4.3117375   -0.93706936\n",
      "     -2.5150895   -3.121864  ]\n",
      "   [  2.2285655    0.59783006   2.3479755    1.71059     -4.110444\n",
      "     -4.411427    -1.5514175 ]\n",
      "   [  0.04869156   4.3527355    3.3943183    3.6977544    2.4533684\n",
      "     -0.86546636  -0.87341183]\n",
      "   [ -0.17167635  -4.339262    -4.7814736   -7.6732864   -1.959909\n",
      "     -0.5605185    5.837771  ]]\n",
      "\n",
      "  [[ -2.3999348    1.3400928    0.6617681   -1.8778961   -0.45578986\n",
      "      2.2384698    1.2069004 ]\n",
      "   [ -1.6690627    0.10749228   0.34268987   1.8814775    2.1918917\n",
      "      0.12542067   2.5933814 ]\n",
      "   [ -1.4865489   -3.3957736   -3.0061002    3.6486413    3.2406335\n",
      "      1.885369    -3.1033058 ]\n",
      "   [  2.238486     0.6319354   -1.8617017   -5.3909755   -1.2285255\n",
      "     -2.478067    -0.79074955]\n",
      "   [  2.793881    -1.9779224   -3.7091935   -1.6687323   -2.2475994\n",
      "     -2.747856    -4.7172976 ]\n",
      "   [  2.2604463    0.89827144  -2.7981474    0.4819163    1.2817676\n",
      "     -2.3887415   -3.3407822 ]\n",
      "   [ -0.22698143  -0.77705026  -0.24666077   1.3159165    0.6648407\n",
      "     -1.9169033   -3.035924  ]]\n",
      "\n",
      "  [[ -1.9244474    2.1994686    1.347579     0.2057227   -0.04820146\n",
      "      2.141403    -1.3585992 ]\n",
      "   [ -0.34355617  -2.1330175    0.8624636   -2.9745345   -1.522738\n",
      "      0.9593218    1.0111009 ]\n",
      "   [  0.41986907  -3.7553241   -1.9478018   -1.2957213   -3.4820578\n",
      "     -2.2611165   -1.2612828 ]\n",
      "   [  0.42432842   0.5639865    4.050384    -1.9052758    1.185033\n",
      "      1.0191929    3.7474763 ]\n",
      "   [  0.7143441   -2.7222555    1.654944    -0.4344939   -3.5097253\n",
      "      0.19337063   0.334209  ]\n",
      "   [ -0.24068627   0.5629927    3.9413342    2.948522     6.2797832\n",
      "      1.3845129    7.0708156 ]\n",
      "   [  0.23093589   2.4334755   -1.4492497   -0.90904254   2.798429\n",
      "      0.2731452   -0.4825536 ]]]], shape=(1, 8, 7, 7), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 7, 7)\n",
      "output.shape = (1, 8, 7, 32)\n",
      "scaled_attention.shape= (1, 7, 8, 32)\n",
      "concat_attention.shape= (1, 7, 256)\n",
      "outputs.shape= (1, 7, 256)\n",
      "(1, 7, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 7, 256)\n",
      "(1, 7, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 7, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 7, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.4390437   -0.33947098   4.013443    -0.29624832]\n",
      "   [ -0.2583174    0.33025625  -0.39602375  -0.30866668]\n",
      "   [  0.32099545  -0.95505685  -4.5404463    0.24053822]\n",
      "   [  0.63142234  -1.2339483   -6.734192     0.53280115]\n",
      "   [  0.54130554   0.21318378  -0.52515334   0.56435126]\n",
      "   [  0.36491862  -0.681736    -3.822705     0.32760125]\n",
      "   [  0.41342953  -0.61083317  -4.5110703    0.26699755]]\n",
      "\n",
      "  [[ -0.43239835   0.42818624   2.6262326   -0.4062209 ]\n",
      "   [ -0.22419338   1.074776     1.1630725   -0.19326892]\n",
      "   [  0.51628953   0.88442105  -0.02622377   0.5438406 ]\n",
      "   [  0.7893162   -3.560948    -7.5051656    0.6630447 ]\n",
      "   [  0.57238746  -1.7088602   -4.2579656    0.5265007 ]\n",
      "   [  0.4162348   -1.7032979   -3.9784098    0.36342958]\n",
      "   [  0.7785082    0.6506525   -1.7838467    0.78410023]]\n",
      "\n",
      "  [[ -0.44927993   0.54527193   4.6114116   -0.29629943]\n",
      "   [  0.67831874  -0.902597    -5.0636787    0.530373  ]\n",
      "   [  1.1608257   -0.9582575   -6.0131745    1.0148909 ]\n",
      "   [  1.8622837   -0.54619     -6.1305943    1.7046378 ]\n",
      "   [  0.5365511   -0.7164705   -4.3196692    0.4233776 ]\n",
      "   [  0.9778634    0.05141426  -1.4833766    0.9106759 ]\n",
      "   [  0.8324067   -1.1943607   -5.281966     0.704296  ]]\n",
      "\n",
      "  [[ -1.5574945   -0.13661464   4.8192244   -1.5274014 ]\n",
      "   [  1.8428049    0.10988725  -3.7634213    1.8194834 ]\n",
      "   [  1.8790634    0.16765016  -5.2636895    1.8923659 ]\n",
      "   [  1.8488076    0.38219947  -2.467583     1.8204844 ]\n",
      "   [  1.9478984    0.24887773  -4.0790386    1.8869959 ]\n",
      "   [  2.0514908    0.8322106   -3.4669142    2.0144742 ]\n",
      "   [  3.016969    -1.706834    -9.285852     2.9572225 ]]\n",
      "\n",
      "  [[ -1.0361291    1.0311096    2.9110353   -1.0666645 ]\n",
      "   [  0.17529838  -0.76200086  -1.3703144    0.18987674]\n",
      "   [  0.9855322   -1.2880604   -2.9897346    0.9731935 ]\n",
      "   [  0.7058304   -0.60330254  -3.2749028    0.66278076]\n",
      "   [  0.73217136   0.31563684  -1.2379584    0.7082298 ]\n",
      "   [  1.0366942   -0.6526082   -2.5682065    1.0441903 ]\n",
      "   [  1.6105137   -1.1913971   -3.527507     1.6231617 ]]\n",
      "\n",
      "  [[ -0.7908757   -1.3115877    0.67647886  -0.8404614 ]\n",
      "   [ -0.19861834  -1.3611583   -2.9659412   -0.28904545]\n",
      "   [ -0.5602697   -0.27112013   0.11186516  -0.6005517 ]\n",
      "   [  0.77641386  -0.26496348  -3.3041744    0.8275802 ]\n",
      "   [  1.4017886    1.7031087   -1.5688373    1.5331511 ]\n",
      "   [  1.1998662    0.4657598   -1.240734     1.2614617 ]\n",
      "   [ -0.01129011   1.611884     3.5301998    0.02480664]]\n",
      "\n",
      "  [[ -0.21379697  -0.3550262    2.452176    -0.14885877]\n",
      "   [  1.1190844   -0.06087807  -3.8031468    1.0202737 ]\n",
      "   [  0.3503374   -1.1730963   -4.796474     0.2665696 ]\n",
      "   [  0.7476409    0.9311516   -1.8346077    0.6924049 ]\n",
      "   [  0.8267951    0.640895    -1.8839159    0.7792115 ]\n",
      "   [  0.15174165  -0.8480215   -3.930585     0.0703403 ]\n",
      "   [  0.48000973  -2.0706      -5.5565906    0.40147567]]\n",
      "\n",
      "  [[ -1.6940544   -1.1217345    4.9840846   -1.5811146 ]\n",
      "   [  2.1202106   -1.363047   -10.718351     1.9247364 ]\n",
      "   [  0.94408834   1.107803    -3.365614     0.8552538 ]\n",
      "   [  2.3723536   -2.03031    -11.663365     2.1812994 ]\n",
      "   [  1.1664867   -1.2401501   -6.0198855    1.0814999 ]\n",
      "   [  2.8200953   -0.9444836   -9.885286     2.6860921 ]\n",
      "   [  1.6303653    2.4244456   -1.4615241    1.6263697 ]]]], shape=(1, 8, 7, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 7, 4)\n",
      "output.shape = (1, 8, 7, 32)\n",
      "scaled_attention.shape= (1, 7, 8, 32)\n",
      "concat_attention.shape= (1, 7, 256)\n",
      "outputs.shape= (1, 7, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724   -0.7859426    1.8895346   -4.1808095 ]\n",
      "   [  3.1438417   -2.3516042   -2.567732     3.3101664 ]\n",
      "   [ -6.821511     5.132966    -1.515522    -8.113494  ]\n",
      "   [ -2.6520565    0.29321504   2.1502676   -3.7211149 ]]\n",
      "\n",
      "  [[ -5.830585    -3.796887     3.4242606   -5.8514013 ]\n",
      "   [ -4.592556     0.16063029   2.767142    -4.5720806 ]\n",
      "   [ -3.28553     -1.1929119    0.7792256   -3.4044573 ]\n",
      "   [ -3.889495    -2.520736     2.122136    -3.5668743 ]]\n",
      "\n",
      "  [[  0.21261294  -2.0974307    1.9989094   -1.4663465 ]\n",
      "   [ -8.633069    -8.325801    -1.5773704  -10.09647   ]\n",
      "   [ -8.450605    -4.192688     1.4179147   -7.872258  ]\n",
      "   [ -3.4774246   -1.0960379    2.3552895   -3.6499627 ]]\n",
      "\n",
      "  [[ -0.62310183   0.02354282   0.13477258  -1.0099144 ]\n",
      "   [ -2.3111663   -0.6952288    2.469388    -1.3457242 ]\n",
      "   [ -3.9942987   -1.6525842   -1.1253656   -4.210669  ]\n",
      "   [ -1.8414911    0.44378278   1.8752314   -2.3266878 ]]\n",
      "\n",
      "  [[ -3.365542    -1.0635742   -0.04164821  -1.7994132 ]\n",
      "   [ -6.9826465   -2.3183198    2.294877    -4.308402  ]\n",
      "   [ -5.801219    -3.403646    -4.5645704   -6.675435  ]\n",
      "   [  0.2971933    0.63473487  -0.6389538    2.6413    ]]\n",
      "\n",
      "  [[ -2.1686854   -1.023346     0.65018183  -2.5024278 ]\n",
      "   [ -4.622171    -0.8858723    5.574534    -4.401349  ]\n",
      "   [ -3.8713155    1.6690211    3.75436     -4.813896  ]\n",
      "   [ -1.399356    -0.4949215    2.2085276   -2.2406054 ]]\n",
      "\n",
      "  [[ -0.3049526    2.224203    -1.2487973   -3.0664551 ]\n",
      "   [ -4.3723803    1.9350511    0.1110547   -4.0373297 ]\n",
      "   [ -8.824966    -2.4120855    4.2221565   -6.5589924 ]\n",
      "   [ -2.1759007    2.9709213    0.01304059  -3.8550992 ]]\n",
      "\n",
      "  [[ -2.8292158    0.7908498   -2.1495862   -2.1313064 ]\n",
      "   [ -4.523009    -2.7817159   -3.0650115   -5.429205  ]\n",
      "   [ -5.458025     1.8018078   -4.9602304   -6.058091  ]\n",
      "   [ -4.791121     1.7835791   -2.0525951   -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.3626041   -3.6032155    4.161095     0.0656366 ]\n",
      "   [ -6.182477    -2.5122466    7.009508    -3.584936  ]\n",
      "   [ -3.4696164    1.1780642    5.1291347   -2.1059198 ]\n",
      "   [ -3.483398    -4.1618204    4.4601355   -0.07843098]]\n",
      "\n",
      "  [[  0.510499     1.036337    -1.0375209    0.28767318]\n",
      "   [ -3.3273916   -0.11347406   3.4049916   -3.419848  ]\n",
      "   [ -4.9530396    0.3851128    4.3673553   -5.12493   ]\n",
      "   [ -2.7946234    0.38115668  -0.20627333  -2.9363682 ]]\n",
      "\n",
      "  [[ -9.042869    -3.423595    -4.7580905   -7.272155  ]\n",
      "   [ -5.3064766   -1.7224908   -1.1489992   -3.3164253 ]\n",
      "   [ -5.864008     1.9313622    5.1383986   -4.273402  ]\n",
      "   [-10.078705    -3.3595207   -3.9119687   -7.9476895 ]]\n",
      "\n",
      "  [[  0.704116     0.5427282   -1.371693     1.7133871 ]\n",
      "   [ -7.3349566   -2.3262897    2.0722024   -6.863343  ]\n",
      "   [ -7.6698356   -5.039354     0.01469475  -8.557696  ]\n",
      "   [  0.14490138   1.3311632   -0.6794642    1.1242119 ]]\n",
      "\n",
      "  [[  5.0507197    3.1588564    1.3940698    5.1164274 ]\n",
      "   [-13.740164    -2.4651692    8.651231   -13.1501875 ]\n",
      "   [-13.236485    -2.9835963    8.292635   -12.939023  ]\n",
      "   [  1.3312078    2.3829906    2.8037574    1.0776967 ]]\n",
      "\n",
      "  [[ 12.874086     2.85469     -6.3369064   11.067723  ]\n",
      "   [ -1.5080522   -3.8416598    0.31655705  -2.3005078 ]\n",
      "   [ -5.6672664   -1.453503     2.7044997   -4.9066    ]\n",
      "   [ 10.062546     2.1319842   -5.8750806    8.317264  ]]\n",
      "\n",
      "  [[ -8.929089    -8.014712    -1.0568765   -8.620834  ]\n",
      "   [-10.908268   -10.764266    -0.8827691  -10.867963  ]\n",
      "   [-10.751021    -8.130157     2.3616416   -9.656115  ]\n",
      "   [ -9.412735    -8.36169     -1.0328311   -8.912132  ]]\n",
      "\n",
      "  [[ 10.875551     2.7801728   -1.4670037   12.899784  ]\n",
      "   [  2.4307857    2.7539134    2.4796565    3.4081655 ]\n",
      "   [ -7.5609837   -0.33894548   3.7714336   -8.186357  ]\n",
      "   [  8.785441     2.427588    -1.0477616   10.493459  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    0.86808515   0.68926543  -0.07710785   0.5841838\n",
      "      0.57039815   0.14776815   1.7208656 ]\n",
      "   [ -0.8052174    1.2325178    1.3822585   -0.6516272    0.6315028\n",
      "     -0.14370728   3.4813554    3.745227  ]\n",
      "   [ -0.29754275   1.5583472   -0.8161052    1.4953052    0.10321569\n",
      "     -3.3889081   -0.38736     -2.8569012 ]\n",
      "   [ -0.5443256    2.2166584    6.8072824   -9.876017     2.3808532\n",
      "      8.322778     7.1853633    9.047077  ]\n",
      "   [  0.7472351   -4.5735664    2.1001763   -0.68935      0.6213042\n",
      "      6.4843163    0.10306584   5.867399  ]\n",
      "   [ -0.29372013   1.6946385   -2.4606276   -1.9047768    3.2469358\n",
      "      2.6467237    0.40995654   9.953102  ]\n",
      "   [ -0.9051418    3.3132124   -0.7918448    7.3934846   -0.4331388\n",
      "     -9.205105    -4.1221614   -9.793383  ]\n",
      "   [  0.53786474   5.3989763   -0.25722694   3.55501      3.392188\n",
      "     12.746511    -0.88862467  34.191288  ]]\n",
      "\n",
      "  [[ -0.1123142    0.22450152   0.1997487   -0.5255737    0.31211773\n",
      "     -0.39361402  -0.09552613  -0.72650015]\n",
      "   [  0.31872883  -4.25332     -2.458986     4.503875    -2.5093522\n",
      "      0.16508146  -1.1448668   -2.2714942 ]\n",
      "   [  0.06266014  -2.8750339   -5.2976384    1.4493271    2.1512098\n",
      "     -1.3659165   -1.542527     8.231022  ]\n",
      "   [ -0.54926753   4.0720015    2.403759    -3.7070231    0.68023175\n",
      "      0.05022379  -2.1575356  -12.159414  ]\n",
      "   [  0.8732976   -1.8000302    3.722933     1.1266485   -2.602924\n",
      "      3.7650833    1.260653    -2.5786293 ]\n",
      "   [  0.33890194   2.1156766    0.45062694   1.4144641   -3.823428\n",
      "     -5.970876    -1.2156229    0.7578725 ]\n",
      "   [ -0.16106324   2.994075     0.45485264  -0.35378376  -4.149223\n",
      "     -3.9140332   -4.860044     1.4133708 ]\n",
      "   [ -0.11244611  -0.8303963   -7.594785    -5.361258    -0.77410525\n",
      "     -2.05425     10.385209    52.038113  ]]\n",
      "\n",
      "  [[ -0.10918517   0.20129828  -0.1020415    0.657444     0.22135806\n",
      "      0.15187913  -0.11050548   0.4570461 ]\n",
      "   [  0.3942903   -3.7951708   -3.841178    -0.39951256   0.29694498\n",
      "     -0.4107963    0.40402666  -7.3379164 ]\n",
      "   [ -0.45792332  -2.8219523   -3.4908211   -0.8908195    1.6258212\n",
      "     -2.935929    -4.5810666   -3.7579439 ]\n",
      "   [  0.4032216    1.4782436    8.961608    -4.7050643    4.6159906\n",
      "      4.017976     7.0261846    6.3173122 ]\n",
      "   [ -0.39189684   4.1333766    9.57504      1.0154871    0.6772467\n",
      "      0.85469484   5.3734183   12.290649  ]\n",
      "   [ -0.6209384   -0.1761216    3.239656     0.448386     2.965093\n",
      "      0.04321224  -0.55968136  16.471697  ]\n",
      "   [ -0.13727854  -2.334021    -0.12732092   2.6658416   -0.16601619\n",
      "      5.0760036   -8.124224    -7.4266424 ]\n",
      "   [  0.15005337   2.332366    -9.562306    -1.163947     1.6641262\n",
      "      0.01446699  -1.8103666    2.723766  ]]\n",
      "\n",
      "  [[ -0.22114104   0.78687924   0.38468125  -0.35858485   0.4814524\n",
      "      0.7716349    0.18577488   1.2330012 ]\n",
      "   [ -0.32672447   1.066411     0.20903866  -0.35502467   1.9553363\n",
      "      2.3845599    4.0547504   18.488571  ]\n",
      "   [  0.17269178  -1.3044931   -4.883713    -0.30554432   0.34996486\n",
      "     -3.483534    -0.35805675 -10.02802   ]\n",
      "   [  0.08333597   1.9566892    3.4552226   -3.034405     1.9709632\n",
      "     -0.00770508   1.6790633   -0.6475895 ]\n",
      "   [  0.19127834  -0.42828786   4.3851147    0.4973215    2.414637\n",
      "      2.6963992    4.829734    14.507266  ]\n",
      "   [ -0.02093794   3.6430805    8.713464    -0.52347636   9.823708\n",
      "     -0.37148795   4.448587    14.360994  ]\n",
      "   [ -1.3862007    9.094722     6.5496054   -1.6104932    9.99708\n",
      "      6.1065807    0.33539578   9.827821  ]\n",
      "   [ -1.6252928    7.214711     5.637337     0.02005185   3.2679813\n",
      "     15.615637     9.829886    36.591232  ]]\n",
      "\n",
      "  [[  0.00076126  -0.47878256   0.09911879   0.00893964   0.24509\n",
      "      0.14304979  -0.08038879   0.59638596]\n",
      "   [  0.7206705   -4.158416    -1.74024      4.0903745   -0.7901846\n",
      "     -2.9663303    0.31932393  -5.670474  ]\n",
      "   [  0.12035121   5.0561695   -2.9415672   -0.26743913  -4.871599\n",
      "      1.4902955   -1.8848196   -4.6832967 ]\n",
      "   [  0.03012125   5.2640715    3.1126494   -4.628424    -2.5972195\n",
      "      1.9151638   -0.22672556  -1.8828552 ]\n",
      "   [  0.44786143   4.194443     3.1239393   -2.8355985   -5.2169623\n",
      "      3.1980612    2.4337146   -1.4847306 ]\n",
      "   [  0.61835045  -3.2027795    1.4719862    3.313971     2.5021975\n",
      "     -1.5920473    1.3572292   -1.1208097 ]\n",
      "   [  0.2457593   -7.236913     0.20529102   8.781249     3.4424264\n",
      "     -5.4454756   -6.8731003   -3.8414416 ]\n",
      "   [  0.7628397   -6.2281985   -3.1565511    3.4787645    0.7064748\n",
      "     -6.4276037   -1.101056   -11.2021885 ]]\n",
      "\n",
      "  [[ -0.00999826  -0.17336224  -0.04885251  -0.06919655  -0.32633722\n",
      "      0.0366222    0.16041176   0.54738736]\n",
      "   [  1.4367385   -9.06984     -3.8178802   -0.75747144  -3.7752612\n",
      "     -7.119363    -0.808656    -4.2169037 ]\n",
      "   [  0.92183876  -4.7562103   -6.4032483   -2.5681021   -2.3384533\n",
      "     -3.918842     3.1888914   -5.599762  ]\n",
      "   [ -0.15983817   5.33236      4.7559114    0.13260181   1.7207963\n",
      "      4.4075403    5.9188757    5.94741   ]\n",
      "   [  0.10523379   1.6092048    6.520943    -0.82788485  -2.471699\n",
      "      2.2942595    0.73225766   1.9809994 ]\n",
      "   [  0.71476805  -5.8466797   -1.9964423   -1.6363546   -0.9318489\n",
      "     -4.4617267   -0.7425307   -7.6074624 ]\n",
      "   [  1.4523759   -9.992252    -9.585231    -3.4427912   -8.658167\n",
      "    -10.796175   -10.854523    -7.489971  ]\n",
      "   [ -0.27465576  -1.0464218   -2.3824263    1.4090494    1.2699353\n",
      "     -3.766512    -8.822025    10.6977215 ]]\n",
      "\n",
      "  [[ -0.20863976   0.569198     0.44341293   0.16857311   0.1010042\n",
      "      0.3144073   -0.57303137   0.6825493 ]\n",
      "   [ -0.44067878  -1.3154124    2.2516541    5.957117     3.327574\n",
      "      2.1122663    4.2094145    2.6641772 ]\n",
      "   [ -0.73116076   0.78557914   1.7380664    4.084935     3.872323\n",
      "      3.7347608    1.9456888    2.1396089 ]\n",
      "   [  2.1423054   -2.7513258   -6.220788    -6.2902803   -5.0086665\n",
      "     -6.650022     0.06290687  -0.05239553]\n",
      "   [  2.4718273   -3.7755775   -3.795776    -5.034314   -10.499384\n",
      "     -6.276488    -3.7034833  -11.084311  ]\n",
      "   [  1.5774866   -2.8405964   -6.4159265   -3.2861729   -4.428243\n",
      "     -9.614207    -3.1512659    4.924799  ]\n",
      "   [ -2.476524     1.4815274    2.88176      8.238331     9.522714\n",
      "     11.843423    -2.1702423    3.0296    ]\n",
      "   [  0.7869987   -7.7469463   -6.209672     4.5435677    1.8652452\n",
      "     -7.86043      0.34027758  30.456985  ]]\n",
      "\n",
      "  [[ -0.15726982   0.34657618   0.09771852  -0.06033853   0.13718088\n",
      "      0.4450618   -0.11978562   0.62471557]\n",
      "   [ -0.20781471  -3.9169958   -2.6317358    2.4233148    1.5008358\n",
      "      2.266367    -2.2837386    3.4158893 ]\n",
      "   [  0.49460515  -2.8374786   -6.512395    -2.2088082   -3.047029\n",
      "     -2.7998128   -4.838161    -1.5167636 ]\n",
      "   [  0.40351048   5.496288     7.4884925   -5.00473     -0.34395367\n",
      "      0.5531984    3.1644454   -3.476924  ]\n",
      "   [  0.20931137   1.581105     5.1500797   -1.7881161   -0.8540002\n",
      "     -1.366854     0.834874     1.5237077 ]\n",
      "   [  0.27719426   3.7286282    1.4238677   -2.3924153    6.4014854\n",
      "     -0.2659451    2.9778736    8.756394  ]\n",
      "   [ -0.38605455   4.430161     5.1124682   -4.818994     2.8128343\n",
      "      3.5536995   -1.2899668   16.034073  ]\n",
      "   [  0.3564654   -0.45678574 -10.023928    -4.4554677    6.020903\n",
      "      0.9165316   -5.4710746   37.49395   ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.43004218 -2.1264591  -4.20363     0.41727006]\n",
      "   [ 0.36214504 -0.9224591  -3.69376     0.32345214]\n",
      "   [-0.11888524  0.7827182   1.6091667  -0.07043942]\n",
      "   [ 1.5769911  -1.5107999  -4.705859    1.5822129 ]\n",
      "   [ 1.433958    0.35687694 -0.44859353  1.4714174 ]\n",
      "   [ 0.6759765  -2.6841862  -5.7552285   0.64645445]\n",
      "   [ 1.4815818  -2.0108624  -5.7773795   1.438906  ]\n",
      "   [ 2.3780522  -0.5499924  -3.778074    2.424371  ]]\n",
      "\n",
      "  [[ 1.3539228  -1.1119139  -3.6765318   1.3366559 ]\n",
      "   [ 0.49521226  0.8556765   1.0242827   0.54091555]\n",
      "   [ 1.8845222   0.80958265 -1.4450098   1.8755151 ]\n",
      "   [ 3.1395586  -0.3902885  -4.6706777   3.0263221 ]\n",
      "   [ 2.4448514  -0.79036796 -4.79417     2.363135  ]\n",
      "   [ 1.9017469  -1.5351089  -6.007563    1.8183545 ]\n",
      "   [ 1.7505506  -0.7003997  -4.214069    1.6834856 ]\n",
      "   [ 2.3057952  -1.3811989  -5.0667324   2.2040026 ]]\n",
      "\n",
      "  [[-0.6358987   0.6857092   3.934223   -0.6082751 ]\n",
      "   [ 2.1257792  -0.75171655 -5.1641245   2.0634513 ]\n",
      "   [ 1.9908828  -2.4718666  -8.323398    1.8416339 ]\n",
      "   [ 1.4525386  -1.9171642  -6.0657043   1.3277422 ]\n",
      "   [ 2.0609357  -2.3901558  -8.3400135   1.8740809 ]\n",
      "   [ 2.1479387  -0.04788223 -3.5392594   2.0927868 ]\n",
      "   [ 0.8613854  -1.3274925  -4.003621    0.7955877 ]\n",
      "   [ 2.692351   -1.1135979  -6.6664057   2.5669687 ]]\n",
      "\n",
      "  [[-0.5390432   0.7702814   1.6837941  -0.4367591 ]\n",
      "   [ 1.0922868  -2.2901568  -6.70844     0.99209934]\n",
      "   [ 0.63555646 -0.9029067  -2.4676714   0.5942919 ]\n",
      "   [ 1.3171933  -1.6731497  -4.4172173   1.2324891 ]\n",
      "   [ 1.0370547  -2.4209025  -5.395325    0.9418529 ]\n",
      "   [ 0.70518225 -1.6686244  -4.040526    0.64739156]\n",
      "   [ 1.5462278  -2.7973738  -7.242962    1.4810474 ]\n",
      "   [ 1.3236113  -3.1562757  -7.528523    1.2352623 ]]\n",
      "\n",
      "  [[-1.7341876   0.2025832   1.12208    -1.7460632 ]\n",
      "   [-0.38023633 -0.88305074 -1.6188183  -0.4605108 ]\n",
      "   [ 0.7600631  -0.39524692 -3.455463    0.6971765 ]\n",
      "   [ 3.004541   -3.5289714  -9.588761    2.9130647 ]\n",
      "   [ 2.5986595  -3.3891108  -8.484004    2.4783466 ]\n",
      "   [ 2.6414726  -3.1978264  -6.90967     2.5648355 ]\n",
      "   [ 2.475332   -2.112042   -3.9433625   2.4765618 ]\n",
      "   [ 2.714274   -3.0151482  -7.1374545   2.6470444 ]]\n",
      "\n",
      "  [[ 1.9698906   1.0078789  -1.2122052   2.0134254 ]\n",
      "   [ 1.8958745  -0.45892912 -5.3896065   1.8568405 ]\n",
      "   [ 1.4736686   0.538687   -1.965722    1.469285  ]\n",
      "   [ 3.1977854   0.05167901 -5.276443    3.1679316 ]\n",
      "   [ 2.3644834   0.91031945 -2.6867223   2.3744936 ]\n",
      "   [ 1.4999217  -3.062734   -6.6033297   1.5126845 ]\n",
      "   [ 1.4788055  -2.0994508  -6.462512    1.3902283 ]\n",
      "   [ 3.2440507   0.9307277  -4.3857727   3.26184   ]]\n",
      "\n",
      "  [[ 0.1258963  -0.71917385 -2.2927022   0.03860371]\n",
      "   [ 1.3117977  -1.6638893  -4.465428    1.2304243 ]\n",
      "   [ 2.5624385  -3.8971152  -9.53067     2.4499042 ]\n",
      "   [ 2.0999384  -1.6019284  -5.034336    2.0732462 ]\n",
      "   [ 0.7817824  -1.5657676  -3.6030664   0.7704573 ]\n",
      "   [ 1.6116695  -3.2206306  -7.0788507   1.4964689 ]\n",
      "   [ 1.9198471  -0.6208935  -2.765539    1.9039177 ]\n",
      "   [ 2.2639053  -2.4139745  -7.24187     2.2374074 ]]\n",
      "\n",
      "  [[-0.31415597  2.5866678   6.2770805  -0.21654116]\n",
      "   [ 1.1086718  -2.3592036  -5.937715    1.05256   ]\n",
      "   [ 1.0852163  -4.0399203  -9.892566    0.9899011 ]\n",
      "   [ 1.6877527  -2.2720673  -3.6519856   1.6843686 ]\n",
      "   [ 1.2909392  -2.185475   -4.1043363   1.2519479 ]\n",
      "   [ 2.045505   -2.9272645  -7.5192847   2.0481453 ]\n",
      "   [ 1.0800407  -1.0022451  -3.029588    1.0860733 ]\n",
      "   [ 2.0633914  -2.8612106  -7.9108605   2.066739  ]]]], shape=(1, 8, 8, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 4)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 8, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 8)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.77654105  -1.1600666   -0.7377553    0.15321049  -0.33445883\n",
      "      0.9638917    1.0282744   -0.2849048 ]\n",
      "   [  4.2638865   -4.9808917   -4.7282324   -7.588514    -5.1648345\n",
      "      0.752576     1.5854373   -2.15158   ]\n",
      "   [  3.7290585   -3.6264493   -4.6852508   -2.8495634   -3.194244\n",
      "      2.0674891    0.16462609  -2.2241864 ]\n",
      "   [  3.8470237    1.1639067   -2.604357    -4.39899     -3.9771621\n",
      "     -0.7163415   -2.6310756   -0.75265104]\n",
      "   [  5.5331926   -2.1469176   -7.7634535   -5.166938    -6.2337794\n",
      "      1.4473847   -0.10470335  -2.271537  ]\n",
      "   [  4.7488627    0.8060199   -2.7915359    0.2651618    2.8968983\n",
      "     -4.7307568   -3.8274188    0.29649776]\n",
      "   [ -3.646525    -2.417211     0.9628969    3.695615     4.510241\n",
      "      0.65492576  14.975302     8.191229  ]\n",
      "   [  2.7993503   -0.3259454   -1.584748    -1.4308795    0.67866737\n",
      "     -0.04990503   1.5698798    1.5675912 ]]\n",
      "\n",
      "  [[  0.02615874   1.1034771   -0.24971536  -0.62090695   0.00287016\n",
      "      0.03166713   0.67804277   1.4503165 ]\n",
      "   [ -1.1199212   -1.2406197   -0.3052636    2.3498487    1.2994226\n",
      "     -0.1909589    1.7674519    2.4766655 ]\n",
      "   [ -1.1390502    0.8624328    2.084101     1.3884075   -0.980912\n",
      "      1.0793152    0.07685544  -1.0194689 ]\n",
      "   [  1.2047955   -2.8474126   -2.2027948    1.087519     1.6147972\n",
      "      0.44935256   0.00483328   0.46766946]\n",
      "   [  3.020494    -0.39609548  -2.2673187   -2.7705114   -3.26368\n",
      "     -0.7973336   -1.0194429   -0.5001688 ]\n",
      "   [ -1.8445143   -3.3527074    1.6643316    5.427364     4.1477027\n",
      "      1.6791626    1.010268     2.87207   ]\n",
      "   [ -0.23157938   3.4557557    2.9004824   -0.5162866   -0.8175649\n",
      "      1.2562578   -1.8160521   -1.7075723 ]\n",
      "   [  0.7464798   -1.4194397    0.13460317   0.12417914   0.9162833\n",
      "      2.1309881    0.5100187    1.2598475 ]]\n",
      "\n",
      "  [[ -2.9237115    3.363596     2.3430805    0.21954887  -0.1109451\n",
      "      0.492053    -0.16566317   2.1276236 ]\n",
      "   [ -0.80274636   4.677598     1.8153228    2.9806826    2.4021566\n",
      "     -1.9523367    0.08145932   2.0710855 ]\n",
      "   [ -2.9179065    2.4044318    0.6520111    6.7438183    2.0902429\n",
      "     -2.2136843    2.1637242    6.279543  ]\n",
      "   [ -3.690457     0.7928321    8.471516    -1.8322024    4.8284726\n",
      "      2.881544     4.1723924    8.86362   ]\n",
      "   [  1.9101584    1.9778312    1.4799114    3.4420455    4.4670367\n",
      "     -1.8857652    4.515708     3.8148475 ]\n",
      "   [ -1.2988942   -0.97658354  -0.9146665    2.3100488   -1.0963827\n",
      "      0.26074374   0.40357405   1.558576  ]\n",
      "   [ -1.8591923    3.461019    -2.158382     5.977408     0.09147047\n",
      "     -7.350376     0.48368448   0.4763946 ]\n",
      "   [  1.9888997    0.60271686  -0.70304686   2.4956057    2.826067\n",
      "     -2.518859     2.7215922    4.3303957 ]]\n",
      "\n",
      "  [[ -2.8092232    1.49841      1.9324561    0.96108407   1.7129885\n",
      "      1.6189119    0.54601675   1.7578119 ]\n",
      "   [ -2.5165534    2.19107      0.6257863    7.272254     2.259812\n",
      "      3.410876     0.34877956  -1.077494  ]\n",
      "   [ -2.6379817    2.7387853   -1.0823029    6.826854     1.8786801\n",
      "      2.3327372   -2.5237718    0.81594455]\n",
      "   [ -2.4187973   -3.4014497    3.5254593    0.5569787    5.067711\n",
      "      0.5633194    7.8339043    0.92556983]\n",
      "   [ -1.945622    -0.65303457   2.4640982    3.3566499    4.1663265\n",
      "     -1.1195359    0.9511546    2.4992924 ]\n",
      "   [ -4.171936    -0.01147364   3.4513283    3.152668     4.512732\n",
      "      9.864028     4.1050034    3.75863   ]\n",
      "   [  1.0633516    8.69028     -0.31681222   2.8985076    0.9200999\n",
      "      0.9111165  -13.286961     0.17364249]\n",
      "   [  1.5136805   -0.02733484  -0.39001307  -3.0631852   -0.87316304\n",
      "      0.41900763  -1.3661568    2.2355921 ]]\n",
      "\n",
      "  [[ -0.4573368   -0.41149053   0.57131124   0.99204594   0.7350709\n",
      "      0.20620687  -0.9990892    0.08503257]\n",
      "   [  3.5183      -1.5149683   -4.673192    -3.8204358   -2.732508\n",
      "      0.3246554   -3.8637986   -4.811856  ]\n",
      "   [ -0.12923025   1.2998483   -1.2514737   -1.2474041   -0.5723316\n",
      "     -2.1154923   -0.44402006   1.7364616 ]\n",
      "   [  1.3779988    4.212641    -3.3267968   -0.59099406   3.2183704\n",
      "     -5.157247    -2.7787948    1.1561528 ]\n",
      "   [  1.6397702    0.8696879   -2.9280584   -0.5760767   -0.3494962\n",
      "     -2.4273357   -1.5823427   -0.34897333]\n",
      "   [  0.33756587   2.3127203   -0.16594982   2.7338834    6.459333\n",
      "     -3.8244364    0.3468593    2.1036468 ]\n",
      "   [ -0.4835711   -2.4353967   -2.2305977   -2.4151573   -5.3643565\n",
      "      0.38799766   0.41155824  -2.5352807 ]\n",
      "   [  1.8838992   -0.4463871   -3.5580878   -3.5084794   -3.9236977\n",
      "     -1.1194205   -1.008667    -2.3395345 ]]\n",
      "\n",
      "  [[ -1.4886304   -0.8160711    1.272815     1.069158    -1.2259004\n",
      "      0.27663958   0.5322572    0.5516533 ]\n",
      "   [  0.90435994  -2.5749598   -1.9510299   -1.5345374   -0.7728811\n",
      "     -1.8463182    0.02996135  -0.52689105]\n",
      "   [ -0.34667596  -1.3035538   -2.9048364   -0.24566376  -0.21664956\n",
      "     -2.8083339    1.8281504    0.4519842 ]\n",
      "   [  1.2138464    1.9463997    1.1304235    4.3117375   -0.93706936\n",
      "     -2.5150895   -3.121864    -0.9676549 ]\n",
      "   [  2.2285655    0.59783006   2.3479755    1.71059     -4.110444\n",
      "     -4.411427    -1.5514175   -6.017995  ]\n",
      "   [  0.04869156   4.3527355    3.3943183    3.6977544    2.4533684\n",
      "     -0.86546636  -0.87341183   3.2614863 ]\n",
      "   [ -0.17167635  -4.339262    -4.7814736   -7.6732864   -1.959909\n",
      "     -0.5605185    5.837771    -2.5001943 ]\n",
      "   [  0.77141756   0.8610089    1.142861     0.47858885  -0.17277306\n",
      "     -0.5125513    2.0650654   -1.8335835 ]]\n",
      "\n",
      "  [[ -2.3999348    1.3400928    0.6617681   -1.8778961   -0.45578986\n",
      "      2.2384698    1.2069004    2.4183605 ]\n",
      "   [ -1.6690627    0.10749228   0.34268987   1.8814775    2.1918917\n",
      "      0.12542067   2.5933814    1.2904687 ]\n",
      "   [ -1.4865489   -3.3957736   -3.0061002    3.6486413    3.2406335\n",
      "      1.885369    -3.1033058    3.348345  ]\n",
      "   [  2.238486     0.6319354   -1.8617017   -5.3909755   -1.2285255\n",
      "     -2.478067    -0.79074955  -4.5622325 ]\n",
      "   [  2.793881    -1.9779224   -3.7091935   -1.6687323   -2.2475994\n",
      "     -2.747856    -4.7172976   -3.6291654 ]\n",
      "   [  2.2604463    0.89827144  -2.7981474    0.4819163    1.2817676\n",
      "     -2.3887415   -3.3407822   -0.8095417 ]\n",
      "   [ -0.22698143  -0.77705026  -0.24666077   1.3159165    0.6648407\n",
      "     -1.9169033   -3.035924     0.6617716 ]\n",
      "   [  2.6439736   -2.576209    -1.4652632    1.4973457   -0.8203294\n",
      "     -1.3426143   -2.8720593   -3.0487075 ]]\n",
      "\n",
      "  [[ -1.9244474    2.1994686    1.347579     0.2057227   -0.04820146\n",
      "      2.141403    -1.3585992    1.0393548 ]\n",
      "   [ -0.34355617  -2.1330175    0.8624636   -2.9745345   -1.522738\n",
      "      0.9593218    1.0111009   -0.6772518 ]\n",
      "   [  0.41986907  -3.7553241   -1.9478018   -1.2957213   -3.4820578\n",
      "     -2.2611165   -1.2612828   -3.0849237 ]\n",
      "   [  0.42432842   0.5639865    4.050384    -1.9052758    1.185033\n",
      "      1.0191929    3.7474763    1.1618733 ]\n",
      "   [  0.7143441   -2.7222555    1.654944    -0.4344939   -3.5097253\n",
      "      0.19337063   0.334209    -0.459867  ]\n",
      "   [ -0.24068627   0.5629927    3.9413342    2.948522     6.2797832\n",
      "      1.3845129    7.0708156    4.1701803 ]\n",
      "   [  0.23093589   2.4334755   -1.4492497   -0.90904254   2.798429\n",
      "      0.2731452   -0.4825536    1.6540896 ]\n",
      "   [  1.7498912   -3.047833    -1.3996111   -0.60452443  -0.47163075\n",
      "     -1.143289    -0.23828623   3.332716  ]]]], shape=(1, 8, 8, 8), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 8)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "(1, 8, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 8, 256)\n",
      "(1, 8, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 8, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.4390437   -0.33947098   4.013443    -0.29624832]\n",
      "   [ -0.2583174    0.33025625  -0.39602375  -0.30866668]\n",
      "   [  0.32099545  -0.95505685  -4.5404463    0.24053822]\n",
      "   [  0.63142234  -1.2339483   -6.734192     0.53280115]\n",
      "   [  0.54130554   0.21318378  -0.52515334   0.56435126]\n",
      "   [  0.36491862  -0.681736    -3.822705     0.32760125]\n",
      "   [  0.41342953  -0.61083317  -4.5110703    0.26699755]\n",
      "   [  1.3322448   -1.2706621   -7.9235134    1.2262651 ]]\n",
      "\n",
      "  [[ -0.43239835   0.42818624   2.6262326   -0.4062209 ]\n",
      "   [ -0.22419338   1.074776     1.1630725   -0.19326892]\n",
      "   [  0.51628953   0.88442105  -0.02622377   0.5438406 ]\n",
      "   [  0.7893162   -3.560948    -7.5051656    0.6630447 ]\n",
      "   [  0.57238746  -1.7088602   -4.2579656    0.5265007 ]\n",
      "   [  0.4162348   -1.7032979   -3.9784098    0.36342958]\n",
      "   [  0.7785082    0.6506525   -1.7838467    0.78410023]\n",
      "   [  0.955276    -4.4643097   -8.716853     0.80637485]]\n",
      "\n",
      "  [[ -0.44927993   0.54527193   4.6114116   -0.29629943]\n",
      "   [  0.67831874  -0.902597    -5.0636787    0.530373  ]\n",
      "   [  1.1608257   -0.9582575   -6.0131745    1.0148909 ]\n",
      "   [  1.8622837   -0.54619     -6.1305943    1.7046378 ]\n",
      "   [  0.5365511   -0.7164705   -4.3196692    0.4233776 ]\n",
      "   [  0.9778634    0.05141426  -1.4833766    0.9106759 ]\n",
      "   [  0.8324067   -1.1943607   -5.281966     0.704296  ]\n",
      "   [  1.7688038    0.34723103  -4.047477     1.6763012 ]]\n",
      "\n",
      "  [[ -1.5574945   -0.13661464   4.8192244   -1.5274014 ]\n",
      "   [  1.8428049    0.10988725  -3.7634213    1.8194834 ]\n",
      "   [  1.8790634    0.16765016  -5.2636895    1.8923659 ]\n",
      "   [  1.8488076    0.38219947  -2.467583     1.8204844 ]\n",
      "   [  1.9478984    0.24887773  -4.0790386    1.8869959 ]\n",
      "   [  2.0514908    0.8322106   -3.4669142    2.0144742 ]\n",
      "   [  3.016969    -1.706834    -9.285852     2.9572225 ]\n",
      "   [  2.9171197   -0.49268994  -7.3772154    2.900941  ]]\n",
      "\n",
      "  [[ -1.0361291    1.0311096    2.9110353   -1.0666645 ]\n",
      "   [  0.17529838  -0.76200086  -1.3703144    0.18987674]\n",
      "   [  0.9855322   -1.2880604   -2.9897346    0.9731935 ]\n",
      "   [  0.7058304   -0.60330254  -3.2749028    0.66278076]\n",
      "   [  0.73217136   0.31563684  -1.2379584    0.7082298 ]\n",
      "   [  1.0366942   -0.6526082   -2.5682065    1.0441903 ]\n",
      "   [  1.6105137   -1.1913971   -3.527507     1.6231617 ]\n",
      "   [  1.3619897   -0.6999447   -3.4508295    1.3328699 ]]\n",
      "\n",
      "  [[ -0.7908757   -1.3115877    0.67647886  -0.8404614 ]\n",
      "   [ -0.19861834  -1.3611583   -2.9659412   -0.28904545]\n",
      "   [ -0.5602697   -0.27112013   0.11186516  -0.6005517 ]\n",
      "   [  0.77641386  -0.26496348  -3.3041744    0.8275802 ]\n",
      "   [  1.4017886    1.7031087   -1.5688373    1.5331511 ]\n",
      "   [  1.1998662    0.4657598   -1.240734     1.2614617 ]\n",
      "   [ -0.01129011   1.611884     3.5301998    0.02480664]\n",
      "   [  1.0106618   -0.13875784  -2.6199062    1.0687411 ]]\n",
      "\n",
      "  [[ -0.21379697  -0.3550262    2.452176    -0.14885877]\n",
      "   [  1.1190844   -0.06087807  -3.8031468    1.0202737 ]\n",
      "   [  0.3503374   -1.1730963   -4.796474     0.2665696 ]\n",
      "   [  0.7476409    0.9311516   -1.8346077    0.6924049 ]\n",
      "   [  0.8267951    0.640895    -1.8839159    0.7792115 ]\n",
      "   [  0.15174165  -0.8480215   -3.930585     0.0703403 ]\n",
      "   [  0.48000973  -2.0706      -5.5565906    0.40147567]\n",
      "   [  0.7635295   -0.961924    -3.7272053    0.67435676]]\n",
      "\n",
      "  [[ -1.6940544   -1.1217345    4.9840846   -1.5811146 ]\n",
      "   [  2.1202106   -1.363047   -10.718351     1.9247364 ]\n",
      "   [  0.94408834   1.107803    -3.365614     0.8552538 ]\n",
      "   [  2.3723536   -2.03031    -11.663365     2.1812994 ]\n",
      "   [  1.1664867   -1.2401501   -6.0198855    1.0814999 ]\n",
      "   [  2.8200953   -0.9444836   -9.885286     2.6860921 ]\n",
      "   [  1.6303653    2.4244456   -1.4615241    1.6263697 ]\n",
      "   [  2.2062964   -0.65275395  -7.387656     2.1077921 ]]]], shape=(1, 8, 8, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 8, 4)\n",
      "output.shape = (1, 8, 8, 32)\n",
      "scaled_attention.shape= (1, 8, 8, 32)\n",
      "concat_attention.shape= (1, 8, 256)\n",
      "outputs.shape= (1, 8, 256)\n",
      "Input: 너무 화가나\n",
      "Output: 그럴수록 당신이 힘들 거예요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"너무 화가나\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724    5.2739615    0.44445598   0.6199022   -1.8606094\n",
      "     -5.4542537 ]\n",
      "   [ -2.6434557    0.7762684   -0.8715434   -2.1795447   -3.9540012\n",
      "     -4.6782713 ]\n",
      "   [ -6.1793337    5.196875     0.9097851   -2.007469    -3.7270458\n",
      "     -8.857345  ]\n",
      "   [ -5.1220007    6.1545386   -1.5864905   -0.9809261   -1.6544849\n",
      "     -6.897224  ]\n",
      "   [ -2.9175513    4.668971     1.8842993   -1.2460753   -3.9725554\n",
      "     -4.511021  ]\n",
      "   [ -3.92207      5.6859646   -0.5558028   -1.2563441   -1.7261751\n",
      "     -6.1812677 ]]\n",
      "\n",
      "  [[ -5.830585     6.4584546    1.6885545   -2.9425578   -2.1129007\n",
      "     -4.4590826 ]\n",
      "   [ -1.4542547    0.66857773  -0.8920885    3.2392426    0.6499656\n",
      "     -0.78637475]\n",
      "   [ -0.77001333   2.5683331   -1.5277501   -1.2103951   -3.361052\n",
      "     -0.6701153 ]\n",
      "   [ -6.770008     4.0346427    3.700929    -5.633308    -3.0441353\n",
      "     -4.6253424 ]\n",
      "   [ -3.266022    -0.2358284    5.192181    -3.326681    -1.1964418\n",
      "     -3.002487  ]\n",
      "   [ -3.8395674    5.5131884    2.4825306   -2.030505    -1.288316\n",
      "     -2.5861757 ]]\n",
      "\n",
      "  [[  0.21261294   2.6522226   -2.1467347   -1.7591757   -1.8478366\n",
      "     -1.005188  ]\n",
      "   [  3.4450672   -9.425682     2.968103     1.0088919    0.15486461\n",
      "      2.6339269 ]\n",
      "   [-10.890656    14.99533      1.3811018    2.7970884   -4.1616673\n",
      "    -12.529773  ]\n",
      "   [ -5.4948997    7.8948936    2.588478     0.5435209   -0.2743195\n",
      "     -5.1724825 ]\n",
      "   [ -3.2168324    0.6599828    1.3618138    2.5563478   -1.6389858\n",
      "     -2.9406655 ]\n",
      "   [ -1.7681361    6.479452     0.12432577  -1.3926682   -2.481772\n",
      "     -2.4043    ]]\n",
      "\n",
      "  [[ -0.62310183   2.588975     0.39473668   2.3254414    0.3566933\n",
      "     -0.95473033]\n",
      "   [ -3.2329118   -1.6349312    4.420214    -1.4593328   -0.2873762\n",
      "     -3.3657458 ]\n",
      "   [ -5.5691104    0.9040649    1.5155857   -1.662806    -1.17778\n",
      "     -5.2585945 ]\n",
      "   [ -5.513655     0.39596844   1.2438383   -1.9474496   -1.6561016\n",
      "     -4.673764  ]\n",
      "   [ -3.756813     2.3833623    7.4268293    1.2679268   -2.3513136\n",
      "     -4.9471974 ]\n",
      "   [ -1.728882     1.0250845    1.9032011   -0.48647845  -1.0321277\n",
      "     -2.5369413 ]]\n",
      "\n",
      "  [[ -3.365542     1.8547945    2.3893828   -0.2378472   -1.7009665\n",
      "     -2.4370363 ]\n",
      "   [ -1.2823843    4.7429194   -4.1070457   -0.47443444  -1.9375597\n",
      "     -1.6068171 ]\n",
      "   [ -5.0705113    7.2453456    0.22619136   3.822259    -2.3195603\n",
      "     -3.822914  ]\n",
      "   [ -3.6519911    2.915474     0.76882476  -0.10871613  -4.1921997\n",
      "     -2.662963  ]\n",
      "   [  2.4449081   -1.4922832    0.5579315   -0.2331474    1.4929048\n",
      "      1.6919394 ]\n",
      "   [  0.68565965  -1.3965738    2.239955     1.8028462    2.3655758\n",
      "      1.6723614 ]]\n",
      "\n",
      "  [[ -2.1686854    1.8683971   -0.46407908  -0.78368795  -2.3975863\n",
      "     -2.3532345 ]\n",
      "   [ -6.185767     2.228677     0.71968794  -2.434057    -5.3960166\n",
      "     -4.845154  ]\n",
      "   [  0.25900394   2.3122008    0.9818626    2.2493217   -0.2135596\n",
      "     -1.5250982 ]\n",
      "   [ -3.6363199    5.520364     2.119328    -1.6462586   -3.9594364\n",
      "     -5.000841  ]\n",
      "   [ -3.2407305    4.0637727   -3.65974     -1.2874398   -5.055235\n",
      "     -1.0524285 ]\n",
      "   [ -1.6338614    2.8127375   -0.63038987  -1.1972872   -2.4497988\n",
      "     -2.5156808 ]]\n",
      "\n",
      "  [[ -0.3049526   -0.4231821    2.3549898    1.2362137   -3.3428524\n",
      "     -2.4705725 ]\n",
      "   [ -8.393576     1.2230886   -0.47221878  -0.3785542   -9.154858\n",
      "     -9.623112  ]\n",
      "   [ -7.879456     1.0038036    6.269445     3.383196    -5.1612587\n",
      "     -9.652613  ]\n",
      "   [ -6.5884266    1.8914772    0.58854324   0.7356621   -3.4622736\n",
      "     -8.281077  ]\n",
      "   [  3.2697265   -0.22400014  -1.8101139    0.84822214   0.9626521\n",
      "      4.273874  ]\n",
      "   [ -1.6477983   -0.3436993    2.7197764    0.96644586  -0.9888108\n",
      "     -3.2478032 ]]\n",
      "\n",
      "  [[ -2.8292158    2.69152     -5.9879627    0.05573386  -0.583922\n",
      "     -0.6253632 ]\n",
      "   [ -5.2028084    1.2464813   -0.28601608   0.14257993  -7.337432\n",
      "     -6.4034657 ]\n",
      "   [ -8.92803      6.5982924   -2.5037014    1.7294589   -8.505811\n",
      "     -8.242324  ]\n",
      "   [ -2.8773344    3.950198     0.63110507   2.2820823   -5.6118255\n",
      "     -3.8134937 ]\n",
      "   [ -1.743894     3.723778     3.8605156    5.6352224   -8.514911\n",
      "     -4.5736675 ]\n",
      "   [ -1.8900229    3.1358528   -4.051401    -0.7488651   -2.9355698\n",
      "     -1.1315701 ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.921542     1.008876     0.3188283   -0.09219998  -3.1753793\n",
      "     -6.121585  ]\n",
      "   [ -7.19034      2.2363086    1.0159979    2.2710261    1.5986332\n",
      "     -6.611934  ]\n",
      "   [ -7.901739     3.423445     0.28121415   1.4161541    0.7545412\n",
      "     -6.8629513 ]\n",
      "   [ -4.4973965    2.089329    -1.5849537    0.9947852    1.7196369\n",
      "     -4.0510983 ]\n",
      "   [ -3.9672196   -0.16474341  -4.148401    -1.2713867    0.75565946\n",
      "     -4.159244  ]\n",
      "   [ -5.64101      0.69921917  -0.1817732   -0.8431055   -2.560344\n",
      "     -4.9824452 ]]\n",
      "\n",
      "  [[ -1.7980527   -1.5812842   -1.9451343   -3.2220113  -10.310742\n",
      "     -3.824608  ]\n",
      "   [ -3.479981     3.2808747    2.645604     1.2420398   -2.2346535\n",
      "     -2.8920004 ]\n",
      "   [-12.847355     4.693563     1.7069713    0.14368261  -8.192775\n",
      "    -12.160423  ]\n",
      "   [ -6.526992     5.337606     4.5975156    2.282755    -3.9640458\n",
      "     -5.790892  ]\n",
      "   [ -6.0473094    6.4522133    4.423212     2.9676814   -6.354302\n",
      "     -5.978778  ]\n",
      "   [ -1.6442655   -1.3541979   -1.5947205   -3.101895   -11.002462\n",
      "     -3.6366527 ]]\n",
      "\n",
      "  [[ -6.8985233    4.0711756    4.4317036    5.133        9.072707\n",
      "     -6.7747245 ]\n",
      "   [ -4.049389     1.4091482   -1.5386783    2.7641459    2.5902662\n",
      "     -4.0136867 ]\n",
      "   [ -5.813297     2.3433473   -0.34706935   3.3857057    2.8579068\n",
      "     -6.316168  ]\n",
      "   [ -0.3411818    0.1245306    0.50826675   3.0477035    3.963468\n",
      "     -0.06130816]\n",
      "   [ -0.22494124  -0.91744673   0.13633734   0.49933708   0.61860615\n",
      "     -0.83748645]\n",
      "   [ -6.4443727    3.8669732    4.5778284    4.6446123    8.3103285\n",
      "     -6.404629  ]]\n",
      "\n",
      "  [[  1.6161397   -3.2769458    1.7488533    0.9128914   -0.7893981\n",
      "      0.947706  ]\n",
      "   [ -6.524896     0.959797    -2.4774404   -2.2015417    0.32523543\n",
      "     -5.8774953 ]\n",
      "   [ -3.653585    -0.7462094   -1.9396536   -1.485212     1.18516\n",
      "     -3.3098702 ]\n",
      "   [ -3.3113432    1.9316599   -0.39751256   0.7585439    2.0309095\n",
      "     -2.8579175 ]\n",
      "   [ -3.7562704   -0.42039806  -2.50029     -3.4232845   -1.0550084\n",
      "     -3.2843208 ]\n",
      "   [  2.5203333   -3.4583664    1.6528863    0.20075847  -0.96879214\n",
      "      1.6454371 ]]\n",
      "\n",
      "  [[ 10.207999    -2.2303858    1.9669571    1.798427     0.77476597\n",
      "      8.378769  ]\n",
      "   [-13.955634     0.30851325  -3.7349398   -7.5653377   -8.753612\n",
      "    -14.702252  ]\n",
      "   [-12.764687    -0.7902593   -4.3138323   -7.5657315   -9.601286\n",
      "    -14.23448   ]\n",
      "   [-10.681347    -0.11150143  -3.1348755   -7.045869   -10.228984\n",
      "    -12.339248  ]\n",
      "   [ -1.1423893   -0.65638995  -1.4652091   -1.5921276   -3.463386\n",
      "     -1.949174  ]\n",
      "   [  6.412017    -1.0095496    1.5431724    0.47404575  -0.6772411\n",
      "      4.897004  ]]\n",
      "\n",
      "  [[ 12.913199    -2.6469307   -0.9226576    3.6522176   10.160698\n",
      "     15.271271  ]\n",
      "   [-15.362403     4.1531754   -1.7186612   -0.8952222   -2.7088346\n",
      "    -15.511199  ]\n",
      "   [-13.55538      3.9044719   -2.516759     1.5854884    0.962125\n",
      "    -13.231382  ]\n",
      "   [-11.3605385    5.017847    -0.27478325   1.2963864    0.11557722\n",
      "    -10.511725  ]\n",
      "   [-11.652038     8.419807     1.2486496    3.690536    -5.5331264\n",
      "    -11.319855  ]\n",
      "   [ 11.963745    -0.8787226    0.3619605    4.453389     9.392935\n",
      "     14.247142  ]]\n",
      "\n",
      "  [[ -8.070269     1.7602693   -0.64440346  -0.81564397  -5.2441835\n",
      "     -7.418611  ]\n",
      "   [ -6.6380925    1.2234269    0.5143768   -3.5395942   -3.2241187\n",
      "     -6.24075   ]\n",
      "   [ -5.3416753    0.17293984  -0.8595884   -2.5809216   -2.9293919\n",
      "     -5.446527  ]\n",
      "   [ -5.386313     1.0168418    0.3420028   -2.2858331   -2.7953925\n",
      "     -5.25029   ]\n",
      "   [ -5.84336      1.6246461    0.1882815   -0.18845634  -0.4196565\n",
      "     -5.6329203 ]\n",
      "   [ -6.8899517    1.6988323   -0.6224127   -0.4996385   -4.342832\n",
      "     -6.242506  ]]\n",
      "\n",
      "  [[ 13.240363     4.66841     13.042674     5.9210863   -0.81710047\n",
      "     12.923436  ]\n",
      "   [ -7.091225     2.8033364   -1.5975537   -2.3024895   -1.2088296\n",
      "     -7.397709  ]\n",
      "   [ -3.5253422    6.1410527    5.51238      2.8467648   -1.1766208\n",
      "     -4.0684705 ]\n",
      "   [ -1.7825942    5.909616     4.947849     1.271513    -2.085788\n",
      "     -1.9155308 ]\n",
      "   [ -7.756648     7.3098855    5.8315535    4.5246334   -1.2094965\n",
      "     -7.161164  ]\n",
      "   [ 13.163366     4.3483615   12.748606     6.562309    -0.33964238\n",
      "     13.154069  ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 1)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106 ]]\n",
      "\n",
      "  [[-0.1123142 ]]\n",
      "\n",
      "  [[-0.10918517]]\n",
      "\n",
      "  [[-0.22114104]]\n",
      "\n",
      "  [[ 0.00076126]]\n",
      "\n",
      "  [[-0.00999826]]\n",
      "\n",
      "  [[-0.20863976]]\n",
      "\n",
      "  [[-0.15726982]]]], shape=(1, 8, 1, 1), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 1)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.55216384 -3.8232827  -2.910573   -3.5972157  -1.0817922\n",
      "     0.5912545 ]]\n",
      "\n",
      "  [[ 1.3908786  -1.9680318  -2.4739487  -2.0419824  -1.1162156\n",
      "     1.3957309 ]]\n",
      "\n",
      "  [[-0.8532795  -1.1699389  -0.5007857  -0.49849874 -1.1035897\n",
      "    -0.8753896 ]]\n",
      "\n",
      "  [[-0.54138374 -3.166048   -2.7694507  -3.96975    -4.5371394\n",
      "    -0.5512311 ]]\n",
      "\n",
      "  [[-1.7378478   0.02165176 -0.90774333 -1.4618493  -0.88553613\n",
      "    -1.7313937 ]]\n",
      "\n",
      "  [[ 1.6262298  -5.331017   -5.3980026  -5.357892   -0.5227707\n",
      "     1.6225317 ]]\n",
      "\n",
      "  [[ 0.3844978   3.3728192   1.4984373   2.177764    1.7664791\n",
      "     0.3842628 ]]\n",
      "\n",
      "  [[-0.3909884   1.3290287   0.7153773   0.30487654  0.01067167\n",
      "    -0.37957388]]]], shape=(1, 8, 1, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 6)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 1)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.854575  ]]\n",
      "\n",
      "  [[-0.28805032]]\n",
      "\n",
      "  [[-3.0765195 ]]\n",
      "\n",
      "  [[-3.0387642 ]]\n",
      "\n",
      "  [[-0.61142784]]\n",
      "\n",
      "  [[-1.497981  ]]\n",
      "\n",
      "  [[-2.76698   ]]\n",
      "\n",
      "  [[-1.9492282 ]]]], shape=(1, 8, 1, 1), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 1)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.53411824  3.138721    3.4550288   2.1090372   1.3961537\n",
      "    -0.47872242]]\n",
      "\n",
      "  [[-0.6982859   1.8579963   0.2598044   0.11909989  0.10357708\n",
      "    -0.68354815]]\n",
      "\n",
      "  [[-0.7371446   0.21600349 -0.9151045  -0.87319124 -0.58509016\n",
      "    -0.6977605 ]]\n",
      "\n",
      "  [[-2.016561    1.2345561  -0.22407255 -0.47709444 -0.02320303\n",
      "    -1.952554  ]]\n",
      "\n",
      "  [[-1.13986     0.06528747 -0.5116981   0.75716764  4.4378066\n",
      "    -1.1630316 ]]\n",
      "\n",
      "  [[-0.5769834   4.418479    2.9384317   2.4967303  -0.14060836\n",
      "    -0.5965953 ]]\n",
      "\n",
      "  [[-0.44670728  3.0870311   1.6550196   1.9220535  -0.1178598\n",
      "    -0.40508384]]\n",
      "\n",
      "  [[-1.7284982   3.5421734   3.979648    2.255315   -0.08031027\n",
      "    -1.7139624 ]]]], shape=(1, 8, 1, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 6)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724    5.2739615    0.44445598   0.6199022   -1.8606094\n",
      "     -5.4542537 ]\n",
      "   [ -2.6434557    0.7762684   -0.8715434   -2.1795447   -3.9540012\n",
      "     -4.6782713 ]\n",
      "   [ -6.1793337    5.196875     0.9097851   -2.007469    -3.7270458\n",
      "     -8.857345  ]\n",
      "   [ -5.1220007    6.1545386   -1.5864905   -0.9809261   -1.6544849\n",
      "     -6.897224  ]\n",
      "   [ -2.9175513    4.668971     1.8842993   -1.2460753   -3.9725554\n",
      "     -4.511021  ]\n",
      "   [ -3.92207      5.6859646   -0.5558028   -1.2563441   -1.7261751\n",
      "     -6.1812677 ]]\n",
      "\n",
      "  [[ -5.830585     6.4584546    1.6885545   -2.9425578   -2.1129007\n",
      "     -4.4590826 ]\n",
      "   [ -1.4542547    0.66857773  -0.8920885    3.2392426    0.6499656\n",
      "     -0.78637475]\n",
      "   [ -0.77001333   2.5683331   -1.5277501   -1.2103951   -3.361052\n",
      "     -0.6701153 ]\n",
      "   [ -6.770008     4.0346427    3.700929    -5.633308    -3.0441353\n",
      "     -4.6253424 ]\n",
      "   [ -3.266022    -0.2358284    5.192181    -3.326681    -1.1964418\n",
      "     -3.002487  ]\n",
      "   [ -3.8395674    5.5131884    2.4825306   -2.030505    -1.288316\n",
      "     -2.5861757 ]]\n",
      "\n",
      "  [[  0.21261294   2.6522226   -2.1467347   -1.7591757   -1.8478366\n",
      "     -1.005188  ]\n",
      "   [  3.4450672   -9.425682     2.968103     1.0088919    0.15486461\n",
      "      2.6339269 ]\n",
      "   [-10.890656    14.99533      1.3811018    2.7970884   -4.1616673\n",
      "    -12.529773  ]\n",
      "   [ -5.4948997    7.8948936    2.588478     0.5435209   -0.2743195\n",
      "     -5.1724825 ]\n",
      "   [ -3.2168324    0.6599828    1.3618138    2.5563478   -1.6389858\n",
      "     -2.9406655 ]\n",
      "   [ -1.7681361    6.479452     0.12432577  -1.3926682   -2.481772\n",
      "     -2.4043    ]]\n",
      "\n",
      "  [[ -0.62310183   2.588975     0.39473668   2.3254414    0.3566933\n",
      "     -0.95473033]\n",
      "   [ -3.2329118   -1.6349312    4.420214    -1.4593328   -0.2873762\n",
      "     -3.3657458 ]\n",
      "   [ -5.5691104    0.9040649    1.5155857   -1.662806    -1.17778\n",
      "     -5.2585945 ]\n",
      "   [ -5.513655     0.39596844   1.2438383   -1.9474496   -1.6561016\n",
      "     -4.673764  ]\n",
      "   [ -3.756813     2.3833623    7.4268293    1.2679268   -2.3513136\n",
      "     -4.9471974 ]\n",
      "   [ -1.728882     1.0250845    1.9032011   -0.48647845  -1.0321277\n",
      "     -2.5369413 ]]\n",
      "\n",
      "  [[ -3.365542     1.8547945    2.3893828   -0.2378472   -1.7009665\n",
      "     -2.4370363 ]\n",
      "   [ -1.2823843    4.7429194   -4.1070457   -0.47443444  -1.9375597\n",
      "     -1.6068171 ]\n",
      "   [ -5.0705113    7.2453456    0.22619136   3.822259    -2.3195603\n",
      "     -3.822914  ]\n",
      "   [ -3.6519911    2.915474     0.76882476  -0.10871613  -4.1921997\n",
      "     -2.662963  ]\n",
      "   [  2.4449081   -1.4922832    0.5579315   -0.2331474    1.4929048\n",
      "      1.6919394 ]\n",
      "   [  0.68565965  -1.3965738    2.239955     1.8028462    2.3655758\n",
      "      1.6723614 ]]\n",
      "\n",
      "  [[ -2.1686854    1.8683971   -0.46407908  -0.78368795  -2.3975863\n",
      "     -2.3532345 ]\n",
      "   [ -6.185767     2.228677     0.71968794  -2.434057    -5.3960166\n",
      "     -4.845154  ]\n",
      "   [  0.25900394   2.3122008    0.9818626    2.2493217   -0.2135596\n",
      "     -1.5250982 ]\n",
      "   [ -3.6363199    5.520364     2.119328    -1.6462586   -3.9594364\n",
      "     -5.000841  ]\n",
      "   [ -3.2407305    4.0637727   -3.65974     -1.2874398   -5.055235\n",
      "     -1.0524285 ]\n",
      "   [ -1.6338614    2.8127375   -0.63038987  -1.1972872   -2.4497988\n",
      "     -2.5156808 ]]\n",
      "\n",
      "  [[ -0.3049526   -0.4231821    2.3549898    1.2362137   -3.3428524\n",
      "     -2.4705725 ]\n",
      "   [ -8.393576     1.2230886   -0.47221878  -0.3785542   -9.154858\n",
      "     -9.623112  ]\n",
      "   [ -7.879456     1.0038036    6.269445     3.383196    -5.1612587\n",
      "     -9.652613  ]\n",
      "   [ -6.5884266    1.8914772    0.58854324   0.7356621   -3.4622736\n",
      "     -8.281077  ]\n",
      "   [  3.2697265   -0.22400014  -1.8101139    0.84822214   0.9626521\n",
      "      4.273874  ]\n",
      "   [ -1.6477983   -0.3436993    2.7197764    0.96644586  -0.9888108\n",
      "     -3.2478032 ]]\n",
      "\n",
      "  [[ -2.8292158    2.69152     -5.9879627    0.05573386  -0.583922\n",
      "     -0.6253632 ]\n",
      "   [ -5.2028084    1.2464813   -0.28601608   0.14257993  -7.337432\n",
      "     -6.4034657 ]\n",
      "   [ -8.92803      6.5982924   -2.5037014    1.7294589   -8.505811\n",
      "     -8.242324  ]\n",
      "   [ -2.8773344    3.950198     0.63110507   2.2820823   -5.6118255\n",
      "     -3.8134937 ]\n",
      "   [ -1.743894     3.723778     3.8605156    5.6352224   -8.514911\n",
      "     -4.5736675 ]\n",
      "   [ -1.8900229    3.1358528   -4.051401    -0.7488651   -2.9355698\n",
      "     -1.1315701 ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.921542     1.008876     0.3188283   -0.09219998  -3.1753793\n",
      "     -6.121585  ]\n",
      "   [ -7.19034      2.2363086    1.0159979    2.2710261    1.5986332\n",
      "     -6.611934  ]\n",
      "   [ -7.901739     3.423445     0.28121415   1.4161541    0.7545412\n",
      "     -6.8629513 ]\n",
      "   [ -4.4973965    2.089329    -1.5849537    0.9947852    1.7196369\n",
      "     -4.0510983 ]\n",
      "   [ -3.9672196   -0.16474341  -4.148401    -1.2713867    0.75565946\n",
      "     -4.159244  ]\n",
      "   [ -5.64101      0.69921917  -0.1817732   -0.8431055   -2.560344\n",
      "     -4.9824452 ]]\n",
      "\n",
      "  [[ -1.7980527   -1.5812842   -1.9451343   -3.2220113  -10.310742\n",
      "     -3.824608  ]\n",
      "   [ -3.479981     3.2808747    2.645604     1.2420398   -2.2346535\n",
      "     -2.8920004 ]\n",
      "   [-12.847355     4.693563     1.7069713    0.14368261  -8.192775\n",
      "    -12.160423  ]\n",
      "   [ -6.526992     5.337606     4.5975156    2.282755    -3.9640458\n",
      "     -5.790892  ]\n",
      "   [ -6.0473094    6.4522133    4.423212     2.9676814   -6.354302\n",
      "     -5.978778  ]\n",
      "   [ -1.6442655   -1.3541979   -1.5947205   -3.101895   -11.002462\n",
      "     -3.6366527 ]]\n",
      "\n",
      "  [[ -6.8985233    4.0711756    4.4317036    5.133        9.072707\n",
      "     -6.7747245 ]\n",
      "   [ -4.049389     1.4091482   -1.5386783    2.7641459    2.5902662\n",
      "     -4.0136867 ]\n",
      "   [ -5.813297     2.3433473   -0.34706935   3.3857057    2.8579068\n",
      "     -6.316168  ]\n",
      "   [ -0.3411818    0.1245306    0.50826675   3.0477035    3.963468\n",
      "     -0.06130816]\n",
      "   [ -0.22494124  -0.91744673   0.13633734   0.49933708   0.61860615\n",
      "     -0.83748645]\n",
      "   [ -6.4443727    3.8669732    4.5778284    4.6446123    8.3103285\n",
      "     -6.404629  ]]\n",
      "\n",
      "  [[  1.6161397   -3.2769458    1.7488533    0.9128914   -0.7893981\n",
      "      0.947706  ]\n",
      "   [ -6.524896     0.959797    -2.4774404   -2.2015417    0.32523543\n",
      "     -5.8774953 ]\n",
      "   [ -3.653585    -0.7462094   -1.9396536   -1.485212     1.18516\n",
      "     -3.3098702 ]\n",
      "   [ -3.3113432    1.9316599   -0.39751256   0.7585439    2.0309095\n",
      "     -2.8579175 ]\n",
      "   [ -3.7562704   -0.42039806  -2.50029     -3.4232845   -1.0550084\n",
      "     -3.2843208 ]\n",
      "   [  2.5203333   -3.4583664    1.6528863    0.20075847  -0.96879214\n",
      "      1.6454371 ]]\n",
      "\n",
      "  [[ 10.207999    -2.2303858    1.9669571    1.798427     0.77476597\n",
      "      8.378769  ]\n",
      "   [-13.955634     0.30851325  -3.7349398   -7.5653377   -8.753612\n",
      "    -14.702252  ]\n",
      "   [-12.764687    -0.7902593   -4.3138323   -7.5657315   -9.601286\n",
      "    -14.23448   ]\n",
      "   [-10.681347    -0.11150143  -3.1348755   -7.045869   -10.228984\n",
      "    -12.339248  ]\n",
      "   [ -1.1423893   -0.65638995  -1.4652091   -1.5921276   -3.463386\n",
      "     -1.949174  ]\n",
      "   [  6.412017    -1.0095496    1.5431724    0.47404575  -0.6772411\n",
      "      4.897004  ]]\n",
      "\n",
      "  [[ 12.913199    -2.6469307   -0.9226576    3.6522176   10.160698\n",
      "     15.271271  ]\n",
      "   [-15.362403     4.1531754   -1.7186612   -0.8952222   -2.7088346\n",
      "    -15.511199  ]\n",
      "   [-13.55538      3.9044719   -2.516759     1.5854884    0.962125\n",
      "    -13.231382  ]\n",
      "   [-11.3605385    5.017847    -0.27478325   1.2963864    0.11557722\n",
      "    -10.511725  ]\n",
      "   [-11.652038     8.419807     1.2486496    3.690536    -5.5331264\n",
      "    -11.319855  ]\n",
      "   [ 11.963745    -0.8787226    0.3619605    4.453389     9.392935\n",
      "     14.247142  ]]\n",
      "\n",
      "  [[ -8.070269     1.7602693   -0.64440346  -0.81564397  -5.2441835\n",
      "     -7.418611  ]\n",
      "   [ -6.6380925    1.2234269    0.5143768   -3.5395942   -3.2241187\n",
      "     -6.24075   ]\n",
      "   [ -5.3416753    0.17293984  -0.8595884   -2.5809216   -2.9293919\n",
      "     -5.446527  ]\n",
      "   [ -5.386313     1.0168418    0.3420028   -2.2858331   -2.7953925\n",
      "     -5.25029   ]\n",
      "   [ -5.84336      1.6246461    0.1882815   -0.18845634  -0.4196565\n",
      "     -5.6329203 ]\n",
      "   [ -6.8899517    1.6988323   -0.6224127   -0.4996385   -4.342832\n",
      "     -6.242506  ]]\n",
      "\n",
      "  [[ 13.240363     4.66841     13.042674     5.9210863   -0.81710047\n",
      "     12.923436  ]\n",
      "   [ -7.091225     2.8033364   -1.5975537   -2.3024895   -1.2088296\n",
      "     -7.397709  ]\n",
      "   [ -3.5253422    6.1410527    5.51238      2.8467648   -1.1766208\n",
      "     -4.0684705 ]\n",
      "   [ -1.7825942    5.909616     4.947849     1.271513    -2.085788\n",
      "     -1.9155308 ]\n",
      "   [ -7.756648     7.3098855    5.8315535    4.5246334   -1.2094965\n",
      "     -7.161164  ]\n",
      "   [ 13.163366     4.3483615   12.748606     6.562309    -0.33964238\n",
      "     13.154069  ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 2)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.07309047]\n",
      "   [-1.2853749   3.06507   ]]\n",
      "\n",
      "  [[-0.1123142   0.04219194]\n",
      "   [-0.02044308 -1.6470729 ]]\n",
      "\n",
      "  [[-0.10918517 -0.00974669]\n",
      "   [-0.2557946  -1.5501258 ]]\n",
      "\n",
      "  [[-0.22114104  0.3014268 ]\n",
      "   [-0.12756507 -2.1900728 ]]\n",
      "\n",
      "  [[ 0.00076126 -0.22633906]\n",
      "   [-0.30625445 -1.0766943 ]]\n",
      "\n",
      "  [[-0.00999826  0.10504516]\n",
      "   [ 0.46795642 -2.4885566 ]]\n",
      "\n",
      "  [[-0.20863976  0.3057065 ]\n",
      "   [ 0.4478607  -3.5688307 ]]\n",
      "\n",
      "  [[-0.15726982  0.07697631]\n",
      "   [ 0.03995608 -3.892801  ]]]], shape=(1, 8, 2, 2), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 2)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.55216384 -3.8232827  -2.910573   -3.5972157  -1.0817922\n",
      "     0.5912545 ]\n",
      "   [ 0.6380123   1.8036875   1.6386297   2.195628    2.0554762\n",
      "     0.64612067]]\n",
      "\n",
      "  [[ 1.3908786  -1.9680318  -2.4739487  -2.0419824  -1.1162156\n",
      "     1.3957309 ]\n",
      "   [ 0.02316882  1.4757677   1.3413774   0.54396117 -0.06397482\n",
      "     0.01687552]]\n",
      "\n",
      "  [[-0.8532795  -1.1699389  -0.5007857  -0.49849874 -1.1035897\n",
      "    -0.8753896 ]\n",
      "   [ 0.47264126 -5.680002   -4.517802   -4.377047   -2.9761038\n",
      "     0.4754463 ]]\n",
      "\n",
      "  [[-0.54138374 -3.166048   -2.7694507  -3.96975    -4.5371394\n",
      "    -0.5512311 ]\n",
      "   [ 0.5716766  -8.624254   -7.4991584  -7.3359303  -6.564319\n",
      "     0.57178384]]\n",
      "\n",
      "  [[-1.7378478   0.02165176 -0.90774333 -1.4618493  -0.88553613\n",
      "    -1.7313937 ]\n",
      "   [-0.27279285 -0.59817743 -1.842621   -1.2421002  -0.9153028\n",
      "    -0.28948683]]\n",
      "\n",
      "  [[ 1.6262298  -5.331017   -5.3980026  -5.357892   -0.5227707\n",
      "     1.6225317 ]\n",
      "   [ 1.4594786  -4.6213126  -5.345402   -4.8237014   0.0415122\n",
      "     1.4560635 ]]\n",
      "\n",
      "  [[ 0.3844978   3.3728192   1.4984373   2.177764    1.7664791\n",
      "     0.3842628 ]\n",
      "   [ 1.3693358  -5.0935154  -3.921827   -3.6555476  -1.8274075\n",
      "     1.3558773 ]]\n",
      "\n",
      "  [[-0.3909884   1.3290287   0.7153773   0.30487654  0.01067167\n",
      "    -0.37957388]\n",
      "   [ 0.91534704 -2.004378   -1.3137145  -1.8292936   0.04522658\n",
      "     0.95372003]]]], shape=(1, 8, 2, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 6)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 2)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.854575   -0.76395994]\n",
      "   [ 2.769262   -2.4458904 ]]\n",
      "\n",
      "  [[-0.28805032 -0.33996823]\n",
      "   [-0.6685229  -2.6304247 ]]\n",
      "\n",
      "  [[-3.0765195   1.7763902 ]\n",
      "   [-4.732454    2.829565  ]]\n",
      "\n",
      "  [[-3.0387642   1.292204  ]\n",
      "   [-5.8640046   2.6051586 ]]\n",
      "\n",
      "  [[-0.61142784 -0.4549538 ]\n",
      "   [ 3.101384   -1.9635861 ]]\n",
      "\n",
      "  [[-1.497981    1.4031547 ]\n",
      "   [ 1.1607789   1.7605858 ]]\n",
      "\n",
      "  [[-2.76698    -1.0661263 ]\n",
      "   [ 0.196701    0.70394504]]\n",
      "\n",
      "  [[-1.9492282   1.2290938 ]\n",
      "   [-0.40289074 -0.30640975]]]], shape=(1, 8, 2, 2), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 2)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.53411824  3.138721    3.4550288   2.1090372   1.3961537\n",
      "    -0.47872242]\n",
      "   [ 0.16838217 -0.68966126 -0.65448135 -0.50101525 -0.9362338\n",
      "     0.17256774]]\n",
      "\n",
      "  [[-0.6982859   1.8579963   0.2598044   0.11909989  0.10357708\n",
      "    -0.68354815]\n",
      "   [-0.70370317 -1.1944655  -1.8300511  -1.625776   -1.3867573\n",
      "    -0.7268194 ]]\n",
      "\n",
      "  [[-0.7371446   0.21600349 -0.9151045  -0.87319124 -0.58509016\n",
      "    -0.6977605 ]\n",
      "   [ 0.24199572  0.91136837  0.80540216  0.90500486  0.531705\n",
      "     0.21405065]]\n",
      "\n",
      "  [[-2.016561    1.2345561  -0.22407255 -0.47709444 -0.02320303\n",
      "    -1.952554  ]\n",
      "   [ 1.6252061  -5.532724   -4.697985   -4.2196207  -1.0165944\n",
      "     1.6203778 ]]\n",
      "\n",
      "  [[-1.13986     0.06528747 -0.5116981   0.75716764  4.4378066\n",
      "    -1.1630316 ]\n",
      "   [ 0.23661456 -2.276422   -1.5742096  -1.1921136  -1.0701761\n",
      "     0.24994968]]\n",
      "\n",
      "  [[-0.5769834   4.418479    2.9384317   2.4967303  -0.14060836\n",
      "    -0.5965953 ]\n",
      "   [ 0.4858764  -4.1729226  -2.5100694  -3.0045586  -0.95974565\n",
      "     0.49929497]]\n",
      "\n",
      "  [[-0.44670728  3.0870311   1.6550196   1.9220535  -0.1178598\n",
      "    -0.40508384]\n",
      "   [ 0.39069507 -4.4789886  -3.7056172  -3.9325013  -0.45467767\n",
      "     0.3951269 ]]\n",
      "\n",
      "  [[-1.7284982   3.5421734   3.979648    2.255315   -0.08031027\n",
      "    -1.7139624 ]\n",
      "   [ 1.8577509  -3.874339   -4.0099216  -2.2661076  -0.20281102\n",
      "     1.8486136 ]]]], shape=(1, 8, 2, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 6)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724    5.2739615    0.44445598   0.6199022   -1.8606094\n",
      "     -5.4542537 ]\n",
      "   [ -2.6434557    0.7762684   -0.8715434   -2.1795447   -3.9540012\n",
      "     -4.6782713 ]\n",
      "   [ -6.1793337    5.196875     0.9097851   -2.007469    -3.7270458\n",
      "     -8.857345  ]\n",
      "   [ -5.1220007    6.1545386   -1.5864905   -0.9809261   -1.6544849\n",
      "     -6.897224  ]\n",
      "   [ -2.9175513    4.668971     1.8842993   -1.2460753   -3.9725554\n",
      "     -4.511021  ]\n",
      "   [ -3.92207      5.6859646   -0.5558028   -1.2563441   -1.7261751\n",
      "     -6.1812677 ]]\n",
      "\n",
      "  [[ -5.830585     6.4584546    1.6885545   -2.9425578   -2.1129007\n",
      "     -4.4590826 ]\n",
      "   [ -1.4542547    0.66857773  -0.8920885    3.2392426    0.6499656\n",
      "     -0.78637475]\n",
      "   [ -0.77001333   2.5683331   -1.5277501   -1.2103951   -3.361052\n",
      "     -0.6701153 ]\n",
      "   [ -6.770008     4.0346427    3.700929    -5.633308    -3.0441353\n",
      "     -4.6253424 ]\n",
      "   [ -3.266022    -0.2358284    5.192181    -3.326681    -1.1964418\n",
      "     -3.002487  ]\n",
      "   [ -3.8395674    5.5131884    2.4825306   -2.030505    -1.288316\n",
      "     -2.5861757 ]]\n",
      "\n",
      "  [[  0.21261294   2.6522226   -2.1467347   -1.7591757   -1.8478366\n",
      "     -1.005188  ]\n",
      "   [  3.4450672   -9.425682     2.968103     1.0088919    0.15486461\n",
      "      2.6339269 ]\n",
      "   [-10.890656    14.99533      1.3811018    2.7970884   -4.1616673\n",
      "    -12.529773  ]\n",
      "   [ -5.4948997    7.8948936    2.588478     0.5435209   -0.2743195\n",
      "     -5.1724825 ]\n",
      "   [ -3.2168324    0.6599828    1.3618138    2.5563478   -1.6389858\n",
      "     -2.9406655 ]\n",
      "   [ -1.7681361    6.479452     0.12432577  -1.3926682   -2.481772\n",
      "     -2.4043    ]]\n",
      "\n",
      "  [[ -0.62310183   2.588975     0.39473668   2.3254414    0.3566933\n",
      "     -0.95473033]\n",
      "   [ -3.2329118   -1.6349312    4.420214    -1.4593328   -0.2873762\n",
      "     -3.3657458 ]\n",
      "   [ -5.5691104    0.9040649    1.5155857   -1.662806    -1.17778\n",
      "     -5.2585945 ]\n",
      "   [ -5.513655     0.39596844   1.2438383   -1.9474496   -1.6561016\n",
      "     -4.673764  ]\n",
      "   [ -3.756813     2.3833623    7.4268293    1.2679268   -2.3513136\n",
      "     -4.9471974 ]\n",
      "   [ -1.728882     1.0250845    1.9032011   -0.48647845  -1.0321277\n",
      "     -2.5369413 ]]\n",
      "\n",
      "  [[ -3.365542     1.8547945    2.3893828   -0.2378472   -1.7009665\n",
      "     -2.4370363 ]\n",
      "   [ -1.2823843    4.7429194   -4.1070457   -0.47443444  -1.9375597\n",
      "     -1.6068171 ]\n",
      "   [ -5.0705113    7.2453456    0.22619136   3.822259    -2.3195603\n",
      "     -3.822914  ]\n",
      "   [ -3.6519911    2.915474     0.76882476  -0.10871613  -4.1921997\n",
      "     -2.662963  ]\n",
      "   [  2.4449081   -1.4922832    0.5579315   -0.2331474    1.4929048\n",
      "      1.6919394 ]\n",
      "   [  0.68565965  -1.3965738    2.239955     1.8028462    2.3655758\n",
      "      1.6723614 ]]\n",
      "\n",
      "  [[ -2.1686854    1.8683971   -0.46407908  -0.78368795  -2.3975863\n",
      "     -2.3532345 ]\n",
      "   [ -6.185767     2.228677     0.71968794  -2.434057    -5.3960166\n",
      "     -4.845154  ]\n",
      "   [  0.25900394   2.3122008    0.9818626    2.2493217   -0.2135596\n",
      "     -1.5250982 ]\n",
      "   [ -3.6363199    5.520364     2.119328    -1.6462586   -3.9594364\n",
      "     -5.000841  ]\n",
      "   [ -3.2407305    4.0637727   -3.65974     -1.2874398   -5.055235\n",
      "     -1.0524285 ]\n",
      "   [ -1.6338614    2.8127375   -0.63038987  -1.1972872   -2.4497988\n",
      "     -2.5156808 ]]\n",
      "\n",
      "  [[ -0.3049526   -0.4231821    2.3549898    1.2362137   -3.3428524\n",
      "     -2.4705725 ]\n",
      "   [ -8.393576     1.2230886   -0.47221878  -0.3785542   -9.154858\n",
      "     -9.623112  ]\n",
      "   [ -7.879456     1.0038036    6.269445     3.383196    -5.1612587\n",
      "     -9.652613  ]\n",
      "   [ -6.5884266    1.8914772    0.58854324   0.7356621   -3.4622736\n",
      "     -8.281077  ]\n",
      "   [  3.2697265   -0.22400014  -1.8101139    0.84822214   0.9626521\n",
      "      4.273874  ]\n",
      "   [ -1.6477983   -0.3436993    2.7197764    0.96644586  -0.9888108\n",
      "     -3.2478032 ]]\n",
      "\n",
      "  [[ -2.8292158    2.69152     -5.9879627    0.05573386  -0.583922\n",
      "     -0.6253632 ]\n",
      "   [ -5.2028084    1.2464813   -0.28601608   0.14257993  -7.337432\n",
      "     -6.4034657 ]\n",
      "   [ -8.92803      6.5982924   -2.5037014    1.7294589   -8.505811\n",
      "     -8.242324  ]\n",
      "   [ -2.8773344    3.950198     0.63110507   2.2820823   -5.6118255\n",
      "     -3.8134937 ]\n",
      "   [ -1.743894     3.723778     3.8605156    5.6352224   -8.514911\n",
      "     -4.5736675 ]\n",
      "   [ -1.8900229    3.1358528   -4.051401    -0.7488651   -2.9355698\n",
      "     -1.1315701 ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.921542     1.008876     0.3188283   -0.09219998  -3.1753793\n",
      "     -6.121585  ]\n",
      "   [ -7.19034      2.2363086    1.0159979    2.2710261    1.5986332\n",
      "     -6.611934  ]\n",
      "   [ -7.901739     3.423445     0.28121415   1.4161541    0.7545412\n",
      "     -6.8629513 ]\n",
      "   [ -4.4973965    2.089329    -1.5849537    0.9947852    1.7196369\n",
      "     -4.0510983 ]\n",
      "   [ -3.9672196   -0.16474341  -4.148401    -1.2713867    0.75565946\n",
      "     -4.159244  ]\n",
      "   [ -5.64101      0.69921917  -0.1817732   -0.8431055   -2.560344\n",
      "     -4.9824452 ]]\n",
      "\n",
      "  [[ -1.7980527   -1.5812842   -1.9451343   -3.2220113  -10.310742\n",
      "     -3.824608  ]\n",
      "   [ -3.479981     3.2808747    2.645604     1.2420398   -2.2346535\n",
      "     -2.8920004 ]\n",
      "   [-12.847355     4.693563     1.7069713    0.14368261  -8.192775\n",
      "    -12.160423  ]\n",
      "   [ -6.526992     5.337606     4.5975156    2.282755    -3.9640458\n",
      "     -5.790892  ]\n",
      "   [ -6.0473094    6.4522133    4.423212     2.9676814   -6.354302\n",
      "     -5.978778  ]\n",
      "   [ -1.6442655   -1.3541979   -1.5947205   -3.101895   -11.002462\n",
      "     -3.6366527 ]]\n",
      "\n",
      "  [[ -6.8985233    4.0711756    4.4317036    5.133        9.072707\n",
      "     -6.7747245 ]\n",
      "   [ -4.049389     1.4091482   -1.5386783    2.7641459    2.5902662\n",
      "     -4.0136867 ]\n",
      "   [ -5.813297     2.3433473   -0.34706935   3.3857057    2.8579068\n",
      "     -6.316168  ]\n",
      "   [ -0.3411818    0.1245306    0.50826675   3.0477035    3.963468\n",
      "     -0.06130816]\n",
      "   [ -0.22494124  -0.91744673   0.13633734   0.49933708   0.61860615\n",
      "     -0.83748645]\n",
      "   [ -6.4443727    3.8669732    4.5778284    4.6446123    8.3103285\n",
      "     -6.404629  ]]\n",
      "\n",
      "  [[  1.6161397   -3.2769458    1.7488533    0.9128914   -0.7893981\n",
      "      0.947706  ]\n",
      "   [ -6.524896     0.959797    -2.4774404   -2.2015417    0.32523543\n",
      "     -5.8774953 ]\n",
      "   [ -3.653585    -0.7462094   -1.9396536   -1.485212     1.18516\n",
      "     -3.3098702 ]\n",
      "   [ -3.3113432    1.9316599   -0.39751256   0.7585439    2.0309095\n",
      "     -2.8579175 ]\n",
      "   [ -3.7562704   -0.42039806  -2.50029     -3.4232845   -1.0550084\n",
      "     -3.2843208 ]\n",
      "   [  2.5203333   -3.4583664    1.6528863    0.20075847  -0.96879214\n",
      "      1.6454371 ]]\n",
      "\n",
      "  [[ 10.207999    -2.2303858    1.9669571    1.798427     0.77476597\n",
      "      8.378769  ]\n",
      "   [-13.955634     0.30851325  -3.7349398   -7.5653377   -8.753612\n",
      "    -14.702252  ]\n",
      "   [-12.764687    -0.7902593   -4.3138323   -7.5657315   -9.601286\n",
      "    -14.23448   ]\n",
      "   [-10.681347    -0.11150143  -3.1348755   -7.045869   -10.228984\n",
      "    -12.339248  ]\n",
      "   [ -1.1423893   -0.65638995  -1.4652091   -1.5921276   -3.463386\n",
      "     -1.949174  ]\n",
      "   [  6.412017    -1.0095496    1.5431724    0.47404575  -0.6772411\n",
      "      4.897004  ]]\n",
      "\n",
      "  [[ 12.913199    -2.6469307   -0.9226576    3.6522176   10.160698\n",
      "     15.271271  ]\n",
      "   [-15.362403     4.1531754   -1.7186612   -0.8952222   -2.7088346\n",
      "    -15.511199  ]\n",
      "   [-13.55538      3.9044719   -2.516759     1.5854884    0.962125\n",
      "    -13.231382  ]\n",
      "   [-11.3605385    5.017847    -0.27478325   1.2963864    0.11557722\n",
      "    -10.511725  ]\n",
      "   [-11.652038     8.419807     1.2486496    3.690536    -5.5331264\n",
      "    -11.319855  ]\n",
      "   [ 11.963745    -0.8787226    0.3619605    4.453389     9.392935\n",
      "     14.247142  ]]\n",
      "\n",
      "  [[ -8.070269     1.7602693   -0.64440346  -0.81564397  -5.2441835\n",
      "     -7.418611  ]\n",
      "   [ -6.6380925    1.2234269    0.5143768   -3.5395942   -3.2241187\n",
      "     -6.24075   ]\n",
      "   [ -5.3416753    0.17293984  -0.8595884   -2.5809216   -2.9293919\n",
      "     -5.446527  ]\n",
      "   [ -5.386313     1.0168418    0.3420028   -2.2858331   -2.7953925\n",
      "     -5.25029   ]\n",
      "   [ -5.84336      1.6246461    0.1882815   -0.18845634  -0.4196565\n",
      "     -5.6329203 ]\n",
      "   [ -6.8899517    1.6988323   -0.6224127   -0.4996385   -4.342832\n",
      "     -6.242506  ]]\n",
      "\n",
      "  [[ 13.240363     4.66841     13.042674     5.9210863   -0.81710047\n",
      "     12.923436  ]\n",
      "   [ -7.091225     2.8033364   -1.5975537   -2.3024895   -1.2088296\n",
      "     -7.397709  ]\n",
      "   [ -3.5253422    6.1410527    5.51238      2.8467648   -1.1766208\n",
      "     -4.0684705 ]\n",
      "   [ -1.7825942    5.909616     4.947849     1.271513    -2.085788\n",
      "     -1.9155308 ]\n",
      "   [ -7.756648     7.3098855    5.8315535    4.5246334   -1.2094965\n",
      "     -7.161164  ]\n",
      "   [ 13.163366     4.3483615   12.748606     6.562309    -0.33964238\n",
      "     13.154069  ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 3)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.07309047  0.47737727]\n",
      "   [-1.2853749   3.06507    -0.8552955 ]\n",
      "   [ 0.58923376  0.9813636   0.905565  ]]\n",
      "\n",
      "  [[-0.1123142   0.04219194  0.25517872]\n",
      "   [-0.02044308 -1.6470729   0.7732946 ]\n",
      "   [ 0.47959617  2.2830687  -2.5592422 ]]\n",
      "\n",
      "  [[-0.10918517 -0.00974669  0.32006803]\n",
      "   [-0.2557946  -1.5501258  -0.98448783]\n",
      "   [ 0.26977882 -1.2627326  -5.8444214 ]]\n",
      "\n",
      "  [[-0.22114104  0.3014268   0.42661816]\n",
      "   [-0.12756507 -2.1900728   1.8131207 ]\n",
      "   [ 0.54721624  2.5360978  -3.0377705 ]]\n",
      "\n",
      "  [[ 0.00076126 -0.22633906 -0.29323676]\n",
      "   [-0.30625445 -1.0766943  -1.4381294 ]\n",
      "   [ 0.17989263 -1.2728978  -1.1890442 ]]\n",
      "\n",
      "  [[-0.00999826  0.10504516  0.17285524]\n",
      "   [ 0.46795642 -2.4885566   1.4032799 ]\n",
      "   [ 0.26839072  0.88721114 -3.3536348 ]]\n",
      "\n",
      "  [[-0.20863976  0.3057065   0.37231094]\n",
      "   [ 0.4478607  -3.5688307  -1.2092146 ]\n",
      "   [ 1.2270066  -3.1708426  -7.3976893 ]]\n",
      "\n",
      "  [[-0.15726982  0.07697631 -0.3421147 ]\n",
      "   [ 0.03995608 -3.892801   -1.946208  ]\n",
      "   [ 0.36951503  3.2891269  -3.5526025 ]]]], shape=(1, 8, 3, 3), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 3)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.55216384  -3.8232827   -2.910573    -3.5972157   -1.0817922\n",
      "      0.5912545 ]\n",
      "   [  0.6380123    1.8036875    1.6386297    2.195628     2.0554762\n",
      "      0.64612067]\n",
      "   [  0.56770134  -2.2716873   -2.5307856   -3.5080245   -2.9300187\n",
      "      0.5614793 ]]\n",
      "\n",
      "  [[  1.3908786   -1.9680318   -2.4739487   -2.0419824   -1.1162156\n",
      "      1.3957309 ]\n",
      "   [  0.02316882   1.4757677    1.3413774    0.54396117  -0.06397482\n",
      "      0.01687552]\n",
      "   [  0.33217046  -2.1242876   -1.7803466   -2.0098727   -1.3619229\n",
      "      0.3444393 ]]\n",
      "\n",
      "  [[ -0.8532795   -1.1699389   -0.5007857   -0.49849874  -1.1035897\n",
      "     -0.8753896 ]\n",
      "   [  0.47264126  -5.680002    -4.517802    -4.377047    -2.9761038\n",
      "      0.4754463 ]\n",
      "   [  1.8673078  -13.079067   -11.496453   -10.314076    -6.891684\n",
      "      1.8901012 ]]\n",
      "\n",
      "  [[ -0.54138374  -3.166048    -2.7694507   -3.96975     -4.5371394\n",
      "     -0.5512311 ]\n",
      "   [  0.5716766   -8.624254    -7.4991584   -7.3359303   -6.564319\n",
      "      0.57178384]\n",
      "   [  1.3138024   -5.056788    -4.579848    -3.984582    -3.0800314\n",
      "      1.33183   ]]\n",
      "\n",
      "  [[ -1.7378478    0.02165176  -0.90774333  -1.4618493   -0.88553613\n",
      "     -1.7313937 ]\n",
      "   [ -0.27279285  -0.59817743  -1.842621    -1.2421002   -0.9153028\n",
      "     -0.28948683]\n",
      "   [  2.572501    -1.9725133   -1.8943148   -2.6406763   -1.6912937\n",
      "      2.630526  ]]\n",
      "\n",
      "  [[  1.6262298   -5.331017    -5.3980026   -5.357892    -0.5227707\n",
      "      1.6225317 ]\n",
      "   [  1.4594786   -4.6213126   -5.345402    -4.8237014    0.0415122\n",
      "      1.4560635 ]\n",
      "   [  1.6029699   -5.7235622   -5.4344754   -5.700281    -3.458166\n",
      "      1.5927473 ]]\n",
      "\n",
      "  [[  0.3844978    3.3728192    1.4984373    2.177764     1.7664791\n",
      "      0.3842628 ]\n",
      "   [  1.3693358   -5.0935154   -3.921827    -3.6555476   -1.8274075\n",
      "      1.3558773 ]\n",
      "   [  1.1213593   -4.7008147   -3.0253868   -4.153023    -3.3693302\n",
      "      1.1555594 ]]\n",
      "\n",
      "  [[ -0.3909884    1.3290287    0.7153773    0.30487654   0.01067167\n",
      "     -0.37957388]\n",
      "   [  0.91534704  -2.004378    -1.3137145   -1.8292936    0.04522658\n",
      "      0.95372003]\n",
      "   [  0.8450295   -6.3343554   -4.8446584   -4.540325    -3.0086432\n",
      "      0.8311861 ]]]], shape=(1, 8, 3, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 6)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 3)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.854575   -0.76395994  1.379787  ]\n",
      "   [ 2.769262   -2.4458904  -0.38577634]\n",
      "   [ 4.645977    0.5011021  -4.5003643 ]]\n",
      "\n",
      "  [[-0.28805032 -0.33996823  0.56497   ]\n",
      "   [-0.6685229  -2.6304247  -2.186717  ]\n",
      "   [ 1.9647298   0.46823615 -7.4626803 ]]\n",
      "\n",
      "  [[-3.0765195   1.7763902   1.9594084 ]\n",
      "   [-4.732454    2.829565    3.985526  ]\n",
      "   [ 0.04412972  1.7056273  -0.07778924]]\n",
      "\n",
      "  [[-3.0387642   1.292204    3.5883346 ]\n",
      "   [-5.8640046   2.6051586   9.00416   ]\n",
      "   [-2.221647   -2.3018663   0.05621783]]\n",
      "\n",
      "  [[-0.61142784 -0.4549538   0.9963378 ]\n",
      "   [ 3.101384   -1.9635861  -1.9723365 ]\n",
      "   [ 3.8482957   5.668536   -3.3393283 ]]\n",
      "\n",
      "  [[-1.497981    1.4031547   0.91072094]\n",
      "   [ 1.1607789   1.7605858   0.20715673]\n",
      "   [ 1.3948244   1.7155915   1.5817758 ]]\n",
      "\n",
      "  [[-2.76698    -1.0661263   0.08043555]\n",
      "   [ 0.196701    0.70394504  0.4231429 ]\n",
      "   [ 2.5972507   5.5233197  -0.16624556]]\n",
      "\n",
      "  [[-1.9492282   1.2290938   1.8699198 ]\n",
      "   [-0.40289074 -0.30640975  0.74108803]\n",
      "   [ 0.49884918 -0.6150069  -2.8845108 ]]]], shape=(1, 8, 3, 3), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 3)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.53411824  3.138721    3.4550288   2.1090372   1.3961537\n",
      "    -0.47872242]\n",
      "   [ 0.16838217 -0.68966126 -0.65448135 -0.50101525 -0.9362338\n",
      "     0.17256774]\n",
      "   [ 0.18245015 -3.0500743  -2.9301724  -2.5275035  -1.6815708\n",
      "     0.16176914]]\n",
      "\n",
      "  [[-0.6982859   1.8579963   0.2598044   0.11909989  0.10357708\n",
      "    -0.68354815]\n",
      "   [-0.70370317 -1.1944655  -1.8300511  -1.625776   -1.3867573\n",
      "    -0.7268194 ]\n",
      "   [ 0.8023996  -5.720824   -3.8634129  -3.597791   -1.2961257\n",
      "     0.79261404]]\n",
      "\n",
      "  [[-0.7371446   0.21600349 -0.9151045  -0.87319124 -0.58509016\n",
      "    -0.6977605 ]\n",
      "   [ 0.24199572  0.91136837  0.80540216  0.90500486  0.531705\n",
      "     0.21405065]\n",
      "   [ 0.428904    3.0467944   3.654848    3.5137124   2.1350443\n",
      "     0.39721748]]\n",
      "\n",
      "  [[-2.016561    1.2345561  -0.22407255 -0.47709444 -0.02320303\n",
      "    -1.952554  ]\n",
      "   [ 1.6252061  -5.532724   -4.697985   -4.2196207  -1.0165944\n",
      "     1.6203778 ]\n",
      "   [ 2.4023926  -3.3139603  -1.6055428  -0.78964245  0.77323705\n",
      "     2.3298233 ]]\n",
      "\n",
      "  [[-1.13986     0.06528747 -0.5116981   0.75716764  4.4378066\n",
      "    -1.1630316 ]\n",
      "   [ 0.23661456 -2.276422   -1.5742096  -1.1921136  -1.0701761\n",
      "     0.24994968]\n",
      "   [ 1.2117268  -0.8709579   0.27299374 -0.46032202 -3.2070596\n",
      "     1.2159494 ]]\n",
      "\n",
      "  [[-0.5769834   4.418479    2.9384317   2.4967303  -0.14060836\n",
      "    -0.5965953 ]\n",
      "   [ 0.4858764  -4.1729226  -2.5100694  -3.0045586  -0.95974565\n",
      "     0.49929497]\n",
      "   [ 1.2385108  -6.2040477  -5.024088   -4.462554   -1.948856\n",
      "     1.2523348 ]]\n",
      "\n",
      "  [[-0.44670728  3.0870311   1.6550196   1.9220535  -0.1178598\n",
      "    -0.40508384]\n",
      "   [ 0.39069507 -4.4789886  -3.7056172  -3.9325013  -0.45467767\n",
      "     0.3951269 ]\n",
      "   [ 0.73082334 -7.219702   -5.0240455  -5.9594297  -0.41063663\n",
      "     0.7376761 ]]\n",
      "\n",
      "  [[-1.7284982   3.5421734   3.979648    2.255315   -0.08031027\n",
      "    -1.7139624 ]\n",
      "   [ 1.8577509  -3.874339   -4.0099216  -2.2661076  -0.20281102\n",
      "     1.8486136 ]\n",
      "   [ 1.437504   -4.077823   -4.272856   -2.7402203   0.05215758\n",
      "     1.4500792 ]]]], shape=(1, 8, 3, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 6)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724    5.2739615    0.44445598   0.6199022   -1.8606094\n",
      "     -5.4542537 ]\n",
      "   [ -2.6434557    0.7762684   -0.8715434   -2.1795447   -3.9540012\n",
      "     -4.6782713 ]\n",
      "   [ -6.1793337    5.196875     0.9097851   -2.007469    -3.7270458\n",
      "     -8.857345  ]\n",
      "   [ -5.1220007    6.1545386   -1.5864905   -0.9809261   -1.6544849\n",
      "     -6.897224  ]\n",
      "   [ -2.9175513    4.668971     1.8842993   -1.2460753   -3.9725554\n",
      "     -4.511021  ]\n",
      "   [ -3.92207      5.6859646   -0.5558028   -1.2563441   -1.7261751\n",
      "     -6.1812677 ]]\n",
      "\n",
      "  [[ -5.830585     6.4584546    1.6885545   -2.9425578   -2.1129007\n",
      "     -4.4590826 ]\n",
      "   [ -1.4542547    0.66857773  -0.8920885    3.2392426    0.6499656\n",
      "     -0.78637475]\n",
      "   [ -0.77001333   2.5683331   -1.5277501   -1.2103951   -3.361052\n",
      "     -0.6701153 ]\n",
      "   [ -6.770008     4.0346427    3.700929    -5.633308    -3.0441353\n",
      "     -4.6253424 ]\n",
      "   [ -3.266022    -0.2358284    5.192181    -3.326681    -1.1964418\n",
      "     -3.002487  ]\n",
      "   [ -3.8395674    5.5131884    2.4825306   -2.030505    -1.288316\n",
      "     -2.5861757 ]]\n",
      "\n",
      "  [[  0.21261294   2.6522226   -2.1467347   -1.7591757   -1.8478366\n",
      "     -1.005188  ]\n",
      "   [  3.4450672   -9.425682     2.968103     1.0088919    0.15486461\n",
      "      2.6339269 ]\n",
      "   [-10.890656    14.99533      1.3811018    2.7970884   -4.1616673\n",
      "    -12.529773  ]\n",
      "   [ -5.4948997    7.8948936    2.588478     0.5435209   -0.2743195\n",
      "     -5.1724825 ]\n",
      "   [ -3.2168324    0.6599828    1.3618138    2.5563478   -1.6389858\n",
      "     -2.9406655 ]\n",
      "   [ -1.7681361    6.479452     0.12432577  -1.3926682   -2.481772\n",
      "     -2.4043    ]]\n",
      "\n",
      "  [[ -0.62310183   2.588975     0.39473668   2.3254414    0.3566933\n",
      "     -0.95473033]\n",
      "   [ -3.2329118   -1.6349312    4.420214    -1.4593328   -0.2873762\n",
      "     -3.3657458 ]\n",
      "   [ -5.5691104    0.9040649    1.5155857   -1.662806    -1.17778\n",
      "     -5.2585945 ]\n",
      "   [ -5.513655     0.39596844   1.2438383   -1.9474496   -1.6561016\n",
      "     -4.673764  ]\n",
      "   [ -3.756813     2.3833623    7.4268293    1.2679268   -2.3513136\n",
      "     -4.9471974 ]\n",
      "   [ -1.728882     1.0250845    1.9032011   -0.48647845  -1.0321277\n",
      "     -2.5369413 ]]\n",
      "\n",
      "  [[ -3.365542     1.8547945    2.3893828   -0.2378472   -1.7009665\n",
      "     -2.4370363 ]\n",
      "   [ -1.2823843    4.7429194   -4.1070457   -0.47443444  -1.9375597\n",
      "     -1.6068171 ]\n",
      "   [ -5.0705113    7.2453456    0.22619136   3.822259    -2.3195603\n",
      "     -3.822914  ]\n",
      "   [ -3.6519911    2.915474     0.76882476  -0.10871613  -4.1921997\n",
      "     -2.662963  ]\n",
      "   [  2.4449081   -1.4922832    0.5579315   -0.2331474    1.4929048\n",
      "      1.6919394 ]\n",
      "   [  0.68565965  -1.3965738    2.239955     1.8028462    2.3655758\n",
      "      1.6723614 ]]\n",
      "\n",
      "  [[ -2.1686854    1.8683971   -0.46407908  -0.78368795  -2.3975863\n",
      "     -2.3532345 ]\n",
      "   [ -6.185767     2.228677     0.71968794  -2.434057    -5.3960166\n",
      "     -4.845154  ]\n",
      "   [  0.25900394   2.3122008    0.9818626    2.2493217   -0.2135596\n",
      "     -1.5250982 ]\n",
      "   [ -3.6363199    5.520364     2.119328    -1.6462586   -3.9594364\n",
      "     -5.000841  ]\n",
      "   [ -3.2407305    4.0637727   -3.65974     -1.2874398   -5.055235\n",
      "     -1.0524285 ]\n",
      "   [ -1.6338614    2.8127375   -0.63038987  -1.1972872   -2.4497988\n",
      "     -2.5156808 ]]\n",
      "\n",
      "  [[ -0.3049526   -0.4231821    2.3549898    1.2362137   -3.3428524\n",
      "     -2.4705725 ]\n",
      "   [ -8.393576     1.2230886   -0.47221878  -0.3785542   -9.154858\n",
      "     -9.623112  ]\n",
      "   [ -7.879456     1.0038036    6.269445     3.383196    -5.1612587\n",
      "     -9.652613  ]\n",
      "   [ -6.5884266    1.8914772    0.58854324   0.7356621   -3.4622736\n",
      "     -8.281077  ]\n",
      "   [  3.2697265   -0.22400014  -1.8101139    0.84822214   0.9626521\n",
      "      4.273874  ]\n",
      "   [ -1.6477983   -0.3436993    2.7197764    0.96644586  -0.9888108\n",
      "     -3.2478032 ]]\n",
      "\n",
      "  [[ -2.8292158    2.69152     -5.9879627    0.05573386  -0.583922\n",
      "     -0.6253632 ]\n",
      "   [ -5.2028084    1.2464813   -0.28601608   0.14257993  -7.337432\n",
      "     -6.4034657 ]\n",
      "   [ -8.92803      6.5982924   -2.5037014    1.7294589   -8.505811\n",
      "     -8.242324  ]\n",
      "   [ -2.8773344    3.950198     0.63110507   2.2820823   -5.6118255\n",
      "     -3.8134937 ]\n",
      "   [ -1.743894     3.723778     3.8605156    5.6352224   -8.514911\n",
      "     -4.5736675 ]\n",
      "   [ -1.8900229    3.1358528   -4.051401    -0.7488651   -2.9355698\n",
      "     -1.1315701 ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.921542     1.008876     0.3188283   -0.09219998  -3.1753793\n",
      "     -6.121585  ]\n",
      "   [ -7.19034      2.2363086    1.0159979    2.2710261    1.5986332\n",
      "     -6.611934  ]\n",
      "   [ -7.901739     3.423445     0.28121415   1.4161541    0.7545412\n",
      "     -6.8629513 ]\n",
      "   [ -4.4973965    2.089329    -1.5849537    0.9947852    1.7196369\n",
      "     -4.0510983 ]\n",
      "   [ -3.9672196   -0.16474341  -4.148401    -1.2713867    0.75565946\n",
      "     -4.159244  ]\n",
      "   [ -5.64101      0.69921917  -0.1817732   -0.8431055   -2.560344\n",
      "     -4.9824452 ]]\n",
      "\n",
      "  [[ -1.7980527   -1.5812842   -1.9451343   -3.2220113  -10.310742\n",
      "     -3.824608  ]\n",
      "   [ -3.479981     3.2808747    2.645604     1.2420398   -2.2346535\n",
      "     -2.8920004 ]\n",
      "   [-12.847355     4.693563     1.7069713    0.14368261  -8.192775\n",
      "    -12.160423  ]\n",
      "   [ -6.526992     5.337606     4.5975156    2.282755    -3.9640458\n",
      "     -5.790892  ]\n",
      "   [ -6.0473094    6.4522133    4.423212     2.9676814   -6.354302\n",
      "     -5.978778  ]\n",
      "   [ -1.6442655   -1.3541979   -1.5947205   -3.101895   -11.002462\n",
      "     -3.6366527 ]]\n",
      "\n",
      "  [[ -6.8985233    4.0711756    4.4317036    5.133        9.072707\n",
      "     -6.7747245 ]\n",
      "   [ -4.049389     1.4091482   -1.5386783    2.7641459    2.5902662\n",
      "     -4.0136867 ]\n",
      "   [ -5.813297     2.3433473   -0.34706935   3.3857057    2.8579068\n",
      "     -6.316168  ]\n",
      "   [ -0.3411818    0.1245306    0.50826675   3.0477035    3.963468\n",
      "     -0.06130816]\n",
      "   [ -0.22494124  -0.91744673   0.13633734   0.49933708   0.61860615\n",
      "     -0.83748645]\n",
      "   [ -6.4443727    3.8669732    4.5778284    4.6446123    8.3103285\n",
      "     -6.404629  ]]\n",
      "\n",
      "  [[  1.6161397   -3.2769458    1.7488533    0.9128914   -0.7893981\n",
      "      0.947706  ]\n",
      "   [ -6.524896     0.959797    -2.4774404   -2.2015417    0.32523543\n",
      "     -5.8774953 ]\n",
      "   [ -3.653585    -0.7462094   -1.9396536   -1.485212     1.18516\n",
      "     -3.3098702 ]\n",
      "   [ -3.3113432    1.9316599   -0.39751256   0.7585439    2.0309095\n",
      "     -2.8579175 ]\n",
      "   [ -3.7562704   -0.42039806  -2.50029     -3.4232845   -1.0550084\n",
      "     -3.2843208 ]\n",
      "   [  2.5203333   -3.4583664    1.6528863    0.20075847  -0.96879214\n",
      "      1.6454371 ]]\n",
      "\n",
      "  [[ 10.207999    -2.2303858    1.9669571    1.798427     0.77476597\n",
      "      8.378769  ]\n",
      "   [-13.955634     0.30851325  -3.7349398   -7.5653377   -8.753612\n",
      "    -14.702252  ]\n",
      "   [-12.764687    -0.7902593   -4.3138323   -7.5657315   -9.601286\n",
      "    -14.23448   ]\n",
      "   [-10.681347    -0.11150143  -3.1348755   -7.045869   -10.228984\n",
      "    -12.339248  ]\n",
      "   [ -1.1423893   -0.65638995  -1.4652091   -1.5921276   -3.463386\n",
      "     -1.949174  ]\n",
      "   [  6.412017    -1.0095496    1.5431724    0.47404575  -0.6772411\n",
      "      4.897004  ]]\n",
      "\n",
      "  [[ 12.913199    -2.6469307   -0.9226576    3.6522176   10.160698\n",
      "     15.271271  ]\n",
      "   [-15.362403     4.1531754   -1.7186612   -0.8952222   -2.7088346\n",
      "    -15.511199  ]\n",
      "   [-13.55538      3.9044719   -2.516759     1.5854884    0.962125\n",
      "    -13.231382  ]\n",
      "   [-11.3605385    5.017847    -0.27478325   1.2963864    0.11557722\n",
      "    -10.511725  ]\n",
      "   [-11.652038     8.419807     1.2486496    3.690536    -5.5331264\n",
      "    -11.319855  ]\n",
      "   [ 11.963745    -0.8787226    0.3619605    4.453389     9.392935\n",
      "     14.247142  ]]\n",
      "\n",
      "  [[ -8.070269     1.7602693   -0.64440346  -0.81564397  -5.2441835\n",
      "     -7.418611  ]\n",
      "   [ -6.6380925    1.2234269    0.5143768   -3.5395942   -3.2241187\n",
      "     -6.24075   ]\n",
      "   [ -5.3416753    0.17293984  -0.8595884   -2.5809216   -2.9293919\n",
      "     -5.446527  ]\n",
      "   [ -5.386313     1.0168418    0.3420028   -2.2858331   -2.7953925\n",
      "     -5.25029   ]\n",
      "   [ -5.84336      1.6246461    0.1882815   -0.18845634  -0.4196565\n",
      "     -5.6329203 ]\n",
      "   [ -6.8899517    1.6988323   -0.6224127   -0.4996385   -4.342832\n",
      "     -6.242506  ]]\n",
      "\n",
      "  [[ 13.240363     4.66841     13.042674     5.9210863   -0.81710047\n",
      "     12.923436  ]\n",
      "   [ -7.091225     2.8033364   -1.5975537   -2.3024895   -1.2088296\n",
      "     -7.397709  ]\n",
      "   [ -3.5253422    6.1410527    5.51238      2.8467648   -1.1766208\n",
      "     -4.0684705 ]\n",
      "   [ -1.7825942    5.909616     4.947849     1.271513    -2.085788\n",
      "     -1.9155308 ]\n",
      "   [ -7.756648     7.3098855    5.8315535    4.5246334   -1.2094965\n",
      "     -7.161164  ]\n",
      "   [ 13.163366     4.3483615   12.748606     6.562309    -0.33964238\n",
      "     13.154069  ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.07309047  0.47737727  0.34132183]\n",
      "   [-1.2853749   3.06507    -0.8552955  -3.787713  ]\n",
      "   [ 0.58923376  0.9813636   0.905565   -0.45120466]\n",
      "   [-0.480265    1.2182947  -0.6317736  -3.0247095 ]]\n",
      "\n",
      "  [[-0.1123142   0.04219194  0.25517872  0.09259826]\n",
      "   [-0.02044308 -1.6470729   0.7732946   2.1267853 ]\n",
      "   [ 0.47959617  2.2830687  -2.5592422  -1.2285285 ]\n",
      "   [-0.27183947  2.0290937  -0.43420914 -0.27512893]]\n",
      "\n",
      "  [[-0.10918517 -0.00974669  0.32006803  0.37504762]\n",
      "   [-0.2557946  -1.5501258  -0.98448783  0.7212172 ]\n",
      "   [ 0.26977882 -1.2627326  -5.8444214   0.83772284]\n",
      "   [-0.14635846  4.5247483   3.7914298   2.6523268 ]]\n",
      "\n",
      "  [[-0.22114104  0.3014268   0.42661816  0.26912642]\n",
      "   [-0.12756507 -2.1900728   1.8131207   2.2891426 ]\n",
      "   [ 0.54721624  2.5360978  -3.0377705   2.7075    ]\n",
      "   [-0.6354002   3.419321    5.463122    3.643488  ]]\n",
      "\n",
      "  [[ 0.00076126 -0.22633906 -0.29323676 -0.50408626]\n",
      "   [-0.30625445 -1.0766943  -1.4381294   0.870895  ]\n",
      "   [ 0.17989263 -1.2728978  -1.1890442   3.6042721 ]\n",
      "   [ 0.20693003  0.61464036  4.297538   -1.1855712 ]]\n",
      "\n",
      "  [[-0.00999826  0.10504516  0.17285524 -0.00655898]\n",
      "   [ 0.46795642 -2.4885566   1.4032799  -0.4286147 ]\n",
      "   [ 0.26839072  0.88721114 -3.3536348   0.825778  ]\n",
      "   [ 1.4696841  -4.796935   -2.0386317  -9.057217  ]]\n",
      "\n",
      "  [[-0.20863976  0.3057065   0.37231094  0.18439673]\n",
      "   [ 0.4478607  -3.5688307  -1.2092146   0.40666017]\n",
      "   [ 1.2270066  -3.1708426  -7.3976893  -3.8444464 ]\n",
      "   [-0.57856214 -3.5443087  -1.1052079   1.9420586 ]]\n",
      "\n",
      "  [[-0.15726982  0.07697631 -0.3421147  -0.34590858]\n",
      "   [ 0.03995608 -3.892801   -1.946208   -2.4140923 ]\n",
      "   [ 0.36951503  3.2891269  -3.5526025   1.1559011 ]\n",
      "   [ 0.05717298 -3.0957925   0.40507582 -2.4314208 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.55216384  -3.8232827   -2.910573    -3.5972157   -1.0817922\n",
      "      0.5912545 ]\n",
      "   [  0.6380123    1.8036875    1.6386297    2.195628     2.0554762\n",
      "      0.64612067]\n",
      "   [  0.5677011   -2.271689    -2.5307884   -3.5080261   -2.9300206\n",
      "      0.561479  ]\n",
      "   [  1.4661043   -3.475675    -3.5228517   -5.107479    -3.4958646\n",
      "      1.4961042 ]]\n",
      "\n",
      "  [[  1.3908786   -1.9680318   -2.4739487   -2.0419824   -1.1162156\n",
      "      1.3957309 ]\n",
      "   [  0.02316882   1.4757677    1.3413774    0.54396117  -0.06397482\n",
      "      0.01687552]\n",
      "   [  0.3321708   -2.1242893   -1.7803483   -2.009874    -1.3619233\n",
      "      0.34443972]\n",
      "   [  1.5660084   -3.5792403   -3.6989744   -3.5270753   -2.0451634\n",
      "      1.5609167 ]]\n",
      "\n",
      "  [[ -0.8532795   -1.1699389   -0.5007857   -0.49849874  -1.1035897\n",
      "     -0.8753896 ]\n",
      "   [  0.47264126  -5.680002    -4.517802    -4.377047    -2.9761038\n",
      "      0.4754463 ]\n",
      "   [  1.8673079  -13.079067   -11.496452   -10.314077    -6.8916826\n",
      "      1.8901018 ]\n",
      "   [  1.0708896    0.00566173   0.93092626  -0.05228119  -0.2120755\n",
      "      1.0923316 ]]\n",
      "\n",
      "  [[ -0.54138374  -3.166048    -2.7694507   -3.96975     -4.5371394\n",
      "     -0.5512311 ]\n",
      "   [  0.5716766   -8.624254    -7.4991584   -7.3359303   -6.564319\n",
      "      0.57178384]\n",
      "   [  1.3138021   -5.0567894   -4.579849    -3.9845834   -3.0800323\n",
      "      1.3318301 ]\n",
      "   [  1.8490229   -3.1415074   -3.7784946   -3.1038036   -3.930179\n",
      "      1.8831608 ]]\n",
      "\n",
      "  [[ -1.7378478    0.02165176  -0.90774333  -1.4618493   -0.88553613\n",
      "     -1.7313937 ]\n",
      "   [ -0.27279285  -0.59817743  -1.842621    -1.2421002   -0.9153028\n",
      "     -0.28948683]\n",
      "   [  2.5725012   -1.9725125   -1.8943149   -2.6406758   -1.6912924\n",
      "      2.6305268 ]\n",
      "   [  4.6581655   -2.885642    -1.5091549   -3.7218456   -2.5507739\n",
      "      4.7409744 ]]\n",
      "\n",
      "  [[  1.6262298   -5.331017    -5.3980026   -5.357892    -0.5227707\n",
      "      1.6225317 ]\n",
      "   [  1.4594786   -4.6213126   -5.345402    -4.8237014    0.0415122\n",
      "      1.4560635 ]\n",
      "   [  1.6029699   -5.7235622   -5.4344754   -5.700281    -3.458166\n",
      "      1.5927473 ]\n",
      "   [  1.2885883   -2.6441948   -3.47773     -3.09467     -1.880738\n",
      "      1.2695488 ]]\n",
      "\n",
      "  [[  0.3844978    3.3728192    1.4984373    2.177764     1.7664791\n",
      "      0.3842628 ]\n",
      "   [  1.3693358   -5.0935154   -3.921827    -3.6555476   -1.8274075\n",
      "      1.3558773 ]\n",
      "   [  1.121359    -4.7008166   -3.0253885   -4.153024    -3.3693306\n",
      "      1.1555592 ]\n",
      "   [  2.4564793   -5.027429    -3.6513312   -4.4679847   -3.1567855\n",
      "      2.4609444 ]]\n",
      "\n",
      "  [[ -0.3909884    1.3290287    0.7153773    0.30487654   0.01067167\n",
      "     -0.37957388]\n",
      "   [  0.91534704  -2.004378    -1.3137145   -1.8292936    0.04522658\n",
      "      0.95372003]\n",
      "   [  0.84502876  -6.334351    -4.8446565   -4.5403237   -3.0086424\n",
      "      0.83118516]\n",
      "   [  1.2349606   -4.2636824   -3.542239    -3.9259446   -2.1887977\n",
      "      1.2550868 ]]]], shape=(1, 8, 4, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 6)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.854575   -0.76395994  1.379787    0.08473525]\n",
      "   [ 2.769262   -2.4458904  -0.38577634 -4.416887  ]\n",
      "   [ 4.645977    0.5011021  -4.5003643  -0.7450462 ]\n",
      "   [-3.7169092  -4.7985697   2.3044372  11.668311  ]]\n",
      "\n",
      "  [[-0.28805032 -0.33996823  0.56497     0.339852  ]\n",
      "   [-0.6685229  -2.6304247  -2.186717   -1.0663583 ]\n",
      "   [ 1.9647298   0.46823615 -7.4626803  -0.66195506]\n",
      "   [-3.10413    -0.5612599   1.8442365   1.6471825 ]]\n",
      "\n",
      "  [[-3.0765195   1.7763902   1.9594084  -0.04509975]\n",
      "   [-4.732454    2.829565    3.985526    0.7470543 ]\n",
      "   [ 0.04412972  1.7056273  -0.07778924 -1.2706735 ]\n",
      "   [-2.4218519   2.0299835  -1.2699906  -0.32161674]]\n",
      "\n",
      "  [[-3.0387642   1.292204    3.5883346   1.1335102 ]\n",
      "   [-5.8640046   2.6051586   9.00416     3.7582636 ]\n",
      "   [-2.221647   -2.3018663   0.05621783  0.84461373]\n",
      "   [-0.11405914  3.126089   10.533683   -7.5244102 ]]\n",
      "\n",
      "  [[-0.61142784 -0.4549538   0.9963378  -0.14479269]\n",
      "   [ 3.101384   -1.9635861  -1.9723365  -4.5019665 ]\n",
      "   [ 3.8482957   5.668536   -3.3393283  -2.0147512 ]\n",
      "   [ 0.20807946  1.303806   -0.99223     0.8040336 ]]\n",
      "\n",
      "  [[-1.497981    1.4031547   0.91072094  0.3312241 ]\n",
      "   [ 1.1607789   1.7605858   0.20715673 -1.451161  ]\n",
      "   [ 1.3948244   1.7155915   1.5817758  -1.3053533 ]\n",
      "   [-0.87899184 -3.5021908  -3.0559537   5.2945843 ]]\n",
      "\n",
      "  [[-2.76698    -1.0661263   0.08043555  2.666324  ]\n",
      "   [ 0.196701    0.70394504  0.4231429  -0.37788275]\n",
      "   [ 2.5972507   5.5233197  -0.16624556 -2.4989765 ]\n",
      "   [ 0.2208142   1.8013881  -1.5426023  -2.4432566 ]]\n",
      "\n",
      "  [[-1.9492282   1.2290938   1.8699198  -1.1443392 ]\n",
      "   [-0.40289074 -0.30640975  0.74108803  2.062364  ]\n",
      "   [ 0.49884918 -0.6150069  -2.8845108   5.5886726 ]\n",
      "   [-0.08005809  2.9849775   0.54212534 -3.2520006 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.53411824  3.138721    3.4550288   2.1090372   1.3961537\n",
      "    -0.47872242]\n",
      "   [ 0.16838217 -0.68966126 -0.65448135 -0.50101525 -0.9362338\n",
      "     0.17256774]\n",
      "   [ 0.18245015 -3.0500743  -2.9301724  -2.5275035  -1.6815708\n",
      "     0.16176914]\n",
      "   [ 0.8455783  -5.5165462  -5.869892   -4.2255783  -1.9790193\n",
      "     0.8301941 ]]\n",
      "\n",
      "  [[-0.6982859   1.8579963   0.2598044   0.11909989  0.10357708\n",
      "    -0.68354815]\n",
      "   [-0.70370317 -1.1944655  -1.8300511  -1.625776   -1.3867573\n",
      "    -0.7268194 ]\n",
      "   [ 0.8023996  -5.720824   -3.8634129  -3.597791   -1.2961257\n",
      "     0.79261404]\n",
      "   [ 0.74754465 -6.191035   -4.8284483  -4.4190493  -1.184658\n",
      "     0.7491211 ]]\n",
      "\n",
      "  [[-0.7371446   0.21600349 -0.9151045  -0.87319124 -0.58509016\n",
      "    -0.6977605 ]\n",
      "   [ 0.24199572  0.91136837  0.80540216  0.90500486  0.531705\n",
      "     0.21405065]\n",
      "   [ 0.428904    3.0467944   3.654848    3.5137124   2.1350443\n",
      "     0.39721748]\n",
      "   [ 1.6770718  -3.8380928  -1.8470231  -2.2208014  -1.8292687\n",
      "     1.6466098 ]]\n",
      "\n",
      "  [[-2.016561    1.2345561  -0.22407255 -0.47709444 -0.02320303\n",
      "    -1.952554  ]\n",
      "   [ 1.6252061  -5.532724   -4.697985   -4.2196207  -1.0165944\n",
      "     1.6203778 ]\n",
      "   [ 2.4023926  -3.3139603  -1.6055428  -0.78964245  0.77323705\n",
      "     2.3298233 ]\n",
      "   [ 3.5562131  -3.931177   -1.4862784  -1.3094387  -0.5170017\n",
      "     3.5151906 ]]\n",
      "\n",
      "  [[-1.13986     0.06528747 -0.5116981   0.75716764  4.4378066\n",
      "    -1.1630316 ]\n",
      "   [ 0.23661456 -2.276422   -1.5742096  -1.1921136  -1.0701761\n",
      "     0.24994968]\n",
      "   [ 1.2117268  -0.8709579   0.27299374 -0.46032202 -3.2070596\n",
      "     1.2159494 ]\n",
      "   [ 2.4224887  -2.187748   -1.8167549  -2.7981403  -5.329892\n",
      "     2.4648128 ]]\n",
      "\n",
      "  [[-0.5769834   4.418479    2.9384317   2.4967303  -0.14060836\n",
      "    -0.5965953 ]\n",
      "   [ 0.4858764  -4.1729226  -2.5100694  -3.0045586  -0.95974565\n",
      "     0.49929497]\n",
      "   [ 1.2385108  -6.2040477  -5.024088   -4.462554   -1.948856\n",
      "     1.2523348 ]\n",
      "   [-0.14234024  0.4847322   1.347807    1.7450886   2.1401093\n",
      "    -0.14628264]]\n",
      "\n",
      "  [[-0.44670728  3.0870311   1.6550196   1.9220535  -0.1178598\n",
      "    -0.40508384]\n",
      "   [ 0.39069507 -4.4789886  -3.7056172  -3.9325013  -0.45467767\n",
      "     0.3951269 ]\n",
      "   [ 0.73082334 -7.219702   -5.0240455  -5.9594297  -0.41063663\n",
      "     0.7376761 ]\n",
      "   [ 0.88285923 -3.030285   -0.5194961  -1.411059    1.797219\n",
      "     0.879616  ]]\n",
      "\n",
      "  [[-1.7284982   3.5421734   3.979648    2.255315   -0.08031027\n",
      "    -1.7139624 ]\n",
      "   [ 1.8577509  -3.874339   -4.0099216  -2.2661076  -0.20281102\n",
      "     1.8486136 ]\n",
      "   [ 1.437504   -4.077823   -4.272856   -2.7402203   0.05215758\n",
      "     1.4500792 ]\n",
      "   [ 0.53686684 -3.6171033  -3.5264325  -2.0010986   0.6845382\n",
      "     0.52465266]]]], shape=(1, 8, 4, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 6)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724    5.2739615    0.44445598   0.6199022   -1.8606094\n",
      "     -5.4542537 ]\n",
      "   [ -2.6434557    0.7762684   -0.8715434   -2.1795447   -3.9540012\n",
      "     -4.6782713 ]\n",
      "   [ -6.1793337    5.196875     0.9097851   -2.007469    -3.7270458\n",
      "     -8.857345  ]\n",
      "   [ -5.1220007    6.1545386   -1.5864905   -0.9809261   -1.6544849\n",
      "     -6.897224  ]\n",
      "   [ -2.9175513    4.668971     1.8842993   -1.2460753   -3.9725554\n",
      "     -4.511021  ]\n",
      "   [ -3.92207      5.6859646   -0.5558028   -1.2563441   -1.7261751\n",
      "     -6.1812677 ]]\n",
      "\n",
      "  [[ -5.830585     6.4584546    1.6885545   -2.9425578   -2.1129007\n",
      "     -4.4590826 ]\n",
      "   [ -1.4542547    0.66857773  -0.8920885    3.2392426    0.6499656\n",
      "     -0.78637475]\n",
      "   [ -0.77001333   2.5683331   -1.5277501   -1.2103951   -3.361052\n",
      "     -0.6701153 ]\n",
      "   [ -6.770008     4.0346427    3.700929    -5.633308    -3.0441353\n",
      "     -4.6253424 ]\n",
      "   [ -3.266022    -0.2358284    5.192181    -3.326681    -1.1964418\n",
      "     -3.002487  ]\n",
      "   [ -3.8395674    5.5131884    2.4825306   -2.030505    -1.288316\n",
      "     -2.5861757 ]]\n",
      "\n",
      "  [[  0.21261294   2.6522226   -2.1467347   -1.7591757   -1.8478366\n",
      "     -1.005188  ]\n",
      "   [  3.4450672   -9.425682     2.968103     1.0088919    0.15486461\n",
      "      2.6339269 ]\n",
      "   [-10.890656    14.99533      1.3811018    2.7970884   -4.1616673\n",
      "    -12.529773  ]\n",
      "   [ -5.4948997    7.8948936    2.588478     0.5435209   -0.2743195\n",
      "     -5.1724825 ]\n",
      "   [ -3.2168324    0.6599828    1.3618138    2.5563478   -1.6389858\n",
      "     -2.9406655 ]\n",
      "   [ -1.7681361    6.479452     0.12432577  -1.3926682   -2.481772\n",
      "     -2.4043    ]]\n",
      "\n",
      "  [[ -0.62310183   2.588975     0.39473668   2.3254414    0.3566933\n",
      "     -0.95473033]\n",
      "   [ -3.2329118   -1.6349312    4.420214    -1.4593328   -0.2873762\n",
      "     -3.3657458 ]\n",
      "   [ -5.5691104    0.9040649    1.5155857   -1.662806    -1.17778\n",
      "     -5.2585945 ]\n",
      "   [ -5.513655     0.39596844   1.2438383   -1.9474496   -1.6561016\n",
      "     -4.673764  ]\n",
      "   [ -3.756813     2.3833623    7.4268293    1.2679268   -2.3513136\n",
      "     -4.9471974 ]\n",
      "   [ -1.728882     1.0250845    1.9032011   -0.48647845  -1.0321277\n",
      "     -2.5369413 ]]\n",
      "\n",
      "  [[ -3.365542     1.8547945    2.3893828   -0.2378472   -1.7009665\n",
      "     -2.4370363 ]\n",
      "   [ -1.2823843    4.7429194   -4.1070457   -0.47443444  -1.9375597\n",
      "     -1.6068171 ]\n",
      "   [ -5.0705113    7.2453456    0.22619136   3.822259    -2.3195603\n",
      "     -3.822914  ]\n",
      "   [ -3.6519911    2.915474     0.76882476  -0.10871613  -4.1921997\n",
      "     -2.662963  ]\n",
      "   [  2.4449081   -1.4922832    0.5579315   -0.2331474    1.4929048\n",
      "      1.6919394 ]\n",
      "   [  0.68565965  -1.3965738    2.239955     1.8028462    2.3655758\n",
      "      1.6723614 ]]\n",
      "\n",
      "  [[ -2.1686854    1.8683971   -0.46407908  -0.78368795  -2.3975863\n",
      "     -2.3532345 ]\n",
      "   [ -6.185767     2.228677     0.71968794  -2.434057    -5.3960166\n",
      "     -4.845154  ]\n",
      "   [  0.25900394   2.3122008    0.9818626    2.2493217   -0.2135596\n",
      "     -1.5250982 ]\n",
      "   [ -3.6363199    5.520364     2.119328    -1.6462586   -3.9594364\n",
      "     -5.000841  ]\n",
      "   [ -3.2407305    4.0637727   -3.65974     -1.2874398   -5.055235\n",
      "     -1.0524285 ]\n",
      "   [ -1.6338614    2.8127375   -0.63038987  -1.1972872   -2.4497988\n",
      "     -2.5156808 ]]\n",
      "\n",
      "  [[ -0.3049526   -0.4231821    2.3549898    1.2362137   -3.3428524\n",
      "     -2.4705725 ]\n",
      "   [ -8.393576     1.2230886   -0.47221878  -0.3785542   -9.154858\n",
      "     -9.623112  ]\n",
      "   [ -7.879456     1.0038036    6.269445     3.383196    -5.1612587\n",
      "     -9.652613  ]\n",
      "   [ -6.5884266    1.8914772    0.58854324   0.7356621   -3.4622736\n",
      "     -8.281077  ]\n",
      "   [  3.2697265   -0.22400014  -1.8101139    0.84822214   0.9626521\n",
      "      4.273874  ]\n",
      "   [ -1.6477983   -0.3436993    2.7197764    0.96644586  -0.9888108\n",
      "     -3.2478032 ]]\n",
      "\n",
      "  [[ -2.8292158    2.69152     -5.9879627    0.05573386  -0.583922\n",
      "     -0.6253632 ]\n",
      "   [ -5.2028084    1.2464813   -0.28601608   0.14257993  -7.337432\n",
      "     -6.4034657 ]\n",
      "   [ -8.92803      6.5982924   -2.5037014    1.7294589   -8.505811\n",
      "     -8.242324  ]\n",
      "   [ -2.8773344    3.950198     0.63110507   2.2820823   -5.6118255\n",
      "     -3.8134937 ]\n",
      "   [ -1.743894     3.723778     3.8605156    5.6352224   -8.514911\n",
      "     -4.5736675 ]\n",
      "   [ -1.8900229    3.1358528   -4.051401    -0.7488651   -2.9355698\n",
      "     -1.1315701 ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 6, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.921542     1.008876     0.3188283   -0.09219998  -3.1753793\n",
      "     -6.121585  ]\n",
      "   [ -7.19034      2.2363086    1.0159979    2.2710261    1.5986332\n",
      "     -6.611934  ]\n",
      "   [ -7.901739     3.423445     0.28121415   1.4161541    0.7545412\n",
      "     -6.8629513 ]\n",
      "   [ -4.4973965    2.089329    -1.5849537    0.9947852    1.7196369\n",
      "     -4.0510983 ]\n",
      "   [ -3.9672196   -0.16474341  -4.148401    -1.2713867    0.75565946\n",
      "     -4.159244  ]\n",
      "   [ -5.64101      0.69921917  -0.1817732   -0.8431055   -2.560344\n",
      "     -4.9824452 ]]\n",
      "\n",
      "  [[ -1.7980527   -1.5812842   -1.9451343   -3.2220113  -10.310742\n",
      "     -3.824608  ]\n",
      "   [ -3.479981     3.2808747    2.645604     1.2420398   -2.2346535\n",
      "     -2.8920004 ]\n",
      "   [-12.847355     4.693563     1.7069713    0.14368261  -8.192775\n",
      "    -12.160423  ]\n",
      "   [ -6.526992     5.337606     4.5975156    2.282755    -3.9640458\n",
      "     -5.790892  ]\n",
      "   [ -6.0473094    6.4522133    4.423212     2.9676814   -6.354302\n",
      "     -5.978778  ]\n",
      "   [ -1.6442655   -1.3541979   -1.5947205   -3.101895   -11.002462\n",
      "     -3.6366527 ]]\n",
      "\n",
      "  [[ -6.8985233    4.0711756    4.4317036    5.133        9.072707\n",
      "     -6.7747245 ]\n",
      "   [ -4.049389     1.4091482   -1.5386783    2.7641459    2.5902662\n",
      "     -4.0136867 ]\n",
      "   [ -5.813297     2.3433473   -0.34706935   3.3857057    2.8579068\n",
      "     -6.316168  ]\n",
      "   [ -0.3411818    0.1245306    0.50826675   3.0477035    3.963468\n",
      "     -0.06130816]\n",
      "   [ -0.22494124  -0.91744673   0.13633734   0.49933708   0.61860615\n",
      "     -0.83748645]\n",
      "   [ -6.4443727    3.8669732    4.5778284    4.6446123    8.3103285\n",
      "     -6.404629  ]]\n",
      "\n",
      "  [[  1.6161397   -3.2769458    1.7488533    0.9128914   -0.7893981\n",
      "      0.947706  ]\n",
      "   [ -6.524896     0.959797    -2.4774404   -2.2015417    0.32523543\n",
      "     -5.8774953 ]\n",
      "   [ -3.653585    -0.7462094   -1.9396536   -1.485212     1.18516\n",
      "     -3.3098702 ]\n",
      "   [ -3.3113432    1.9316599   -0.39751256   0.7585439    2.0309095\n",
      "     -2.8579175 ]\n",
      "   [ -3.7562704   -0.42039806  -2.50029     -3.4232845   -1.0550084\n",
      "     -3.2843208 ]\n",
      "   [  2.5203333   -3.4583664    1.6528863    0.20075847  -0.96879214\n",
      "      1.6454371 ]]\n",
      "\n",
      "  [[ 10.207999    -2.2303858    1.9669571    1.798427     0.77476597\n",
      "      8.378769  ]\n",
      "   [-13.955634     0.30851325  -3.7349398   -7.5653377   -8.753612\n",
      "    -14.702252  ]\n",
      "   [-12.764687    -0.7902593   -4.3138323   -7.5657315   -9.601286\n",
      "    -14.23448   ]\n",
      "   [-10.681347    -0.11150143  -3.1348755   -7.045869   -10.228984\n",
      "    -12.339248  ]\n",
      "   [ -1.1423893   -0.65638995  -1.4652091   -1.5921276   -3.463386\n",
      "     -1.949174  ]\n",
      "   [  6.412017    -1.0095496    1.5431724    0.47404575  -0.6772411\n",
      "      4.897004  ]]\n",
      "\n",
      "  [[ 12.913199    -2.6469307   -0.9226576    3.6522176   10.160698\n",
      "     15.271271  ]\n",
      "   [-15.362403     4.1531754   -1.7186612   -0.8952222   -2.7088346\n",
      "    -15.511199  ]\n",
      "   [-13.55538      3.9044719   -2.516759     1.5854884    0.962125\n",
      "    -13.231382  ]\n",
      "   [-11.3605385    5.017847    -0.27478325   1.2963864    0.11557722\n",
      "    -10.511725  ]\n",
      "   [-11.652038     8.419807     1.2486496    3.690536    -5.5331264\n",
      "    -11.319855  ]\n",
      "   [ 11.963745    -0.8787226    0.3619605    4.453389     9.392935\n",
      "     14.247142  ]]\n",
      "\n",
      "  [[ -8.070269     1.7602693   -0.64440346  -0.81564397  -5.2441835\n",
      "     -7.418611  ]\n",
      "   [ -6.6380925    1.2234269    0.5143768   -3.5395942   -3.2241187\n",
      "     -6.24075   ]\n",
      "   [ -5.3416753    0.17293984  -0.8595884   -2.5809216   -2.9293919\n",
      "     -5.446527  ]\n",
      "   [ -5.386313     1.0168418    0.3420028   -2.2858331   -2.7953925\n",
      "     -5.25029   ]\n",
      "   [ -5.84336      1.6246461    0.1882815   -0.18845634  -0.4196565\n",
      "     -5.6329203 ]\n",
      "   [ -6.8899517    1.6988323   -0.6224127   -0.4996385   -4.342832\n",
      "     -6.242506  ]]\n",
      "\n",
      "  [[ 13.240363     4.66841     13.042674     5.9210863   -0.81710047\n",
      "     12.923436  ]\n",
      "   [ -7.091225     2.8033364   -1.5975537   -2.3024895   -1.2088296\n",
      "     -7.397709  ]\n",
      "   [ -3.5253422    6.1410527    5.51238      2.8467648   -1.1766208\n",
      "     -4.0684705 ]\n",
      "   [ -1.7825942    5.909616     4.947849     1.271513    -2.085788\n",
      "     -1.9155308 ]\n",
      "   [ -7.756648     7.3098855    5.8315535    4.5246334   -1.2094965\n",
      "     -7.161164  ]\n",
      "   [ 13.163366     4.3483615   12.748606     6.562309    -0.33964238\n",
      "     13.154069  ]]]], shape=(1, 8, 6, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 6, 6)\n",
      "output.shape = (1, 8, 6, 32)\n",
      "scaled_attention.shape= (1, 6, 8, 32)\n",
      "concat_attention.shape= (1, 6, 256)\n",
      "outputs.shape= (1, 6, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.07309047  0.47737727  0.34132183  1.6703248 ]\n",
      "   [-1.2853749   3.06507    -0.8552955  -3.787713   -3.2108195 ]\n",
      "   [ 0.58923376  0.9813636   0.905565   -0.45120466 -6.67745   ]\n",
      "   [-0.480265    1.2182947  -0.6317736  -3.0247095  -0.46701738]\n",
      "   [ 1.3933394  -7.691685    5.9806495   5.0620317  20.337044  ]]\n",
      "\n",
      "  [[-0.1123142   0.04219194  0.25517872  0.09259826 -0.8358049 ]\n",
      "   [-0.02044308 -1.6470729   0.7732946   2.1267853   3.5064476 ]\n",
      "   [ 0.47959617  2.2830687  -2.5592422  -1.2285285  -5.789118  ]\n",
      "   [-0.27183947  2.0290937  -0.43420914 -0.27512893 -7.831817  ]\n",
      "   [-0.14613436 -3.3456848   1.7137698   2.1373973  45.86721   ]]\n",
      "\n",
      "  [[-0.10918517 -0.00974669  0.32006803  0.37504762  0.40573326]\n",
      "   [-0.2557946  -1.5501258  -0.98448783  0.7212172  -5.3590527 ]\n",
      "   [ 0.26977882 -1.2627326  -5.8444214   0.83772284 -5.589488  ]\n",
      "   [-0.14635846  4.5247483   3.7914298   2.6523268  -1.0574609 ]\n",
      "   [-0.16144654 -0.1151108   7.0247684   1.6826607   7.738867  ]]\n",
      "\n",
      "  [[-0.22114104  0.3014268   0.42661816  0.26912642  1.2279955 ]\n",
      "   [-0.12756507 -2.1900728   1.8131207   2.2891426  -1.6018178 ]\n",
      "   [ 0.54721624  2.5360978  -3.0377705   2.7075      2.4343994 ]\n",
      "   [-0.6354002   3.419321    5.463122    3.643488   -4.6487865 ]\n",
      "   [-1.4518204   3.5698617   3.2618985  -0.11717048 24.6823    ]]\n",
      "\n",
      "  [[ 0.00076126 -0.22633906 -0.29323676 -0.50408626  0.7745082 ]\n",
      "   [-0.30625445 -1.0766943  -1.4381294   0.870895   -4.037722  ]\n",
      "   [ 0.17989263 -1.2728978  -1.1890442   3.6042721  -2.085762  ]\n",
      "   [ 0.20693003  0.61464036  4.297538   -1.1855712  -1.1640912 ]\n",
      "   [ 0.5581683  -3.4104176   0.5241854  -1.8653144  -9.289575  ]]\n",
      "\n",
      "  [[-0.00999826  0.10504516  0.17285524 -0.00655898  0.5136193 ]\n",
      "   [ 0.46795642 -2.4885566   1.4032799  -0.4286147  -1.4480523 ]\n",
      "   [ 0.26839072  0.88721114 -3.3536348   0.825778    2.2342236 ]\n",
      "   [ 1.4696841  -4.796935   -2.0386317  -9.057217   -7.6220613 ]\n",
      "   [-0.6888059  -0.8873342  -0.16370389 -1.9566156  10.460171  ]]\n",
      "\n",
      "  [[-0.20863976  0.3057065   0.37231094  0.18439673  0.723368  ]\n",
      "   [ 0.4478607  -3.5688307  -1.2092146   0.40666017 -1.2146559 ]\n",
      "   [ 1.2270066  -3.1708426  -7.3976893  -3.8444464  -0.55549395]\n",
      "   [-0.57856214 -3.5443087  -1.1052079   1.9420586  -0.7640051 ]\n",
      "   [ 1.0718609   1.6728916  -4.858909   -1.9094803  28.063498  ]]\n",
      "\n",
      "  [[-0.15726982  0.07697631 -0.3421147  -0.34590858  0.85049   ]\n",
      "   [ 0.03995608 -3.892801   -1.946208   -2.4140923  10.134585  ]\n",
      "   [ 0.36951503  3.2891269  -3.5526025   1.1559011  -1.9707086 ]\n",
      "   [ 0.05717298 -3.0957925   0.40507582 -2.4314208  11.743679  ]\n",
      "   [ 0.6450698  -4.6292143   1.0163666  -8.007423   32.04396   ]]]], shape=(1, 8, 5, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 5)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[  0.55216384  -3.8232827   -2.910573    -3.5972157   -1.0817922\n",
      "      0.5912545 ]\n",
      "   [  0.6380123    1.8036875    1.6386297    2.195628     2.0554762\n",
      "      0.64612067]\n",
      "   [  0.5677011   -2.271689    -2.5307884   -3.5080261   -2.9300206\n",
      "      0.561479  ]\n",
      "   [  1.4661043   -3.475675    -3.5228517   -5.107479    -3.4958646\n",
      "      1.4961042 ]\n",
      "   [  1.7673435   -4.8022385   -5.6021395   -6.4468803   -4.3783693\n",
      "      1.7877297 ]]\n",
      "\n",
      "  [[  1.3908786   -1.9680318   -2.4739487   -2.0419824   -1.1162156\n",
      "      1.3957309 ]\n",
      "   [  0.02316882   1.4757677    1.3413774    0.54396117  -0.06397482\n",
      "      0.01687552]\n",
      "   [  0.3321708   -2.1242893   -1.7803483   -2.009874    -1.3619233\n",
      "      0.34443972]\n",
      "   [  1.5660084   -3.5792403   -3.6989744   -3.5270753   -2.0451634\n",
      "      1.5609167 ]\n",
      "   [  2.4300394   -1.1474522   -2.3360283   -1.8538892   -0.9486084\n",
      "      2.4312239 ]]\n",
      "\n",
      "  [[ -0.8532795   -1.1699389   -0.5007857   -0.49849874  -1.1035897\n",
      "     -0.8753896 ]\n",
      "   [  0.47264126  -5.680002    -4.517802    -4.377047    -2.9761038\n",
      "      0.4754463 ]\n",
      "   [  1.8673079  -13.079067   -11.496452   -10.314077    -6.8916826\n",
      "      1.8901018 ]\n",
      "   [  1.0708896    0.00566173   0.93092626  -0.05228119  -0.2120755\n",
      "      1.0923316 ]\n",
      "   [  2.1261888   -9.722576    -8.843581    -8.6049795   -5.4396734\n",
      "      2.14265   ]]\n",
      "\n",
      "  [[ -0.54138374  -3.166048    -2.7694507   -3.96975     -4.5371394\n",
      "     -0.5512311 ]\n",
      "   [  0.5716766   -8.624254    -7.4991584   -7.3359303   -6.564319\n",
      "      0.57178384]\n",
      "   [  1.3138021   -5.0567894   -4.579849    -3.9845834   -3.0800323\n",
      "      1.3318301 ]\n",
      "   [  1.8490229   -3.1415074   -3.7784946   -3.1038036   -3.930179\n",
      "      1.8831608 ]\n",
      "   [  1.32191     -6.974353    -6.0573606   -6.34324     -6.888908\n",
      "      1.3559779 ]]\n",
      "\n",
      "  [[ -1.7378478    0.02165176  -0.90774333  -1.4618493   -0.88553613\n",
      "     -1.7313937 ]\n",
      "   [ -0.27279285  -0.59817743  -1.842621    -1.2421002   -0.9153028\n",
      "     -0.28948683]\n",
      "   [  2.5725012   -1.9725125   -1.8943149   -2.6406758   -1.6912924\n",
      "      2.6305268 ]\n",
      "   [  4.6581655   -2.885642    -1.5091549   -3.7218456   -2.5507739\n",
      "      4.7409744 ]\n",
      "   [  3.433598    -2.9411862   -1.9828316   -3.4158406   -1.0027388\n",
      "      3.5011582 ]]\n",
      "\n",
      "  [[  1.6262298   -5.331017    -5.3980026   -5.357892    -0.5227707\n",
      "      1.6225317 ]\n",
      "   [  1.4594786   -4.6213126   -5.345402    -4.8237014    0.0415122\n",
      "      1.4560635 ]\n",
      "   [  1.6029699   -5.7235622   -5.4344754   -5.700281    -3.458166\n",
      "      1.5927473 ]\n",
      "   [  1.2885883   -2.6441948   -3.47773     -3.09467     -1.880738\n",
      "      1.2695488 ]\n",
      "   [  2.3467796   -9.7164755   -9.712223    -9.496376    -3.7720776\n",
      "      2.3275328 ]]\n",
      "\n",
      "  [[  0.3844978    3.3728192    1.4984373    2.177764     1.7664791\n",
      "      0.3842628 ]\n",
      "   [  1.3693358   -5.0935154   -3.921827    -3.6555476   -1.8274075\n",
      "      1.3558773 ]\n",
      "   [  1.121359    -4.7008166   -3.0253885   -4.153024    -3.3693306\n",
      "      1.1555592 ]\n",
      "   [  2.4564793   -5.027429    -3.6513312   -4.4679847   -3.1567855\n",
      "      2.4609444 ]\n",
      "   [  2.3656955   -7.5558105   -5.6782837   -5.936429    -3.7016897\n",
      "      2.3576624 ]]\n",
      "\n",
      "  [[ -0.3909884    1.3290287    0.7153773    0.30487654   0.01067167\n",
      "     -0.37957388]\n",
      "   [  0.91534704  -2.004378    -1.3137145   -1.8292936    0.04522658\n",
      "      0.95372003]\n",
      "   [  0.84502876  -6.334351    -4.8446565   -4.5403237   -3.0086424\n",
      "      0.83118516]\n",
      "   [  1.2349606   -4.2636824   -3.542239    -3.9259446   -2.1887977\n",
      "      1.2550868 ]\n",
      "   [  2.0430071   -5.6370063   -5.307305    -5.5471854   -1.913038\n",
      "      2.0589807 ]]]], shape=(1, 8, 5, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 6)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.854575   -0.76395994  1.379787    0.08473525 -0.5442895 ]\n",
      "   [ 2.769262   -2.4458904  -0.38577634 -4.416887   -2.3424602 ]\n",
      "   [ 4.645977    0.5011021  -4.5003643  -0.7450462  -1.608111  ]\n",
      "   [-3.7169092  -4.7985697   2.3044372  11.668311    7.0816207 ]\n",
      "   [ 2.658217   -1.3918507  -3.712777    1.7937368   1.0525923 ]]\n",
      "\n",
      "  [[-0.28805032 -0.33996823  0.56497     0.339852    1.6641504 ]\n",
      "   [-0.6685229  -2.6304247  -2.186717   -1.0663583   0.05724379]\n",
      "   [ 1.9647298   0.46823615 -7.4626803  -0.66195506 -0.58121026]\n",
      "   [-3.10413    -0.5612599   1.8442365   1.6471825   1.238356  ]\n",
      "   [ 0.47162902 -1.6051086   1.3517481   0.27574015  2.1928868 ]]\n",
      "\n",
      "  [[-3.0765195   1.7763902   1.9594084  -0.04509975  1.449443  ]\n",
      "   [-4.732454    2.829565    3.985526    0.7470543   4.5167027 ]\n",
      "   [ 0.04412972  1.7056273  -0.07778924 -1.2706735   0.948514  ]\n",
      "   [-2.4218519   2.0299835  -1.2699906  -0.32161674 -0.29702076]\n",
      "   [ 1.4555037   1.0550262   1.2963976   0.7291336   4.6965575 ]]\n",
      "\n",
      "  [[-3.0387642   1.292204    3.5883346   1.1335102   1.7805948 ]\n",
      "   [-5.8640046   2.6051586   9.00416     3.7582636   6.049483  ]\n",
      "   [-2.221647   -2.3018663   0.05621783  0.84461373  2.0184433 ]\n",
      "   [-0.11405914  3.126089   10.533683   -7.5244102   0.90292376]\n",
      "   [ 1.2849315   0.889098   -0.37310356 -1.3383498   2.4830685 ]]\n",
      "\n",
      "  [[-0.61142784 -0.4549538   0.9963378  -0.14479269  0.95446485]\n",
      "   [ 3.101384   -1.9635861  -1.9723365  -4.5019665  -2.9209993 ]\n",
      "   [ 3.8482957   5.668536   -3.3393283  -2.0147512  -2.6447442 ]\n",
      "   [ 0.20807946  1.303806   -0.99223     0.8040336  -1.8219205 ]\n",
      "   [ 1.2854332   3.3654387  -4.2914696   2.0971103  -3.36987   ]]\n",
      "\n",
      "  [[-1.497981    1.4031547   0.91072094  0.3312241   0.35480243]\n",
      "   [ 1.1607789   1.7605858   0.20715673 -1.451161   -0.53180665]\n",
      "   [ 1.3948244   1.7155915   1.5817758  -1.3053533  -1.593321  ]\n",
      "   [-0.87899184 -3.5021908  -3.0559537   5.2945843  -0.22826338]\n",
      "   [ 0.926185   -0.50154155 -0.8454937   0.746904   -2.689643  ]]\n",
      "\n",
      "  [[-2.76698    -1.0661263   0.08043555  2.666324    2.835372  ]\n",
      "   [ 0.196701    0.70394504  0.4231429  -0.37788275 -0.24705143]\n",
      "   [ 2.5972507   5.5233197  -0.16624556 -2.4989765  -5.1221313 ]\n",
      "   [ 0.2208142   1.8013881  -1.5426023  -2.4432566  -0.65721875]\n",
      "   [ 2.7565699   4.56463    -1.0296626  -4.6295853  -3.6492074 ]]\n",
      "\n",
      "  [[-1.9492282   1.2290938   1.8699198  -1.1443392   0.8815833 ]\n",
      "   [-0.40289074 -0.30640975  0.74108803  2.062364   -1.4423909 ]\n",
      "   [ 0.49884918 -0.6150069  -2.8845108   5.5886726   0.24449943]\n",
      "   [-0.08005809  2.9849775   0.54212534 -3.2520006  -0.17974822]\n",
      "   [ 1.9969447  -0.11711876 -4.9909625   0.03590772  1.6269203 ]]]], shape=(1, 8, 5, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 5)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 6, 256)\n",
      "(1, 6, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "split_heads()\n",
      "(1, 6, 256)\n",
      "(1, 6, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 6, 32)\n",
      "(1, 8, 6, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 6)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.53411824  3.138721    3.4550288   2.1090372   1.3961537\n",
      "    -0.47872242]\n",
      "   [ 0.16838217 -0.68966126 -0.65448135 -0.50101525 -0.9362338\n",
      "     0.17256774]\n",
      "   [ 0.18245015 -3.0500743  -2.9301724  -2.5275035  -1.6815708\n",
      "     0.16176914]\n",
      "   [ 0.8455783  -5.5165462  -5.869892   -4.2255783  -1.9790193\n",
      "     0.8301941 ]\n",
      "   [ 1.4711927  -6.3863015  -7.2945495  -6.2027545  -1.947323\n",
      "     1.4989235 ]]\n",
      "\n",
      "  [[-0.6982859   1.8579963   0.2598044   0.11909989  0.10357708\n",
      "    -0.68354815]\n",
      "   [-0.70370317 -1.1944655  -1.8300511  -1.625776   -1.3867573\n",
      "    -0.7268194 ]\n",
      "   [ 0.8023996  -5.720824   -3.8634129  -3.597791   -1.2961257\n",
      "     0.79261404]\n",
      "   [ 0.74754465 -6.191035   -4.8284483  -4.4190493  -1.184658\n",
      "     0.7491211 ]\n",
      "   [ 1.9356661  -5.249562   -3.959453   -3.768885   -1.3811114\n",
      "     1.9547461 ]]\n",
      "\n",
      "  [[-0.7371446   0.21600349 -0.9151045  -0.87319124 -0.58509016\n",
      "    -0.6977605 ]\n",
      "   [ 0.24199572  0.91136837  0.80540216  0.90500486  0.531705\n",
      "     0.21405065]\n",
      "   [ 0.428904    3.0467944   3.654848    3.5137124   2.1350443\n",
      "     0.39721748]\n",
      "   [ 1.6770718  -3.8380928  -1.8470231  -2.2208014  -1.8292687\n",
      "     1.6466098 ]\n",
      "   [ 2.2561204  -6.790701   -4.9990864  -5.2836256  -2.855431\n",
      "     2.2273717 ]]\n",
      "\n",
      "  [[-2.016561    1.2345561  -0.22407255 -0.47709444 -0.02320303\n",
      "    -1.952554  ]\n",
      "   [ 1.6252061  -5.532724   -4.697985   -4.2196207  -1.0165944\n",
      "     1.6203778 ]\n",
      "   [ 2.4023926  -3.3139603  -1.6055428  -0.78964245  0.77323705\n",
      "     2.3298233 ]\n",
      "   [ 3.5562131  -3.931177   -1.4862784  -1.3094387  -0.5170017\n",
      "     3.5151906 ]\n",
      "   [ 3.5270443  -4.378332   -1.572719   -1.8223355  -1.122989\n",
      "     3.5242133 ]]\n",
      "\n",
      "  [[-1.13986     0.06528747 -0.5116981   0.75716764  4.4378066\n",
      "    -1.1630316 ]\n",
      "   [ 0.23661456 -2.276422   -1.5742096  -1.1921136  -1.0701761\n",
      "     0.24994968]\n",
      "   [ 1.2117268  -0.8709579   0.27299374 -0.46032202 -3.2070596\n",
      "     1.2159494 ]\n",
      "   [ 2.4224887  -2.187748   -1.8167549  -2.7981403  -5.329892\n",
      "     2.4648128 ]\n",
      "   [ 1.9925884  -2.4034123  -2.430873   -3.142742   -3.7141628\n",
      "     2.013696  ]]\n",
      "\n",
      "  [[-0.5769834   4.418479    2.9384317   2.4967303  -0.14060836\n",
      "    -0.5965953 ]\n",
      "   [ 0.4858764  -4.1729226  -2.5100694  -3.0045586  -0.95974565\n",
      "     0.49929497]\n",
      "   [ 1.2385108  -6.2040477  -5.024088   -4.462554   -1.948856\n",
      "     1.2523348 ]\n",
      "   [-0.14234024  0.4847322   1.347807    1.7450886   2.1401093\n",
      "    -0.14628264]\n",
      "   [ 1.115051   -3.375228   -3.0224636  -2.886627   -1.6582978\n",
      "     1.1338859 ]]\n",
      "\n",
      "  [[-0.44670728  3.0870311   1.6550196   1.9220535  -0.1178598\n",
      "    -0.40508384]\n",
      "   [ 0.39069507 -4.4789886  -3.7056172  -3.9325013  -0.45467767\n",
      "     0.3951269 ]\n",
      "   [ 0.73082334 -7.219702   -5.0240455  -5.9594297  -0.41063663\n",
      "     0.7376761 ]\n",
      "   [ 0.88285923 -3.030285   -0.5194961  -1.411059    1.797219\n",
      "     0.879616  ]\n",
      "   [ 0.76047105 -6.4831214  -4.6487865  -5.3101406  -1.2119142\n",
      "     0.750611  ]]\n",
      "\n",
      "  [[-1.7284982   3.5421734   3.979648    2.255315   -0.08031027\n",
      "    -1.7139624 ]\n",
      "   [ 1.8577509  -3.874339   -4.0099216  -2.2661076  -0.20281102\n",
      "     1.8486136 ]\n",
      "   [ 1.437504   -4.077823   -4.272856   -2.7402203   0.05215758\n",
      "     1.4500792 ]\n",
      "   [ 0.53686684 -3.6171033  -3.5264325  -2.0010986   0.6845382\n",
      "     0.52465266]\n",
      "   [ 1.83566    -5.138793   -4.6870475  -3.5676553  -1.629553\n",
      "     1.8333005 ]]]], shape=(1, 8, 5, 6), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 6)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "Input: 카페갈래?\n",
      "Output: 카페 데이트 좋죠 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"카페갈래?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724    3.5742848    4.0014167    0.3584842   -5.1269584 ]\n",
      "   [ -7.0726357    5.2153454    6.938354    -1.7308213   -9.474955  ]\n",
      "   [ -7.1526933    4.843132     4.2553306   -3.2136602  -10.090375  ]\n",
      "   [ -8.575991     2.398785     1.2554718   -5.392389    -9.322704  ]\n",
      "   [ -3.6484919    3.9418418    3.6611433    0.27454314  -5.605791  ]]\n",
      "\n",
      "  [[ -5.830585     4.0801115    6.8558397   -0.5618447   -5.069552  ]\n",
      "   [ -6.897297     0.31018955   4.668527    -1.2827225   -5.960391  ]\n",
      "   [ -1.8013436    0.57899565   2.3852823   -0.6738316   -1.3032598 ]\n",
      "   [ -5.3323307    0.02326641   3.5747292   -2.1570039   -4.4026885 ]\n",
      "   [ -3.74596      3.2779086    5.4336576    0.17002271  -2.9562624 ]]\n",
      "\n",
      "  [[  0.21261294   4.3947716    0.5492405    2.3503876   -1.1520578 ]\n",
      "   [ -6.2788625    1.928139     2.6955862    3.4720294   -6.945728  ]\n",
      "   [-10.524358     5.431526     3.3341048    6.6391387   -9.821377  ]\n",
      "   [ -4.77504      1.2527045    2.0886133    2.8158612   -4.505811  ]\n",
      "   [ -2.629521     5.1467323    3.2940044    3.714579    -2.745214  ]]\n",
      "\n",
      "  [[ -0.62310183   1.3455452    2.1357925    1.7603574   -1.0054102 ]\n",
      "   [ -3.257084    -0.75491565   1.108596     0.25073797  -1.6776122 ]\n",
      "   [ -5.7977004    2.4568222    5.3768396    4.0233054   -5.309523  ]\n",
      "   [ -3.168525     3.1646357    5.6924214    0.7746894   -1.4156412 ]\n",
      "   [ -1.9204345   -0.2283184    3.1195102    3.1335924   -2.5214338 ]]\n",
      "\n",
      "  [[ -3.365542     0.53381836   3.1570427   -1.8800229   -2.2221673 ]\n",
      "   [ -5.159744    -1.7272689   -0.24652402  -1.6688449   -5.015979  ]\n",
      "   [ -3.1341922    0.32078025  -0.39432356  -0.6624644   -2.736198  ]\n",
      "   [ -4.0796766   -1.6637708   -1.1532303   -2.9618354   -3.6940794 ]\n",
      "   [  0.33351463   1.4428769    1.6311829   -0.11580007   2.161247  ]]\n",
      "\n",
      "  [[ -2.1686854    0.821977    -0.5503267    0.14002563  -2.5169091 ]\n",
      "   [  1.310264     0.03412449  -1.9453832    0.2640224    0.44718844]\n",
      "   [ -4.8475094    0.92491597   3.445256     4.6040435   -6.1684012 ]\n",
      "   [ -2.8308737    2.284514     1.9508356    2.5041678   -2.817803  ]\n",
      "   [ -1.6255223    2.136444    -0.87744546   1.2968239   -2.4156199 ]]\n",
      "\n",
      "  [[ -0.3049526    0.6409757   -2.2464564   -0.1954639   -3.0315952 ]\n",
      "   [ -6.85605     -2.036426     7.826804     1.449736    -7.2996473 ]\n",
      "   [ -6.8249297   -2.2273815    4.49652      1.8047563  -10.044599  ]\n",
      "   [ -7.433311     0.0593077    6.6656246    2.2665486   -8.169536  ]\n",
      "   [ -1.8779231   -0.07863133   0.6622085    0.42292947  -3.5640974 ]]\n",
      "\n",
      "  [[ -2.8292158   -0.53708357   1.8108406    0.14664291  -1.4596796 ]\n",
      "   [ -7.561357     3.697397     5.0739865    1.405784    -9.4847765 ]\n",
      "   [ -6.0025716    0.59931606   6.7989697    2.7160077   -6.821532  ]\n",
      "   [ -6.8640413   -0.00481993   5.6336255   -0.93331844  -7.4130435 ]\n",
      "   [ -3.2595315   -0.91627586   3.6727068    2.307071    -3.3569238 ]]]], shape=(1, 8, 5, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 5)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -5.8286276    0.48856914   3.8878171    4.6285925   -4.0658426 ]\n",
      "   [ -6.320388     3.1081338    5.3603873    5.0230355   -5.1726236 ]\n",
      "   [ -6.4356365    4.4060626    6.654999     6.7385206   -5.6770105 ]\n",
      "   [ -3.6972258    2.7641807    3.415932     3.7761729   -3.2467914 ]\n",
      "   [ -5.4245625    0.4738584    3.9079578    4.555981    -3.773873  ]]\n",
      "\n",
      "  [[ -1.1487516    3.1870008    4.558113     1.7427115   -1.8127248 ]\n",
      "   [ -4.358519     5.8359137    7.518063     3.6749327   -5.224846  ]\n",
      "   [ -6.8253446    5.279299     8.066162     2.8207176   -7.583182  ]\n",
      "   [ -5.8575234    6.6365805    7.5875287    5.0424895   -5.9264965 ]\n",
      "   [ -3.9532342    4.422        6.072745     2.3031952   -4.6186533 ]]\n",
      "\n",
      "  [[ -1.7144724    4.3740845   -1.4910696   -0.25062606  -2.9232454 ]\n",
      "   [ -3.7955298    4.011021     2.4545755    0.5847195   -4.285131  ]\n",
      "   [ -4.8082285    3.8842165    3.2413545    1.0647149   -5.166344  ]\n",
      "   [ -4.403742     4.095239     2.8198972    0.47457293  -4.7439466 ]\n",
      "   [ -2.9183807    4.057358    -0.9215891    0.4505259   -3.8730795 ]]\n",
      "\n",
      "  [[  3.057866    -3.086751    -1.8377507   -1.2700328    2.9135048 ]\n",
      "   [ -4.4824758    2.3042805    3.6485844    1.5110083   -4.648266  ]\n",
      "   [ -6.6206346    4.305434     5.401879     2.8121948   -6.291019  ]\n",
      "   [ -4.9486003    1.6004934    3.3036005    1.9982697   -5.206664  ]\n",
      "   [  2.372392    -3.3788872   -1.6276515   -1.4736614    2.3492627 ]]\n",
      "\n",
      "  [[ 15.77232     -3.6245103   -1.1162255    1.7997437   17.085209  ]\n",
      "   [ -9.674349     2.9840267    1.6807344    0.57559144  -9.526209  ]\n",
      "   [ -9.235962     3.3664145    1.457442    -0.28260094  -9.635702  ]\n",
      "   [ -5.690659     3.998365     3.5633047    1.9702096   -6.532978  ]\n",
      "   [ 13.116363    -1.3242979    1.1050001    2.870215    14.000973  ]]\n",
      "\n",
      "  [[ 13.14682     -0.6089448   -4.302192     2.528658    13.664801  ]\n",
      "   [-10.302833     5.466745     9.373575     5.0930376  -10.851073  ]\n",
      "   [-10.7095995    4.9783936    8.563593     4.106375   -11.619558  ]\n",
      "   [ -9.524725     3.8366277    7.7665253    2.9146354   -9.760278  ]\n",
      "   [ 11.486847    -0.4241497   -3.5002959    2.4099183   11.720144  ]]\n",
      "\n",
      "  [[ -6.2917733    4.1096454    0.15914279   2.7929456   -5.798745  ]\n",
      "   [ -8.169023     6.311886     7.36866      6.1407843   -7.0172873 ]\n",
      "   [ -8.639621     3.6929977    7.7578864    5.70277     -6.487073  ]\n",
      "   [ -8.642758     5.533195     7.3967705    6.205844    -6.6878734 ]\n",
      "   [ -6.841967     3.4671836    1.142583     2.8404312   -6.0056434 ]]\n",
      "\n",
      "  [[ 12.039428     2.6219203    5.470266     3.5536172   13.5864    ]\n",
      "   [ -9.483304     3.7886958    5.8358936    1.8794608   -9.178464  ]\n",
      "   [-11.2674675    2.3113372    5.629478     0.6554055  -11.233968  ]\n",
      "   [ -6.4140644    1.9699202    5.995806     0.8396638   -5.6261797 ]\n",
      "   [ 11.685069     3.3350272    6.2477207    4.3434086   13.024555  ]]]], shape=(1, 8, 5, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 5)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 1)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106 ]]\n",
      "\n",
      "  [[-0.1123142 ]]\n",
      "\n",
      "  [[-0.10918517]]\n",
      "\n",
      "  [[-0.22114104]]\n",
      "\n",
      "  [[ 0.00076126]]\n",
      "\n",
      "  [[-0.00999826]]\n",
      "\n",
      "  [[-0.20863976]]\n",
      "\n",
      "  [[-0.15726982]]]], shape=(1, 8, 1, 1), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 1)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.52810717 -2.334922   -1.5888072  -1.413872    0.5603202 ]]\n",
      "\n",
      "  [[ 1.4197671  -0.08221576 -0.8403393   0.08924998  1.4623533 ]]\n",
      "\n",
      "  [[-0.89932156 -2.1984742  -2.0103471  -2.906013   -0.9244283 ]]\n",
      "\n",
      "  [[-0.5227726   0.34332338  0.3185672  -1.0342474  -0.5052332 ]]\n",
      "\n",
      "  [[-2.0420213  -3.2318969  -3.0040214  -4.9111333  -2.1105816 ]]\n",
      "\n",
      "  [[ 1.7959605  -3.8347957  -3.6654394  -3.5587516   1.7917289 ]]\n",
      "\n",
      "  [[-0.11040479 -6.0432525  -7.376758   -7.814558   -0.11433697]]\n",
      "\n",
      "  [[-0.54176444  2.138256    0.51148725  0.48711348 -0.5367518 ]]]], shape=(1, 8, 1, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 5)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 1)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7983999 ]]\n",
      "\n",
      "  [[-0.15824749]]\n",
      "\n",
      "  [[-3.11453   ]]\n",
      "\n",
      "  [[-3.088554  ]]\n",
      "\n",
      "  [[-0.696059  ]]\n",
      "\n",
      "  [[-1.4883428 ]]\n",
      "\n",
      "  [[-2.6267488 ]]\n",
      "\n",
      "  [[-1.970584  ]]]], shape=(1, 8, 1, 1), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 1)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.62861896 -0.04849994  1.2115778   0.74559855 -0.57754195]]\n",
      "\n",
      "  [[-0.59063673  2.4762337   3.1064663   2.24108    -0.5735258 ]]\n",
      "\n",
      "  [[-0.69147325  0.5903196   1.1269689   0.5912301  -0.65541637]]\n",
      "\n",
      "  [[-1.7220801   4.3272915   4.4349694   4.019365   -1.6922302 ]]\n",
      "\n",
      "  [[-1.1504039   3.7682009   3.889204    3.2394311  -1.181548  ]]\n",
      "\n",
      "  [[-0.795657    0.8276825   0.9809301   0.17181185 -0.8156157 ]]\n",
      "\n",
      "  [[-0.39662397  0.04268973  0.9355125   0.35799122 -0.33466953]]\n",
      "\n",
      "  [[-1.6156411   3.250265    6.5857873   3.9562569  -1.5591369 ]]]], shape=(1, 8, 1, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 5)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724    3.5742848    4.0014167    0.3584842   -5.1269584 ]\n",
      "   [ -7.0726357    5.2153454    6.938354    -1.7308213   -9.474955  ]\n",
      "   [ -7.1526933    4.843132     4.2553306   -3.2136602  -10.090375  ]\n",
      "   [ -8.575991     2.398785     1.2554718   -5.392389    -9.322704  ]\n",
      "   [ -3.6484919    3.9418418    3.6611433    0.27454314  -5.605791  ]]\n",
      "\n",
      "  [[ -5.830585     4.0801115    6.8558397   -0.5618447   -5.069552  ]\n",
      "   [ -6.897297     0.31018955   4.668527    -1.2827225   -5.960391  ]\n",
      "   [ -1.8013436    0.57899565   2.3852823   -0.6738316   -1.3032598 ]\n",
      "   [ -5.3323307    0.02326641   3.5747292   -2.1570039   -4.4026885 ]\n",
      "   [ -3.74596      3.2779086    5.4336576    0.17002271  -2.9562624 ]]\n",
      "\n",
      "  [[  0.21261294   4.3947716    0.5492405    2.3503876   -1.1520578 ]\n",
      "   [ -6.2788625    1.928139     2.6955862    3.4720294   -6.945728  ]\n",
      "   [-10.524358     5.431526     3.3341048    6.6391387   -9.821377  ]\n",
      "   [ -4.77504      1.2527045    2.0886133    2.8158612   -4.505811  ]\n",
      "   [ -2.629521     5.1467323    3.2940044    3.714579    -2.745214  ]]\n",
      "\n",
      "  [[ -0.62310183   1.3455452    2.1357925    1.7603574   -1.0054102 ]\n",
      "   [ -3.257084    -0.75491565   1.108596     0.25073797  -1.6776122 ]\n",
      "   [ -5.7977004    2.4568222    5.3768396    4.0233054   -5.309523  ]\n",
      "   [ -3.168525     3.1646357    5.6924214    0.7746894   -1.4156412 ]\n",
      "   [ -1.9204345   -0.2283184    3.1195102    3.1335924   -2.5214338 ]]\n",
      "\n",
      "  [[ -3.365542     0.53381836   3.1570427   -1.8800229   -2.2221673 ]\n",
      "   [ -5.159744    -1.7272689   -0.24652402  -1.6688449   -5.015979  ]\n",
      "   [ -3.1341922    0.32078025  -0.39432356  -0.6624644   -2.736198  ]\n",
      "   [ -4.0796766   -1.6637708   -1.1532303   -2.9618354   -3.6940794 ]\n",
      "   [  0.33351463   1.4428769    1.6311829   -0.11580007   2.161247  ]]\n",
      "\n",
      "  [[ -2.1686854    0.821977    -0.5503267    0.14002563  -2.5169091 ]\n",
      "   [  1.310264     0.03412449  -1.9453832    0.2640224    0.44718844]\n",
      "   [ -4.8475094    0.92491597   3.445256     4.6040435   -6.1684012 ]\n",
      "   [ -2.8308737    2.284514     1.9508356    2.5041678   -2.817803  ]\n",
      "   [ -1.6255223    2.136444    -0.87744546   1.2968239   -2.4156199 ]]\n",
      "\n",
      "  [[ -0.3049526    0.6409757   -2.2464564   -0.1954639   -3.0315952 ]\n",
      "   [ -6.85605     -2.036426     7.826804     1.449736    -7.2996473 ]\n",
      "   [ -6.8249297   -2.2273815    4.49652      1.8047563  -10.044599  ]\n",
      "   [ -7.433311     0.0593077    6.6656246    2.2665486   -8.169536  ]\n",
      "   [ -1.8779231   -0.07863133   0.6622085    0.42292947  -3.5640974 ]]\n",
      "\n",
      "  [[ -2.8292158   -0.53708357   1.8108406    0.14664291  -1.4596796 ]\n",
      "   [ -7.561357     3.697397     5.0739865    1.405784    -9.4847765 ]\n",
      "   [ -6.0025716    0.59931606   6.7989697    2.7160077   -6.821532  ]\n",
      "   [ -6.8640413   -0.00481993   5.6336255   -0.93331844  -7.4130435 ]\n",
      "   [ -3.2595315   -0.91627586   3.6727068    2.307071    -3.3569238 ]]]], shape=(1, 8, 5, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 5)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -5.8286276    0.48856914   3.8878171    4.6285925   -4.0658426 ]\n",
      "   [ -6.320388     3.1081338    5.3603873    5.0230355   -5.1726236 ]\n",
      "   [ -6.4356365    4.4060626    6.654999     6.7385206   -5.6770105 ]\n",
      "   [ -3.6972258    2.7641807    3.415932     3.7761729   -3.2467914 ]\n",
      "   [ -5.4245625    0.4738584    3.9079578    4.555981    -3.773873  ]]\n",
      "\n",
      "  [[ -1.1487516    3.1870008    4.558113     1.7427115   -1.8127248 ]\n",
      "   [ -4.358519     5.8359137    7.518063     3.6749327   -5.224846  ]\n",
      "   [ -6.8253446    5.279299     8.066162     2.8207176   -7.583182  ]\n",
      "   [ -5.8575234    6.6365805    7.5875287    5.0424895   -5.9264965 ]\n",
      "   [ -3.9532342    4.422        6.072745     2.3031952   -4.6186533 ]]\n",
      "\n",
      "  [[ -1.7144724    4.3740845   -1.4910696   -0.25062606  -2.9232454 ]\n",
      "   [ -3.7955298    4.011021     2.4545755    0.5847195   -4.285131  ]\n",
      "   [ -4.8082285    3.8842165    3.2413545    1.0647149   -5.166344  ]\n",
      "   [ -4.403742     4.095239     2.8198972    0.47457293  -4.7439466 ]\n",
      "   [ -2.9183807    4.057358    -0.9215891    0.4505259   -3.8730795 ]]\n",
      "\n",
      "  [[  3.057866    -3.086751    -1.8377507   -1.2700328    2.9135048 ]\n",
      "   [ -4.4824758    2.3042805    3.6485844    1.5110083   -4.648266  ]\n",
      "   [ -6.6206346    4.305434     5.401879     2.8121948   -6.291019  ]\n",
      "   [ -4.9486003    1.6004934    3.3036005    1.9982697   -5.206664  ]\n",
      "   [  2.372392    -3.3788872   -1.6276515   -1.4736614    2.3492627 ]]\n",
      "\n",
      "  [[ 15.77232     -3.6245103   -1.1162255    1.7997437   17.085209  ]\n",
      "   [ -9.674349     2.9840267    1.6807344    0.57559144  -9.526209  ]\n",
      "   [ -9.235962     3.3664145    1.457442    -0.28260094  -9.635702  ]\n",
      "   [ -5.690659     3.998365     3.5633047    1.9702096   -6.532978  ]\n",
      "   [ 13.116363    -1.3242979    1.1050001    2.870215    14.000973  ]]\n",
      "\n",
      "  [[ 13.14682     -0.6089448   -4.302192     2.528658    13.664801  ]\n",
      "   [-10.302833     5.466745     9.373575     5.0930376  -10.851073  ]\n",
      "   [-10.7095995    4.9783936    8.563593     4.106375   -11.619558  ]\n",
      "   [ -9.524725     3.8366277    7.7665253    2.9146354   -9.760278  ]\n",
      "   [ 11.486847    -0.4241497   -3.5002959    2.4099183   11.720144  ]]\n",
      "\n",
      "  [[ -6.2917733    4.1096454    0.15914279   2.7929456   -5.798745  ]\n",
      "   [ -8.169023     6.311886     7.36866      6.1407843   -7.0172873 ]\n",
      "   [ -8.639621     3.6929977    7.7578864    5.70277     -6.487073  ]\n",
      "   [ -8.642758     5.533195     7.3967705    6.205844    -6.6878734 ]\n",
      "   [ -6.841967     3.4671836    1.142583     2.8404312   -6.0056434 ]]\n",
      "\n",
      "  [[ 12.039428     2.6219203    5.470266     3.5536172   13.5864    ]\n",
      "   [ -9.483304     3.7886958    5.8358936    1.8794608   -9.178464  ]\n",
      "   [-11.2674675    2.3113372    5.629478     0.6554055  -11.233968  ]\n",
      "   [ -6.4140644    1.9699202    5.995806     0.8396638   -5.6261797 ]\n",
      "   [ 11.685069     3.3350272    6.2477207    4.3434086   13.024555  ]]]], shape=(1, 8, 5, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 5)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 2)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.7901088 ]\n",
      "   [-0.76940614  1.6768681 ]]\n",
      "\n",
      "  [[-0.1123142   0.6087259 ]\n",
      "   [ 0.19425972 -6.067308  ]]\n",
      "\n",
      "  [[-0.10918517  0.356523  ]\n",
      "   [ 0.3992877   1.4783919 ]]\n",
      "\n",
      "  [[-0.22114104  0.5692701 ]\n",
      "   [-0.14270139 -2.2575488 ]]\n",
      "\n",
      "  [[ 0.00076126 -0.71471184]\n",
      "   [ 1.077351   -2.9682596 ]]\n",
      "\n",
      "  [[-0.00999826 -0.16768762]\n",
      "   [ 1.2585764  -6.5457373 ]]\n",
      "\n",
      "  [[-0.20863976 -0.19510399]\n",
      "   [-0.72930324  0.9806623 ]]\n",
      "\n",
      "  [[-0.15726982 -0.70173645]\n",
      "   [ 0.04178004 -3.564748  ]]]], shape=(1, 8, 2, 2), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 2)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.52810717 -2.334922   -1.5888072  -1.413872    0.5603202 ]\n",
      "   [-0.02618265  2.2979825   2.6186545   2.3236623  -0.04095899]]\n",
      "\n",
      "  [[ 1.4197671  -0.08221576 -0.8403393   0.08924998  1.4623533 ]\n",
      "   [-1.0561928   0.33323586 -0.09222951 -0.44838938 -1.116985  ]]\n",
      "\n",
      "  [[-0.89932156 -2.1984742  -2.0103471  -2.906013   -0.9244283 ]\n",
      "   [ 0.23406678 -2.5802658  -3.6686654  -2.7507718   0.24913327]]\n",
      "\n",
      "  [[-0.5227726   0.34332338  0.3185672  -1.0342474  -0.5052332 ]\n",
      "   [ 0.8306065  -2.283532   -2.8600383  -2.035774    0.84244555]]\n",
      "\n",
      "  [[-2.0420213  -3.2318969  -3.0040214  -4.9111333  -2.1105816 ]\n",
      "   [-0.71993935 -1.2842261  -2.3088906  -2.356233   -0.7477684 ]]\n",
      "\n",
      "  [[ 1.7959605  -3.8347957  -3.6654394  -3.5587516   1.7917289 ]\n",
      "   [-1.8957531   0.26343462 -0.19008406 -0.47779578 -1.8996587 ]]\n",
      "\n",
      "  [[-0.11040479 -6.0432525  -7.376758   -7.814558   -0.11433697]\n",
      "   [ 0.93817455  1.7784356   1.970838    3.1472101   0.9584705 ]]\n",
      "\n",
      "  [[-0.54176444  2.138256    0.51148725  0.48711348 -0.5367518 ]\n",
      "   [-1.0186511   3.0361657   2.4281244   1.4078753  -1.0258018 ]]]], shape=(1, 8, 2, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 5)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 2)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7983999   0.44223335]\n",
      "   [-3.6629524   9.147647  ]]\n",
      "\n",
      "  [[-0.15824749  0.4709822 ]\n",
      "   [-1.3495775   1.082988  ]]\n",
      "\n",
      "  [[-3.11453    -0.566625  ]\n",
      "   [-2.2823813  -1.9889657 ]]\n",
      "\n",
      "  [[-3.088554    0.33684313]\n",
      "   [-2.4640126   0.49281952]]\n",
      "\n",
      "  [[-0.696059   -0.8078529 ]\n",
      "   [ 1.132871    1.506294  ]]\n",
      "\n",
      "  [[-1.4883428   0.27744588]\n",
      "   [-1.8928543   4.774709  ]]\n",
      "\n",
      "  [[-2.6267488   1.5878271 ]\n",
      "   [-1.3783387  -0.18541397]]\n",
      "\n",
      "  [[-1.970584   -0.5466678 ]\n",
      "   [-0.6295914  -4.552605  ]]]], shape=(1, 8, 2, 2), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 2)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.62861896 -0.04849994  1.2115778   0.74559855 -0.57754195]\n",
      "   [ 0.26620185 -1.1088337  -0.7850282  -1.1183405   0.24657819]]\n",
      "\n",
      "  [[-0.59063673  2.4762337   3.1064663   2.24108    -0.5735258 ]\n",
      "   [-0.4162522  -1.8400384  -1.3752189  -1.9507186  -0.4073767 ]]\n",
      "\n",
      "  [[-0.69147325  0.5903196   1.1269689   0.5912301  -0.65541637]\n",
      "   [ 0.7250384   0.7786316   1.3235878   1.9657416   0.7495992 ]]\n",
      "\n",
      "  [[-1.7220801   4.3272915   4.4349694   4.019365   -1.6922302 ]\n",
      "   [ 1.6751902  -7.1754317  -7.106475   -6.093101    1.6769656 ]]\n",
      "\n",
      "  [[-1.1504039   3.7682009   3.889204    3.2394311  -1.181548  ]\n",
      "   [ 0.66782284  1.1060889   1.4097515   1.4414368   0.70379174]]\n",
      "\n",
      "  [[-0.795657    0.8276825   0.9809301   0.17181185 -0.8156157 ]\n",
      "   [-1.7270175  -1.5389717  -0.6122948  -1.4230796  -1.7801974 ]]\n",
      "\n",
      "  [[-0.39662397  0.04268973  0.9355125   0.35799122 -0.33466953]\n",
      "   [ 0.1256398  -0.78801465 -1.4515224  -0.59075534  0.11255832]]\n",
      "\n",
      "  [[-1.6156411   3.250265    6.5857873   3.9562569  -1.5591369 ]\n",
      "   [-1.8046395   0.7688393   2.7692447   0.8544726  -1.786873  ]]]], shape=(1, 8, 2, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 5)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -3.5907724    3.5742848    4.0014167    0.3584842   -5.1269584 ]\n",
      "   [ -7.0726357    5.2153454    6.938354    -1.7308213   -9.474955  ]\n",
      "   [ -7.1526933    4.843132     4.2553306   -3.2136602  -10.090375  ]\n",
      "   [ -8.575991     2.398785     1.2554718   -5.392389    -9.322704  ]\n",
      "   [ -3.6484919    3.9418418    3.6611433    0.27454314  -5.605791  ]]\n",
      "\n",
      "  [[ -5.830585     4.0801115    6.8558397   -0.5618447   -5.069552  ]\n",
      "   [ -6.897297     0.31018955   4.668527    -1.2827225   -5.960391  ]\n",
      "   [ -1.8013436    0.57899565   2.3852823   -0.6738316   -1.3032598 ]\n",
      "   [ -5.3323307    0.02326641   3.5747292   -2.1570039   -4.4026885 ]\n",
      "   [ -3.74596      3.2779086    5.4336576    0.17002271  -2.9562624 ]]\n",
      "\n",
      "  [[  0.21261294   4.3947716    0.5492405    2.3503876   -1.1520578 ]\n",
      "   [ -6.2788625    1.928139     2.6955862    3.4720294   -6.945728  ]\n",
      "   [-10.524358     5.431526     3.3341048    6.6391387   -9.821377  ]\n",
      "   [ -4.77504      1.2527045    2.0886133    2.8158612   -4.505811  ]\n",
      "   [ -2.629521     5.1467323    3.2940044    3.714579    -2.745214  ]]\n",
      "\n",
      "  [[ -0.62310183   1.3455452    2.1357925    1.7603574   -1.0054102 ]\n",
      "   [ -3.257084    -0.75491565   1.108596     0.25073797  -1.6776122 ]\n",
      "   [ -5.7977004    2.4568222    5.3768396    4.0233054   -5.309523  ]\n",
      "   [ -3.168525     3.1646357    5.6924214    0.7746894   -1.4156412 ]\n",
      "   [ -1.9204345   -0.2283184    3.1195102    3.1335924   -2.5214338 ]]\n",
      "\n",
      "  [[ -3.365542     0.53381836   3.1570427   -1.8800229   -2.2221673 ]\n",
      "   [ -5.159744    -1.7272689   -0.24652402  -1.6688449   -5.015979  ]\n",
      "   [ -3.1341922    0.32078025  -0.39432356  -0.6624644   -2.736198  ]\n",
      "   [ -4.0796766   -1.6637708   -1.1532303   -2.9618354   -3.6940794 ]\n",
      "   [  0.33351463   1.4428769    1.6311829   -0.11580007   2.161247  ]]\n",
      "\n",
      "  [[ -2.1686854    0.821977    -0.5503267    0.14002563  -2.5169091 ]\n",
      "   [  1.310264     0.03412449  -1.9453832    0.2640224    0.44718844]\n",
      "   [ -4.8475094    0.92491597   3.445256     4.6040435   -6.1684012 ]\n",
      "   [ -2.8308737    2.284514     1.9508356    2.5041678   -2.817803  ]\n",
      "   [ -1.6255223    2.136444    -0.87744546   1.2968239   -2.4156199 ]]\n",
      "\n",
      "  [[ -0.3049526    0.6409757   -2.2464564   -0.1954639   -3.0315952 ]\n",
      "   [ -6.85605     -2.036426     7.826804     1.449736    -7.2996473 ]\n",
      "   [ -6.8249297   -2.2273815    4.49652      1.8047563  -10.044599  ]\n",
      "   [ -7.433311     0.0593077    6.6656246    2.2665486   -8.169536  ]\n",
      "   [ -1.8779231   -0.07863133   0.6622085    0.42292947  -3.5640974 ]]\n",
      "\n",
      "  [[ -2.8292158   -0.53708357   1.8108406    0.14664291  -1.4596796 ]\n",
      "   [ -7.561357     3.697397     5.0739865    1.405784    -9.4847765 ]\n",
      "   [ -6.0025716    0.59931606   6.7989697    2.7160077   -6.821532  ]\n",
      "   [ -6.8640413   -0.00481993   5.6336255   -0.93331844  -7.4130435 ]\n",
      "   [ -3.2595315   -0.91627586   3.6727068    2.307071    -3.3569238 ]]]], shape=(1, 8, 5, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 5)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 5, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -5.8286276    0.48856914   3.8878171    4.6285925   -4.0658426 ]\n",
      "   [ -6.320388     3.1081338    5.3603873    5.0230355   -5.1726236 ]\n",
      "   [ -6.4356365    4.4060626    6.654999     6.7385206   -5.6770105 ]\n",
      "   [ -3.6972258    2.7641807    3.415932     3.7761729   -3.2467914 ]\n",
      "   [ -5.4245625    0.4738584    3.9079578    4.555981    -3.773873  ]]\n",
      "\n",
      "  [[ -1.1487516    3.1870008    4.558113     1.7427115   -1.8127248 ]\n",
      "   [ -4.358519     5.8359137    7.518063     3.6749327   -5.224846  ]\n",
      "   [ -6.8253446    5.279299     8.066162     2.8207176   -7.583182  ]\n",
      "   [ -5.8575234    6.6365805    7.5875287    5.0424895   -5.9264965 ]\n",
      "   [ -3.9532342    4.422        6.072745     2.3031952   -4.6186533 ]]\n",
      "\n",
      "  [[ -1.7144724    4.3740845   -1.4910696   -0.25062606  -2.9232454 ]\n",
      "   [ -3.7955298    4.011021     2.4545755    0.5847195   -4.285131  ]\n",
      "   [ -4.8082285    3.8842165    3.2413545    1.0647149   -5.166344  ]\n",
      "   [ -4.403742     4.095239     2.8198972    0.47457293  -4.7439466 ]\n",
      "   [ -2.9183807    4.057358    -0.9215891    0.4505259   -3.8730795 ]]\n",
      "\n",
      "  [[  3.057866    -3.086751    -1.8377507   -1.2700328    2.9135048 ]\n",
      "   [ -4.4824758    2.3042805    3.6485844    1.5110083   -4.648266  ]\n",
      "   [ -6.6206346    4.305434     5.401879     2.8121948   -6.291019  ]\n",
      "   [ -4.9486003    1.6004934    3.3036005    1.9982697   -5.206664  ]\n",
      "   [  2.372392    -3.3788872   -1.6276515   -1.4736614    2.3492627 ]]\n",
      "\n",
      "  [[ 15.77232     -3.6245103   -1.1162255    1.7997437   17.085209  ]\n",
      "   [ -9.674349     2.9840267    1.6807344    0.57559144  -9.526209  ]\n",
      "   [ -9.235962     3.3664145    1.457442    -0.28260094  -9.635702  ]\n",
      "   [ -5.690659     3.998365     3.5633047    1.9702096   -6.532978  ]\n",
      "   [ 13.116363    -1.3242979    1.1050001    2.870215    14.000973  ]]\n",
      "\n",
      "  [[ 13.14682     -0.6089448   -4.302192     2.528658    13.664801  ]\n",
      "   [-10.302833     5.466745     9.373575     5.0930376  -10.851073  ]\n",
      "   [-10.7095995    4.9783936    8.563593     4.106375   -11.619558  ]\n",
      "   [ -9.524725     3.8366277    7.7665253    2.9146354   -9.760278  ]\n",
      "   [ 11.486847    -0.4241497   -3.5002959    2.4099183   11.720144  ]]\n",
      "\n",
      "  [[ -6.2917733    4.1096454    0.15914279   2.7929456   -5.798745  ]\n",
      "   [ -8.169023     6.311886     7.36866      6.1407843   -7.0172873 ]\n",
      "   [ -8.639621     3.6929977    7.7578864    5.70277     -6.487073  ]\n",
      "   [ -8.642758     5.533195     7.3967705    6.205844    -6.6878734 ]\n",
      "   [ -6.841967     3.4671836    1.142583     2.8404312   -6.0056434 ]]\n",
      "\n",
      "  [[ 12.039428     2.6219203    5.470266     3.5536172   13.5864    ]\n",
      "   [ -9.483304     3.7886958    5.8358936    1.8794608   -9.178464  ]\n",
      "   [-11.2674675    2.3113372    5.629478     0.6554055  -11.233968  ]\n",
      "   [ -6.4140644    1.9699202    5.995806     0.8396638   -5.6261797 ]\n",
      "   [ 11.685069     3.3350272    6.2477207    4.3434086   13.024555  ]]]], shape=(1, 8, 5, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 5, 5)\n",
      "output.shape = (1, 8, 5, 32)\n",
      "scaled_attention.shape= (1, 5, 8, 32)\n",
      "concat_attention.shape= (1, 5, 256)\n",
      "outputs.shape= (1, 5, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 3)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   0.7901088   1.3238052 ]\n",
      "   [-0.76940614  1.6768681  -3.290495  ]\n",
      "   [ 0.5496094  -1.13541     6.0063386 ]]\n",
      "\n",
      "  [[-0.1123142   0.6087259  -0.41297778]\n",
      "   [ 0.19425972 -6.067308    0.69275564]\n",
      "   [-0.2611823   0.4745637   7.361336  ]]\n",
      "\n",
      "  [[-0.10918517  0.356523    0.21361528]\n",
      "   [ 0.3992877   1.4783919  -2.7050529 ]\n",
      "   [ 0.13678889 -0.09397719  0.3511634 ]]\n",
      "\n",
      "  [[-0.22114104  0.5692701   0.4747489 ]\n",
      "   [-0.14270139 -2.2575488  -1.478206  ]\n",
      "   [-0.9603355  -0.581231    6.6998744 ]]\n",
      "\n",
      "  [[ 0.00076126 -0.71471184  0.4193116 ]\n",
      "   [ 1.077351   -2.9682596   2.1617749 ]\n",
      "   [-0.15525573  1.1912408   0.5077643 ]]\n",
      "\n",
      "  [[-0.00999826 -0.16768762  0.24226059]\n",
      "   [ 1.2585764  -6.5457373   0.698316  ]\n",
      "   [-0.39468518 -0.22899467  3.695164  ]]\n",
      "\n",
      "  [[-0.20863976 -0.19510399  0.13930243]\n",
      "   [-0.72930324  0.9806623  -0.5720963 ]\n",
      "   [ 0.7417437  -0.7452136   8.153973  ]]\n",
      "\n",
      "  [[-0.15726982 -0.70173645  0.24089351]\n",
      "   [ 0.04178004 -3.564748    1.748515  ]\n",
      "   [ 0.406222   -5.2016506  10.808965  ]]]], shape=(1, 8, 3, 3), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 3)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.52810717 -2.334922   -1.5888072  -1.413872    0.5603202 ]\n",
      "   [-0.02618265  2.2979825   2.6186545   2.3236623  -0.04095899]\n",
      "   [ 1.9943576  -4.446302   -3.2967424  -3.8098876   2.015973  ]]\n",
      "\n",
      "  [[ 1.4197671  -0.08221576 -0.8403393   0.08924998  1.4623533 ]\n",
      "   [-1.0561928   0.33323586 -0.09222951 -0.44838938 -1.116985  ]\n",
      "   [ 1.8548565  -1.865143   -2.4549944  -0.81113684  1.8243705 ]]\n",
      "\n",
      "  [[-0.89932156 -2.1984742  -2.0103471  -2.906013   -0.9244283 ]\n",
      "   [ 0.23406678 -2.5802658  -3.6686654  -2.7507718   0.24913327]\n",
      "   [ 2.5397096  -2.203552   -2.123191   -2.0547543   2.5196896 ]]\n",
      "\n",
      "  [[-0.5227726   0.34332338  0.3185672  -1.0342474  -0.5052332 ]\n",
      "   [ 0.8306065  -2.283532   -2.8600383  -2.035774    0.84244555]\n",
      "   [ 1.4909925  -5.5046453  -5.022799   -5.664507    1.5284084 ]]\n",
      "\n",
      "  [[-2.0420213  -3.2318969  -3.0040214  -4.9111333  -2.1105816 ]\n",
      "   [-0.71993935 -1.2842261  -2.3088906  -2.356233   -0.7477684 ]\n",
      "   [ 3.2118213  -4.8083224  -4.7858562  -2.642545    3.2865465 ]]\n",
      "\n",
      "  [[ 1.7959605  -3.8347957  -3.6654394  -3.5587516   1.7917289 ]\n",
      "   [-1.8957531   0.26343462 -0.19008406 -0.47779578 -1.8996587 ]\n",
      "   [ 2.481912   -3.4206028  -3.449921   -2.8012269   2.4752808 ]]\n",
      "\n",
      "  [[-0.11040479 -6.0432525  -7.376758   -7.814558   -0.11433697]\n",
      "   [ 0.93817455  1.7784356   1.970838    3.1472101   0.9584705 ]\n",
      "   [ 2.6929116  -1.2383124  -1.261278   -0.94807476  2.68891   ]]\n",
      "\n",
      "  [[-0.54176444  2.138256    0.51148725  0.48711348 -0.5367518 ]\n",
      "   [-1.0186511   3.0361657   2.4281244   1.4078753  -1.0258018 ]\n",
      "   [ 2.224274   -5.513219   -5.196843   -3.774605    2.2531314 ]]]], shape=(1, 8, 3, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 5)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7983999   0.44223335 -0.7938965 ]\n",
      "   [-3.6629524   9.147647    4.6191163 ]\n",
      "   [ 3.7037299  -0.4735921  -0.9723647 ]]\n",
      "\n",
      "  [[-0.15824749  0.4709822   2.0054326 ]\n",
      "   [-1.3495775   1.082988    1.9059572 ]\n",
      "   [ 0.78809536  0.78308237  1.2840126 ]]\n",
      "\n",
      "  [[-3.11453    -0.566625    2.1476986 ]\n",
      "   [-2.2823813  -1.9889657   0.4346674 ]\n",
      "   [ 0.18850498 -1.9162387   7.6928477 ]]\n",
      "\n",
      "  [[-3.088554    0.33684313  2.0133615 ]\n",
      "   [-2.4640126   0.49281952  1.0701021 ]\n",
      "   [-0.03423405  0.50429946  3.213768  ]]\n",
      "\n",
      "  [[-0.696059   -0.8078529   1.0818917 ]\n",
      "   [ 1.132871    1.506294   -1.5998688 ]\n",
      "   [ 1.9528512   1.8914231  -1.5582881 ]]\n",
      "\n",
      "  [[-1.4883428   0.27744588  0.18821557]\n",
      "   [-1.8928543   4.774709    0.6223393 ]\n",
      "   [ 1.1962873   0.06909335 -2.540665  ]]\n",
      "\n",
      "  [[-2.6267488   1.5878271   2.2988904 ]\n",
      "   [-1.3783387  -0.18541397  1.179133  ]\n",
      "   [ 3.513745   -4.8112173  -3.9981768 ]]\n",
      "\n",
      "  [[-1.970584   -0.5466678   0.82418245]\n",
      "   [-0.6295914  -4.552605    0.92674136]\n",
      "   [ 1.634149   -3.0894818   1.9762158 ]]]], shape=(1, 8, 3, 3), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 3)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 5, 256)\n",
      "(1, 5, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "split_heads()\n",
      "(1, 5, 256)\n",
      "(1, 5, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 5, 32)\n",
      "(1, 8, 5, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 5)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.62861896  -0.04849994   1.2115778    0.74559855  -0.57754195]\n",
      "   [  0.26620185  -1.1088337   -0.7850282   -1.1183405    0.24657819]\n",
      "   [  1.9611688    2.3447301    1.8050878    2.5794632    1.9673446 ]]\n",
      "\n",
      "  [[ -0.59063673   2.4762337    3.1064663    2.24108     -0.5735258 ]\n",
      "   [ -0.4162522   -1.8400384   -1.3752189   -1.9507186   -0.4073767 ]\n",
      "   [  1.1570545   -9.944669   -10.0214205   -9.338583     1.1565827 ]]\n",
      "\n",
      "  [[ -0.69147325   0.5903196    1.1269689    0.5912301   -0.65541637]\n",
      "   [  0.7250384    0.7786316    1.3235878    1.9657416    0.7495992 ]\n",
      "   [  2.4206886   -1.3566306   -0.8806059   -0.62245697   2.4148598 ]]\n",
      "\n",
      "  [[ -1.7220801    4.3272915    4.4349694    4.019365    -1.6922302 ]\n",
      "   [  1.6751902   -7.1754317   -7.106475    -6.093101     1.6769656 ]\n",
      "   [  3.5031483   -4.9672947   -4.4583206   -3.737235     3.534484  ]]\n",
      "\n",
      "  [[ -1.1504039    3.7682009    3.889204     3.2394311   -1.181548  ]\n",
      "   [  0.66782284   1.1060889    1.4097515    1.4414368    0.70379174]\n",
      "   [  1.4013836   -1.1752182   -1.4265712   -0.772269     1.43565   ]]\n",
      "\n",
      "  [[ -0.795657     0.8276825    0.9809301    0.17181185  -0.8156157 ]\n",
      "   [ -1.7270175   -1.5389717   -0.6122948   -1.4230796   -1.7801974 ]\n",
      "   [  0.95013833  -1.8747895   -2.03285     -1.8078911    0.99573493]]\n",
      "\n",
      "  [[ -0.39662397   0.04268973   0.9355125    0.35799122  -0.33466953]\n",
      "   [  0.1256398   -0.78801465  -1.4515224   -0.59075534   0.11255832]\n",
      "   [  0.69250655  -2.6106906   -4.1078205   -2.9906416    0.6361499 ]]\n",
      "\n",
      "  [[ -1.6156411    3.250265     6.5857873    3.9562569   -1.5591369 ]\n",
      "   [ -1.8046395    0.7688393    2.7692447    0.8544726   -1.786873  ]\n",
      "   [  2.9438605   -6.1034803   -9.186896    -5.830798     2.9098804 ]]]], shape=(1, 8, 3, 5), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 5)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "Input: 게임하고싶당\n",
      "Output: 저도요 !\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"게임하고싶당\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   3.5742848   0.47916737 -4.1808095 ]\n",
      "   [-7.0726357   5.2153454  -0.6191547  -7.686931  ]\n",
      "   [-5.0196514   3.5095148   0.26564068 -4.7125306 ]\n",
      "   [-2.6520565   4.155355    1.5553874  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    4.0801115  -0.8106586  -5.8514013 ]\n",
      "   [-6.897297    0.31018955  3.5017562  -6.787184  ]\n",
      "   [-1.6330746   2.2395508  -4.2915373  -0.98903894]\n",
      "   [-3.889495    4.1714396  -0.17852122 -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  4.3947716  -0.661001   -1.4663465 ]\n",
      "   [-6.2788625   1.928139   -0.8061733  -6.9638243 ]\n",
      "   [-7.623632    4.3519716   0.6232831  -7.3906612 ]\n",
      "   [-3.4774246   5.1761966   3.0670707  -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183  1.3455452   1.0637511  -1.0099144 ]\n",
      "   [-3.257084   -0.75491565  3.5904186  -1.9644443 ]\n",
      "   [-4.4190965   2.565092   -0.38624918 -3.4415853 ]\n",
      "   [-1.8414911  -0.91760015  0.36751118 -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    0.53381836 -2.409985   -1.7994132 ]\n",
      "   [-5.159744   -1.7272689  -3.6887574  -5.157165  ]\n",
      "   [-8.038345   -0.19349298 -5.0958123  -6.112233  ]\n",
      "   [ 0.2971933   1.319021    0.11702798  2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.821977   -2.6579876  -2.5024278 ]\n",
      "   [ 1.310264    0.03412449  2.41719     0.71974283]\n",
      "   [-2.0380263   2.2992606  -1.3999702  -1.9594809 ]\n",
      "   [-1.399356    2.0196993  -0.5995703  -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.6409757   1.2338101  -3.0664551 ]\n",
      "   [-6.85605    -2.036426    5.2613115  -5.019433  ]\n",
      "   [-5.874989   -0.90901834  2.837893   -6.5762978 ]\n",
      "   [-2.1759007  -0.67548347  3.5331502  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158  -0.53708357 -0.7094948  -2.1313064 ]\n",
      "   [-7.561357    3.697397   -4.1952496  -9.132537  ]\n",
      "   [-5.8729873  -0.09437667 -4.60697    -5.5851054 ]\n",
      "   [-4.791121   -0.5720516  -1.453231   -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.2474937   -0.41418695  -0.45134023  -6.4543934 ]\n",
      "   [ -6.469707    -0.57196766  -0.68407536  -8.534681  ]\n",
      "   [ -6.6292477   -0.28188297  -1.0462365   -8.814849  ]\n",
      "   [ -6.6225      -0.91170895  -1.4528327   -6.590887  ]]\n",
      "\n",
      "  [[  1.406956     1.0076551    5.707879     1.6527363 ]\n",
      "   [ -4.613382     4.554805     1.008847    -5.670259  ]\n",
      "   [ -3.934454     4.793562     1.5221102   -4.984705  ]\n",
      "   [ -2.0566604    1.2764972    4.5498753   -1.9361823 ]]\n",
      "\n",
      "  [[  1.1651802   11.755798     8.471589    -0.8650375 ]\n",
      "   [ -4.827062     5.9522486    3.516461    -6.2152934 ]\n",
      "   [ -4.54758      4.9799747    2.9965365   -5.7537866 ]\n",
      "   [ -0.7047034   11.391521     7.993846    -2.7024608 ]]\n",
      "\n",
      "  [[  1.4051645   -1.5436141   -0.61915714   2.300935  ]\n",
      "   [ -7.481064    -1.326507    -1.9729244   -8.849386  ]\n",
      "   [ -4.8875756   -0.8316178   -1.230764    -5.399683  ]\n",
      "   [  1.8524147   -3.3698123   -1.3004444    2.8076603 ]]\n",
      "\n",
      "  [[ 14.038909    -1.1073301    4.4311037   14.881157  ]\n",
      "   [-15.253063     7.0223446    0.30012316 -17.242483  ]\n",
      "   [-10.496829     6.2392097    1.8075695  -12.553988  ]\n",
      "   [ 11.9622965   -0.8325739    3.342078    11.930058  ]]\n",
      "\n",
      "  [[ 13.658282     1.1466336    4.6133533   13.644118  ]\n",
      "   [-11.285459     0.57440215  -0.90991193 -13.207878  ]\n",
      "   [ -4.382489     1.1987709    1.0016749   -6.2198586 ]\n",
      "   [ 13.9025755    2.6008193    5.8075957   13.900225  ]]\n",
      "\n",
      "  [[ -7.06424      1.0761411   -5.276111    -7.843285  ]\n",
      "   [ -7.1069236    1.9375304   -0.12720326  -7.099851  ]\n",
      "   [ -7.5099497    1.1038859   -1.7704768   -7.6377015 ]\n",
      "   [ -6.981106     1.2045658   -5.2073503   -7.964103  ]]\n",
      "\n",
      "  [[ 12.859538    -0.9277863    3.4094594   15.096983  ]\n",
      "   [ -7.5078464    4.23272      0.7433306   -9.381615  ]\n",
      "   [ -8.053277     3.265487     0.34209594  -8.793779  ]\n",
      "   [ 11.912383    -0.27324596   3.9327502   14.192233  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 1)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106 ]]\n",
      "\n",
      "  [[-0.1123142 ]]\n",
      "\n",
      "  [[-0.10918517]]\n",
      "\n",
      "  [[-0.22114104]]\n",
      "\n",
      "  [[ 0.00076126]]\n",
      "\n",
      "  [[-0.00999826]]\n",
      "\n",
      "  [[-0.20863976]]\n",
      "\n",
      "  [[-0.15726982]]]], shape=(1, 8, 1, 1), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 1)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.39369673 -5.3638535  -4.299967    0.42683712]]\n",
      "\n",
      "  [[ 1.5368607   0.19877002  0.7816559   1.5527248 ]]\n",
      "\n",
      "  [[-0.81651723 -2.4397342  -2.0768747  -0.87311447]]\n",
      "\n",
      "  [[-0.47767636 -0.3428551  -1.5112916  -0.44078764]]\n",
      "\n",
      "  [[-2.069729    1.0599291  -1.0277501  -2.072358  ]]\n",
      "\n",
      "  [[ 1.8522166  -2.6592717  -2.2402682   1.8493011 ]]\n",
      "\n",
      "  [[ 0.03498075 -3.8845823  -2.7170017   0.0608245 ]]\n",
      "\n",
      "  [[-0.32011715  3.515318    3.3192842  -0.349648  ]]]], shape=(1, 8, 1, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 4)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 1, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 1, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 1)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7592336 ]]\n",
      "\n",
      "  [[-0.12117013]]\n",
      "\n",
      "  [[-3.0542457 ]]\n",
      "\n",
      "  [[-2.936668  ]]\n",
      "\n",
      "  [[-0.6102953 ]]\n",
      "\n",
      "  [[-1.4482276 ]]\n",
      "\n",
      "  [[-2.6406937 ]]\n",
      "\n",
      "  [[-1.9032832 ]]]], shape=(1, 8, 1, 1), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 1)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 1, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 1, 256)\n",
      "(1, 1, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 1, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 1, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.4867603   2.4921799   1.7661256  -0.43306026]]\n",
      "\n",
      "  [[-0.5573432   1.9460608   1.0296916  -0.5577529 ]]\n",
      "\n",
      "  [[-0.6439734   1.3698922   1.2463032  -0.5808168 ]]\n",
      "\n",
      "  [[-1.6797403   4.254358    2.8988574  -1.7371421 ]]\n",
      "\n",
      "  [[-0.96380216  4.3040347   3.7092512  -1.0509831 ]]\n",
      "\n",
      "  [[-0.6760195   1.2851621   0.69724524 -0.69684   ]]\n",
      "\n",
      "  [[-0.22678642  2.485153    1.3785614  -0.21512853]]\n",
      "\n",
      "  [[-1.6091075   4.8962135   4.7211685  -1.5641763 ]]]], shape=(1, 8, 1, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 1, 4)\n",
      "output.shape = (1, 8, 1, 32)\n",
      "scaled_attention.shape= (1, 1, 8, 32)\n",
      "concat_attention.shape= (1, 1, 256)\n",
      "outputs.shape= (1, 1, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   3.5742848   0.47916737 -4.1808095 ]\n",
      "   [-7.0726357   5.2153454  -0.6191547  -7.686931  ]\n",
      "   [-5.0196514   3.5095148   0.26564068 -4.7125306 ]\n",
      "   [-2.6520565   4.155355    1.5553874  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    4.0801115  -0.8106586  -5.8514013 ]\n",
      "   [-6.897297    0.31018955  3.5017562  -6.787184  ]\n",
      "   [-1.6330746   2.2395508  -4.2915373  -0.98903894]\n",
      "   [-3.889495    4.1714396  -0.17852122 -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  4.3947716  -0.661001   -1.4663465 ]\n",
      "   [-6.2788625   1.928139   -0.8061733  -6.9638243 ]\n",
      "   [-7.623632    4.3519716   0.6232831  -7.3906612 ]\n",
      "   [-3.4774246   5.1761966   3.0670707  -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183  1.3455452   1.0637511  -1.0099144 ]\n",
      "   [-3.257084   -0.75491565  3.5904186  -1.9644443 ]\n",
      "   [-4.4190965   2.565092   -0.38624918 -3.4415853 ]\n",
      "   [-1.8414911  -0.91760015  0.36751118 -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    0.53381836 -2.409985   -1.7994132 ]\n",
      "   [-5.159744   -1.7272689  -3.6887574  -5.157165  ]\n",
      "   [-8.038345   -0.19349298 -5.0958123  -6.112233  ]\n",
      "   [ 0.2971933   1.319021    0.11702798  2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.821977   -2.6579876  -2.5024278 ]\n",
      "   [ 1.310264    0.03412449  2.41719     0.71974283]\n",
      "   [-2.0380263   2.2992606  -1.3999702  -1.9594809 ]\n",
      "   [-1.399356    2.0196993  -0.5995703  -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.6409757   1.2338101  -3.0664551 ]\n",
      "   [-6.85605    -2.036426    5.2613115  -5.019433  ]\n",
      "   [-5.874989   -0.90901834  2.837893   -6.5762978 ]\n",
      "   [-2.1759007  -0.67548347  3.5331502  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158  -0.53708357 -0.7094948  -2.1313064 ]\n",
      "   [-7.561357    3.697397   -4.1952496  -9.132537  ]\n",
      "   [-5.8729873  -0.09437667 -4.60697    -5.5851054 ]\n",
      "   [-4.791121   -0.5720516  -1.453231   -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.2474937   -0.41418695  -0.45134023  -6.4543934 ]\n",
      "   [ -6.469707    -0.57196766  -0.68407536  -8.534681  ]\n",
      "   [ -6.6292477   -0.28188297  -1.0462365   -8.814849  ]\n",
      "   [ -6.6225      -0.91170895  -1.4528327   -6.590887  ]]\n",
      "\n",
      "  [[  1.406956     1.0076551    5.707879     1.6527363 ]\n",
      "   [ -4.613382     4.554805     1.008847    -5.670259  ]\n",
      "   [ -3.934454     4.793562     1.5221102   -4.984705  ]\n",
      "   [ -2.0566604    1.2764972    4.5498753   -1.9361823 ]]\n",
      "\n",
      "  [[  1.1651802   11.755798     8.471589    -0.8650375 ]\n",
      "   [ -4.827062     5.9522486    3.516461    -6.2152934 ]\n",
      "   [ -4.54758      4.9799747    2.9965365   -5.7537866 ]\n",
      "   [ -0.7047034   11.391521     7.993846    -2.7024608 ]]\n",
      "\n",
      "  [[  1.4051645   -1.5436141   -0.61915714   2.300935  ]\n",
      "   [ -7.481064    -1.326507    -1.9729244   -8.849386  ]\n",
      "   [ -4.8875756   -0.8316178   -1.230764    -5.399683  ]\n",
      "   [  1.8524147   -3.3698123   -1.3004444    2.8076603 ]]\n",
      "\n",
      "  [[ 14.038909    -1.1073301    4.4311037   14.881157  ]\n",
      "   [-15.253063     7.0223446    0.30012316 -17.242483  ]\n",
      "   [-10.496829     6.2392097    1.8075695  -12.553988  ]\n",
      "   [ 11.9622965   -0.8325739    3.342078    11.930058  ]]\n",
      "\n",
      "  [[ 13.658282     1.1466336    4.6133533   13.644118  ]\n",
      "   [-11.285459     0.57440215  -0.90991193 -13.207878  ]\n",
      "   [ -4.382489     1.1987709    1.0016749   -6.2198586 ]\n",
      "   [ 13.9025755    2.6008193    5.8075957   13.900225  ]]\n",
      "\n",
      "  [[ -7.06424      1.0761411   -5.276111    -7.843285  ]\n",
      "   [ -7.1069236    1.9375304   -0.12720326  -7.099851  ]\n",
      "   [ -7.5099497    1.1038859   -1.7704768   -7.6377015 ]\n",
      "   [ -6.981106     1.2045658   -5.2073503   -7.964103  ]]\n",
      "\n",
      "  [[ 12.859538    -0.9277863    3.4094594   15.096983  ]\n",
      "   [ -7.5078464    4.23272      0.7433306   -9.381615  ]\n",
      "   [ -8.053277     3.265487     0.34209594  -8.793779  ]\n",
      "   [ 11.912383    -0.27324596   3.9327502   14.192233  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 2)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   1.2650883 ]\n",
      "   [-0.07833973 -1.6385591 ]]\n",
      "\n",
      "  [[-0.1123142  -0.79412526]\n",
      "   [ 0.55837923 -2.7966113 ]]\n",
      "\n",
      "  [[-0.10918517  0.25675547]\n",
      "   [ 0.29272753 -2.7020628 ]]\n",
      "\n",
      "  [[-0.22114104  0.3085206 ]\n",
      "   [-0.13387318  0.55366653]]\n",
      "\n",
      "  [[ 0.00076126 -0.05029001]\n",
      "   [ 0.80667967 -8.944626  ]]\n",
      "\n",
      "  [[-0.00999826  0.09598862]\n",
      "   [ 0.2912656  -0.56597483]]\n",
      "\n",
      "  [[-0.20863976  0.51779765]\n",
      "   [ 0.7387994  -3.49712   ]]\n",
      "\n",
      "  [[-0.15726982  0.2944175 ]\n",
      "   [ 0.54700506  0.43704087]]]], shape=(1, 8, 2, 2), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 2)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.39369673 -5.3638535  -4.299967    0.42683712]\n",
      "   [ 0.09401785 -2.7030208  -1.7961587   0.10376523]]\n",
      "\n",
      "  [[ 1.5368607   0.19877002  0.7816559   1.5527248 ]\n",
      "   [ 1.7588829  -3.2109199  -3.2330418   1.7221622 ]]\n",
      "\n",
      "  [[-0.81651723 -2.4397342  -2.0768747  -0.87311447]\n",
      "   [ 1.9779575  -4.3155456  -3.1932695   2.0055554 ]]\n",
      "\n",
      "  [[-0.47767636 -0.3428551  -1.5112916  -0.44078764]\n",
      "   [ 1.3792046  -6.2071404  -5.488946    1.4610296 ]]\n",
      "\n",
      "  [[-2.069729    1.0599291  -1.0277501  -2.072358  ]\n",
      "   [ 2.561066   -4.9379053  -3.1875715   2.6186526 ]]\n",
      "\n",
      "  [[ 1.8522166  -2.6592717  -2.2402682   1.8493011 ]\n",
      "   [ 1.1741358  -5.7214193  -6.150505    1.2088895 ]]\n",
      "\n",
      "  [[ 0.03498075 -3.8845823  -2.7170017   0.0608245 ]\n",
      "   [ 1.7409036  -1.1229707  -0.8143169   1.7229469 ]]\n",
      "\n",
      "  [[-0.32011715  3.515318    3.3192842  -0.349648  ]\n",
      "   [ 1.8891538  -2.260959   -2.4615166   1.9467384 ]]]], shape=(1, 8, 2, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 4)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 2, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 2, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 2)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7592336  -0.87955266]\n",
      "   [ 5.38469    -3.3206117 ]]\n",
      "\n",
      "  [[-0.12117013  0.21998915]\n",
      "   [-1.8788433  -0.13192056]]\n",
      "\n",
      "  [[-3.0542457   3.7293096 ]\n",
      "   [ 0.3945418  -2.993016  ]]\n",
      "\n",
      "  [[-2.936668    4.049849  ]\n",
      "   [-3.7457092  -0.32726535]]\n",
      "\n",
      "  [[-0.6102953  -0.7092985 ]\n",
      "   [ 2.0555217  -1.8799918 ]]\n",
      "\n",
      "  [[-1.4482276   2.392811  ]\n",
      "   [ 2.2391675  -3.2877162 ]]\n",
      "\n",
      "  [[-2.6406937   2.1987119 ]\n",
      "   [-0.79443204  1.1223845 ]]\n",
      "\n",
      "  [[-1.9032832   2.3328505 ]\n",
      "   [-0.86601186  1.4450381 ]]]], shape=(1, 8, 2, 2), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 2)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 2, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 2, 256)\n",
      "(1, 2, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 2, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 2, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.4867603   2.4921799   1.7661256  -0.43306026]\n",
      "   [ 0.3170338  -1.4328523  -1.3162262   0.29860467]]\n",
      "\n",
      "  [[-0.5573432   1.9460608   1.0296916  -0.5577529 ]\n",
      "   [-0.14389874 -3.6176033  -2.738098   -0.12130013]]\n",
      "\n",
      "  [[-0.6439734   1.3698922   1.2463032  -0.5808168 ]\n",
      "   [ 0.73294854 -2.1355572  -2.0845227   0.6757104 ]]\n",
      "\n",
      "  [[-1.6797403   4.254358    2.8988574  -1.7371421 ]\n",
      "   [ 2.275211   -8.141142   -7.0540643   2.3295786 ]]\n",
      "\n",
      "  [[-0.96380216  4.3040347   3.7092512  -1.0509831 ]\n",
      "   [ 1.1054089  -1.7995504  -0.8422471   1.116368  ]]\n",
      "\n",
      "  [[-0.6760195   1.2851621   0.69724524 -0.69684   ]\n",
      "   [ 0.68708336 -3.1938713  -3.26205     0.73202604]]\n",
      "\n",
      "  [[-0.22678642  2.485153    1.3785614  -0.21512853]\n",
      "   [ 0.5409065  -0.8980778  -0.4032854   0.52192277]]\n",
      "\n",
      "  [[-1.6091075   4.8962135   4.7211685  -1.5641763 ]\n",
      "   [ 0.7027493  -3.672305   -3.1662157   0.69015825]]]], shape=(1, 8, 2, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 2, 4)\n",
      "output.shape = (1, 8, 2, 32)\n",
      "scaled_attention.shape= (1, 2, 8, 32)\n",
      "concat_attention.shape= (1, 2, 256)\n",
      "outputs.shape= (1, 2, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   3.5742848   0.47916737 -4.1808095 ]\n",
      "   [-7.0726357   5.2153454  -0.6191547  -7.686931  ]\n",
      "   [-5.0196514   3.5095148   0.26564068 -4.7125306 ]\n",
      "   [-2.6520565   4.155355    1.5553874  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    4.0801115  -0.8106586  -5.8514013 ]\n",
      "   [-6.897297    0.31018955  3.5017562  -6.787184  ]\n",
      "   [-1.6330746   2.2395508  -4.2915373  -0.98903894]\n",
      "   [-3.889495    4.1714396  -0.17852122 -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  4.3947716  -0.661001   -1.4663465 ]\n",
      "   [-6.2788625   1.928139   -0.8061733  -6.9638243 ]\n",
      "   [-7.623632    4.3519716   0.6232831  -7.3906612 ]\n",
      "   [-3.4774246   5.1761966   3.0670707  -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183  1.3455452   1.0637511  -1.0099144 ]\n",
      "   [-3.257084   -0.75491565  3.5904186  -1.9644443 ]\n",
      "   [-4.4190965   2.565092   -0.38624918 -3.4415853 ]\n",
      "   [-1.8414911  -0.91760015  0.36751118 -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    0.53381836 -2.409985   -1.7994132 ]\n",
      "   [-5.159744   -1.7272689  -3.6887574  -5.157165  ]\n",
      "   [-8.038345   -0.19349298 -5.0958123  -6.112233  ]\n",
      "   [ 0.2971933   1.319021    0.11702798  2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.821977   -2.6579876  -2.5024278 ]\n",
      "   [ 1.310264    0.03412449  2.41719     0.71974283]\n",
      "   [-2.0380263   2.2992606  -1.3999702  -1.9594809 ]\n",
      "   [-1.399356    2.0196993  -0.5995703  -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.6409757   1.2338101  -3.0664551 ]\n",
      "   [-6.85605    -2.036426    5.2613115  -5.019433  ]\n",
      "   [-5.874989   -0.90901834  2.837893   -6.5762978 ]\n",
      "   [-2.1759007  -0.67548347  3.5331502  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158  -0.53708357 -0.7094948  -2.1313064 ]\n",
      "   [-7.561357    3.697397   -4.1952496  -9.132537  ]\n",
      "   [-5.8729873  -0.09437667 -4.60697    -5.5851054 ]\n",
      "   [-4.791121   -0.5720516  -1.453231   -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.2474937   -0.41418695  -0.45134023  -6.4543934 ]\n",
      "   [ -6.469707    -0.57196766  -0.68407536  -8.534681  ]\n",
      "   [ -6.6292477   -0.28188297  -1.0462365   -8.814849  ]\n",
      "   [ -6.6225      -0.91170895  -1.4528327   -6.590887  ]]\n",
      "\n",
      "  [[  1.406956     1.0076551    5.707879     1.6527363 ]\n",
      "   [ -4.613382     4.554805     1.008847    -5.670259  ]\n",
      "   [ -3.934454     4.793562     1.5221102   -4.984705  ]\n",
      "   [ -2.0566604    1.2764972    4.5498753   -1.9361823 ]]\n",
      "\n",
      "  [[  1.1651802   11.755798     8.471589    -0.8650375 ]\n",
      "   [ -4.827062     5.9522486    3.516461    -6.2152934 ]\n",
      "   [ -4.54758      4.9799747    2.9965365   -5.7537866 ]\n",
      "   [ -0.7047034   11.391521     7.993846    -2.7024608 ]]\n",
      "\n",
      "  [[  1.4051645   -1.5436141   -0.61915714   2.300935  ]\n",
      "   [ -7.481064    -1.326507    -1.9729244   -8.849386  ]\n",
      "   [ -4.8875756   -0.8316178   -1.230764    -5.399683  ]\n",
      "   [  1.8524147   -3.3698123   -1.3004444    2.8076603 ]]\n",
      "\n",
      "  [[ 14.038909    -1.1073301    4.4311037   14.881157  ]\n",
      "   [-15.253063     7.0223446    0.30012316 -17.242483  ]\n",
      "   [-10.496829     6.2392097    1.8075695  -12.553988  ]\n",
      "   [ 11.9622965   -0.8325739    3.342078    11.930058  ]]\n",
      "\n",
      "  [[ 13.658282     1.1466336    4.6133533   13.644118  ]\n",
      "   [-11.285459     0.57440215  -0.90991193 -13.207878  ]\n",
      "   [ -4.382489     1.1987709    1.0016749   -6.2198586 ]\n",
      "   [ 13.9025755    2.6008193    5.8075957   13.900225  ]]\n",
      "\n",
      "  [[ -7.06424      1.0761411   -5.276111    -7.843285  ]\n",
      "   [ -7.1069236    1.9375304   -0.12720326  -7.099851  ]\n",
      "   [ -7.5099497    1.1038859   -1.7704768   -7.6377015 ]\n",
      "   [ -6.981106     1.2045658   -5.2073503   -7.964103  ]]\n",
      "\n",
      "  [[ 12.859538    -0.9277863    3.4094594   15.096983  ]\n",
      "   [ -7.5078464    4.23272      0.7433306   -9.381615  ]\n",
      "   [ -8.053277     3.265487     0.34209594  -8.793779  ]\n",
      "   [ 11.912383    -0.27324596   3.9327502   14.192233  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 3)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.1967106   1.2650883   0.0383831 ]\n",
      "   [-0.07833973 -1.6385591  -2.1143677 ]\n",
      "   [-1.1880126   4.1137557   3.0179172 ]]\n",
      "\n",
      "  [[-0.1123142  -0.79412526 -0.1381328 ]\n",
      "   [ 0.55837923 -2.7966113  -1.2228664 ]\n",
      "   [ 0.09706794 -2.332889   -5.2742763 ]]\n",
      "\n",
      "  [[-0.10918517  0.25675547 -0.08015265]\n",
      "   [ 0.29272753 -2.7020628  -2.0944023 ]\n",
      "   [ 0.13902216  4.2903423   1.8733459 ]]\n",
      "\n",
      "  [[-0.22114104  0.3085206   0.05852008]\n",
      "   [-0.13387318  0.55366653  2.923324  ]\n",
      "   [-0.34453636  4.216789    1.4287623 ]]\n",
      "\n",
      "  [[ 0.00076126 -0.05029001 -0.5368199 ]\n",
      "   [ 0.80667967 -8.944626    4.4384046 ]\n",
      "   [-0.31940758 -1.0257616  -1.7917205 ]]\n",
      "\n",
      "  [[-0.00999826  0.09598862 -0.07227985]\n",
      "   [ 0.2912656  -0.56597483  2.359715  ]\n",
      "   [ 1.2374828  -7.7947125  -8.148528  ]]\n",
      "\n",
      "  [[-0.20863976  0.51779765  0.13784702]\n",
      "   [ 0.7387994  -3.49712     0.19189925]\n",
      "   [-1.6451546   5.585825   -3.8316076 ]]\n",
      "\n",
      "  [[-0.15726982  0.2944175  -0.45347476]\n",
      "   [ 0.54700506  0.43704087  0.04412017]\n",
      "   [-0.2792745   2.8704321  -0.6910876 ]]]], shape=(1, 8, 3, 3), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 3)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.39369673 -5.3638535  -4.299967    0.42683712]\n",
      "   [ 0.09401785 -2.7030208  -1.7961587   0.10376523]\n",
      "   [ 0.9433887  -6.8878565  -5.742896    0.9758723 ]]\n",
      "\n",
      "  [[ 1.5368607   0.19877002  0.7816559   1.5527248 ]\n",
      "   [ 1.7588829  -3.2109199  -3.2330418   1.7221622 ]\n",
      "   [ 1.1432179  -3.746661   -4.197606    1.1099446 ]]\n",
      "\n",
      "  [[-0.81651723 -2.4397342  -2.0768747  -0.87311447]\n",
      "   [ 1.9779575  -4.3155456  -3.1932695   2.0055554 ]\n",
      "   [ 2.1969419  -3.2358968  -2.2808077   2.2427182 ]]\n",
      "\n",
      "  [[-0.47767636 -0.3428551  -1.5112916  -0.44078764]\n",
      "   [ 1.3792046  -6.2071404  -5.488946    1.4610296 ]\n",
      "   [ 1.691331   -2.5009665  -1.2548616   1.7465296 ]]\n",
      "\n",
      "  [[-2.069729    1.0599291  -1.0277501  -2.072358  ]\n",
      "   [ 2.561066   -4.9379053  -3.1875715   2.6186526 ]\n",
      "   [ 1.748676   -2.8951848  -1.8181933   1.7950103 ]]\n",
      "\n",
      "  [[ 1.8522166  -2.6592717  -2.2402682   1.8493011 ]\n",
      "   [ 1.1741358  -5.7214193  -6.150505    1.2088895 ]\n",
      "   [ 0.875994   -4.3883533  -4.4932675   0.89939815]]\n",
      "\n",
      "  [[ 0.03498075 -3.8845823  -2.7170017   0.0608245 ]\n",
      "   [ 1.7409036  -1.1229707  -0.8143169   1.7229469 ]\n",
      "   [ 2.2427971  -4.334286   -4.5098004   2.2478068 ]]\n",
      "\n",
      "  [[-0.32011715  3.515318    3.3192842  -0.349648  ]\n",
      "   [ 1.8891538  -2.260959   -2.4615166   1.9467384 ]\n",
      "   [ 0.4268372  -0.6538068  -1.6192076   0.44938147]]]], shape=(1, 8, 3, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 4)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 3, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 3, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7592336  -0.87955266 -0.00951981]\n",
      "   [ 5.38469    -3.3206117  -4.0981503 ]\n",
      "   [-1.3342757  -0.14784217  7.6798544 ]]\n",
      "\n",
      "  [[-0.12117013  0.21998915  0.44533262]\n",
      "   [-1.8788433  -0.13192056  0.2545483 ]\n",
      "   [-1.0328436   1.6697764   1.103431  ]]\n",
      "\n",
      "  [[-3.0542457   3.7293096   1.4595202 ]\n",
      "   [ 0.3945418  -2.993016    0.6099984 ]\n",
      "   [-1.4144495  -3.7894874   1.6604149 ]]\n",
      "\n",
      "  [[-2.936668    4.049849    2.2889643 ]\n",
      "   [-3.7457092  -0.32726535  3.892179  ]\n",
      "   [ 0.8981065  -5.610611    0.92076606]]\n",
      "\n",
      "  [[-0.6102953  -0.7092985  -0.19395944]\n",
      "   [ 2.0555217  -1.8799918  -1.1317616 ]\n",
      "   [ 2.1910675  -1.835773   -0.66576225]]\n",
      "\n",
      "  [[-1.4482276   2.392811   -0.41352853]\n",
      "   [ 2.2391675  -3.2877162  -2.7943733 ]\n",
      "   [-1.0674446  -0.6383447  -1.3524429 ]]\n",
      "\n",
      "  [[-2.6406937   2.1987119   0.45130324]\n",
      "   [-0.79443204  1.1223845  -1.4550594 ]\n",
      "   [-0.71375805  1.466068   -0.46504158]]\n",
      "\n",
      "  [[-1.9032832   2.3328505   1.261809  ]\n",
      "   [-0.86601186  1.4450381   4.4363995 ]\n",
      "   [-0.0658852  -1.478034   -0.7516808 ]]]], shape=(1, 8, 3, 3), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 3)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 3, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 3, 256)\n",
      "(1, 3, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 3, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 3, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.4867603   2.4921799   1.7661256  -0.43306026]\n",
      "   [ 0.3170338  -1.4328523  -1.3162262   0.29860467]\n",
      "   [ 0.7253348  -0.6124671  -0.7818125   0.7233737 ]]\n",
      "\n",
      "  [[-0.5573432   1.9460608   1.0296916  -0.5577529 ]\n",
      "   [-0.14389874 -3.6176033  -2.738098   -0.12130013]\n",
      "   [-0.12784405 -3.4491856  -2.2729616  -0.12366522]]\n",
      "\n",
      "  [[-0.6439734   1.3698922   1.2463032  -0.5808168 ]\n",
      "   [ 0.73294854 -2.1355572  -2.0845227   0.6757104 ]\n",
      "   [ 0.8099066  -2.8337831  -2.8025277   0.7912951 ]]\n",
      "\n",
      "  [[-1.6797403   4.254358    2.8988574  -1.7371421 ]\n",
      "   [ 2.275211   -8.141142   -7.0540643   2.3295786 ]\n",
      "   [ 2.5686255  -8.248572   -6.870314    2.5952196 ]]\n",
      "\n",
      "  [[-0.96380216  4.3040347   3.7092512  -1.0509831 ]\n",
      "   [ 1.1054089  -1.7995504  -0.8422471   1.116368  ]\n",
      "   [ 1.222806   -5.428864   -3.977246    1.2649864 ]]\n",
      "\n",
      "  [[-0.6760195   1.2851621   0.69724524 -0.69684   ]\n",
      "   [ 0.68708336 -3.1938713  -3.26205     0.73202604]\n",
      "   [-0.87278605 -3.3989465  -3.5290165  -0.91642433]]\n",
      "\n",
      "  [[-0.22678642  2.485153    1.3785614  -0.21512853]\n",
      "   [ 0.5409065  -0.8980778  -0.4032854   0.52192277]\n",
      "   [ 0.33198375 -0.29659826  0.35777807  0.31423655]]\n",
      "\n",
      "  [[-1.6091075   4.8962135   4.7211685  -1.5641763 ]\n",
      "   [ 0.7027493  -3.672305   -3.1662157   0.69015825]\n",
      "   [-1.1087238  -0.15529159  0.09998463 -1.1015354 ]]]], shape=(1, 8, 3, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 3, 4)\n",
      "output.shape = (1, 8, 3, 32)\n",
      "scaled_attention.shape= (1, 3, 8, 32)\n",
      "concat_attention.shape= (1, 3, 256)\n",
      "outputs.shape= (1, 3, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-3.5907724   3.5742848   0.47916737 -4.1808095 ]\n",
      "   [-7.0726357   5.2153454  -0.6191547  -7.686931  ]\n",
      "   [-5.0196514   3.5095148   0.26564068 -4.7125306 ]\n",
      "   [-2.6520565   4.155355    1.5553874  -3.7211149 ]]\n",
      "\n",
      "  [[-5.830585    4.0801115  -0.8106586  -5.8514013 ]\n",
      "   [-6.897297    0.31018955  3.5017562  -6.787184  ]\n",
      "   [-1.6330746   2.2395508  -4.2915373  -0.98903894]\n",
      "   [-3.889495    4.1714396  -0.17852122 -3.5668743 ]]\n",
      "\n",
      "  [[ 0.21261294  4.3947716  -0.661001   -1.4663465 ]\n",
      "   [-6.2788625   1.928139   -0.8061733  -6.9638243 ]\n",
      "   [-7.623632    4.3519716   0.6232831  -7.3906612 ]\n",
      "   [-3.4774246   5.1761966   3.0670707  -3.6499627 ]]\n",
      "\n",
      "  [[-0.62310183  1.3455452   1.0637511  -1.0099144 ]\n",
      "   [-3.257084   -0.75491565  3.5904186  -1.9644443 ]\n",
      "   [-4.4190965   2.565092   -0.38624918 -3.4415853 ]\n",
      "   [-1.8414911  -0.91760015  0.36751118 -2.3266878 ]]\n",
      "\n",
      "  [[-3.365542    0.53381836 -2.409985   -1.7994132 ]\n",
      "   [-5.159744   -1.7272689  -3.6887574  -5.157165  ]\n",
      "   [-8.038345   -0.19349298 -5.0958123  -6.112233  ]\n",
      "   [ 0.2971933   1.319021    0.11702798  2.6413    ]]\n",
      "\n",
      "  [[-2.1686854   0.821977   -2.6579876  -2.5024278 ]\n",
      "   [ 1.310264    0.03412449  2.41719     0.71974283]\n",
      "   [-2.0380263   2.2992606  -1.3999702  -1.9594809 ]\n",
      "   [-1.399356    2.0196993  -0.5995703  -2.2406054 ]]\n",
      "\n",
      "  [[-0.3049526   0.6409757   1.2338101  -3.0664551 ]\n",
      "   [-6.85605    -2.036426    5.2613115  -5.019433  ]\n",
      "   [-5.874989   -0.90901834  2.837893   -6.5762978 ]\n",
      "   [-2.1759007  -0.67548347  3.5331502  -3.8550992 ]]\n",
      "\n",
      "  [[-2.8292158  -0.53708357 -0.7094948  -2.1313064 ]\n",
      "   [-7.561357    3.697397   -4.1952496  -9.132537  ]\n",
      "   [-5.8729873  -0.09437667 -4.60697    -5.5851054 ]\n",
      "   [-4.791121   -0.5720516  -1.453231   -4.8414316 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -6.2474937   -0.41418695  -0.45134023  -6.4543934 ]\n",
      "   [ -6.469707    -0.57196766  -0.68407536  -8.534681  ]\n",
      "   [ -6.6292477   -0.28188297  -1.0462365   -8.814849  ]\n",
      "   [ -6.6225      -0.91170895  -1.4528327   -6.590887  ]]\n",
      "\n",
      "  [[  1.406956     1.0076551    5.707879     1.6527363 ]\n",
      "   [ -4.613382     4.554805     1.008847    -5.670259  ]\n",
      "   [ -3.934454     4.793562     1.5221102   -4.984705  ]\n",
      "   [ -2.0566604    1.2764972    4.5498753   -1.9361823 ]]\n",
      "\n",
      "  [[  1.1651802   11.755798     8.471589    -0.8650375 ]\n",
      "   [ -4.827062     5.9522486    3.516461    -6.2152934 ]\n",
      "   [ -4.54758      4.9799747    2.9965365   -5.7537866 ]\n",
      "   [ -0.7047034   11.391521     7.993846    -2.7024608 ]]\n",
      "\n",
      "  [[  1.4051645   -1.5436141   -0.61915714   2.300935  ]\n",
      "   [ -7.481064    -1.326507    -1.9729244   -8.849386  ]\n",
      "   [ -4.8875756   -0.8316178   -1.230764    -5.399683  ]\n",
      "   [  1.8524147   -3.3698123   -1.3004444    2.8076603 ]]\n",
      "\n",
      "  [[ 14.038909    -1.1073301    4.4311037   14.881157  ]\n",
      "   [-15.253063     7.0223446    0.30012316 -17.242483  ]\n",
      "   [-10.496829     6.2392097    1.8075695  -12.553988  ]\n",
      "   [ 11.9622965   -0.8325739    3.342078    11.930058  ]]\n",
      "\n",
      "  [[ 13.658282     1.1466336    4.6133533   13.644118  ]\n",
      "   [-11.285459     0.57440215  -0.90991193 -13.207878  ]\n",
      "   [ -4.382489     1.1987709    1.0016749   -6.2198586 ]\n",
      "   [ 13.9025755    2.6008193    5.8075957   13.900225  ]]\n",
      "\n",
      "  [[ -7.06424      1.0761411   -5.276111    -7.843285  ]\n",
      "   [ -7.1069236    1.9375304   -0.12720326  -7.099851  ]\n",
      "   [ -7.5099497    1.1038859   -1.7704768   -7.6377015 ]\n",
      "   [ -6.981106     1.2045658   -5.2073503   -7.964103  ]]\n",
      "\n",
      "  [[ 12.859538    -0.9277863    3.4094594   15.096983  ]\n",
      "   [ -7.5078464    4.23272      0.7433306   -9.381615  ]\n",
      "   [ -8.053277     3.265487     0.34209594  -8.793779  ]\n",
      "   [ 11.912383    -0.27324596   3.9327502   14.192233  ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ -0.1967106    1.2650883    0.0383831    1.5173395 ]\n",
      "   [ -0.07833973  -1.6385591   -2.1143677   -5.7137694 ]\n",
      "   [ -1.1880126    4.1137557    3.0179172   -3.3403745 ]\n",
      "   [  1.1460481   -3.7008677   -1.4849414    7.2132688 ]]\n",
      "\n",
      "  [[ -0.1123142   -0.79412526  -0.1381328   -0.4884019 ]\n",
      "   [  0.55837923  -2.7966113   -1.2228664    4.100561  ]\n",
      "   [  0.09706794  -2.332889    -5.2742763   -4.946403  ]\n",
      "   [ -0.1293308    1.2772424    2.3478665    6.8632793 ]]\n",
      "\n",
      "  [[ -0.10918517   0.25675547  -0.08015265   0.267467  ]\n",
      "   [  0.29272753  -2.7020628   -2.0944023   -0.09006413]\n",
      "   [  0.13902216   4.2903423    1.8733459   -8.03871   ]\n",
      "   [ -0.07535618  -0.9568393    4.341821     4.9751606 ]]\n",
      "\n",
      "  [[ -0.22114104   0.3085206    0.05852008   0.62647724]\n",
      "   [ -0.13387318   0.55366653   2.923324     0.89485973]\n",
      "   [ -0.34453636   4.216789     1.4287623  -10.637009  ]\n",
      "   [ -0.96421295   4.7069335   -0.2518544   11.249042  ]]\n",
      "\n",
      "  [[  0.00076126  -0.05029001  -0.5368199    0.6157495 ]\n",
      "   [  0.80667967  -8.944626     4.4384046   -3.7252994 ]\n",
      "   [ -0.31940758  -1.0257616   -1.7917205    2.5628316 ]\n",
      "   [  0.03507771  -5.47304      1.3063068    0.22849207]]\n",
      "\n",
      "  [[ -0.00999826   0.09598862  -0.07227985   0.35138264]\n",
      "   [  0.2912656   -0.56597483   2.359715     1.9146878 ]\n",
      "   [  1.2374828   -7.7947125   -8.148528    -9.64528   ]\n",
      "   [ -0.49684775   1.0317777   -2.438945     3.4016101 ]]\n",
      "\n",
      "  [[ -0.20863976   0.51779765   0.13784702   0.2674416 ]\n",
      "   [  0.7387994   -3.49712      0.19189925  -0.6511103 ]\n",
      "   [ -1.6451546    5.585825    -3.8316076    4.3324323 ]\n",
      "   [  1.188459    -6.363043    -0.2511124    7.307282  ]]\n",
      "\n",
      "  [[ -0.15726982   0.2944175   -0.45347476   0.489363  ]\n",
      "   [  0.54700506   0.43704087   0.04412017   7.119749  ]\n",
      "   [ -0.2792745    2.8704321   -0.6910876    1.7381401 ]\n",
      "   [  0.52084863  -3.9356534   -2.3107355   11.3856535 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[ 0.39369673 -5.3638535  -4.299967    0.42683712]\n",
      "   [ 0.09401785 -2.7030208  -1.7961587   0.10376523]\n",
      "   [ 0.9433887  -6.8878565  -5.742896    0.9758723 ]\n",
      "   [ 1.9268049  -7.8178434  -6.3084736   1.9605205 ]]\n",
      "\n",
      "  [[ 1.5368607   0.19877002  0.7816559   1.5527248 ]\n",
      "   [ 1.7588829  -3.2109199  -3.2330418   1.7221622 ]\n",
      "   [ 1.1432179  -3.746661   -4.197606    1.1099446 ]\n",
      "   [ 2.37297    -1.9130998  -1.0165025   2.3694665 ]]\n",
      "\n",
      "  [[-0.81651723 -2.4397342  -2.0768747  -0.87311447]\n",
      "   [ 1.9779575  -4.3155456  -3.1932695   2.0055554 ]\n",
      "   [ 2.1969419  -3.2358968  -2.2808077   2.2427182 ]\n",
      "   [ 2.4244008  -4.4103413  -3.2382874   2.4459846 ]]\n",
      "\n",
      "  [[-0.47767636 -0.3428551  -1.5112916  -0.44078764]\n",
      "   [ 1.3792046  -6.2071404  -5.488946    1.4610296 ]\n",
      "   [ 1.691331   -2.5009665  -1.2548616   1.7465296 ]\n",
      "   [ 1.4270285  -7.6668034  -6.8956103   1.5365219 ]]\n",
      "\n",
      "  [[-2.069729    1.0599291  -1.0277501  -2.072358  ]\n",
      "   [ 2.561066   -4.9379053  -3.1875715   2.6186526 ]\n",
      "   [ 1.748676   -2.8951848  -1.8181933   1.7950103 ]\n",
      "   [ 3.7833207  -7.5089006  -5.03968     3.8595357 ]]\n",
      "\n",
      "  [[ 1.8522166  -2.6592717  -2.2402682   1.8493011 ]\n",
      "   [ 1.1741358  -5.7214193  -6.150505    1.2088895 ]\n",
      "   [ 0.875994   -4.3883533  -4.4932675   0.89939815]\n",
      "   [ 2.408106   -5.9485273  -4.6615996   2.403912  ]]\n",
      "\n",
      "  [[ 0.03498075 -3.8845823  -2.7170017   0.0608245 ]\n",
      "   [ 1.7409036  -1.1229707  -0.8143169   1.7229469 ]\n",
      "   [ 2.2427971  -4.334286   -4.5098004   2.2478068 ]\n",
      "   [ 2.3989706  -8.100318   -6.9591146   2.40688   ]]\n",
      "\n",
      "  [[-0.32011715  3.515318    3.3192842  -0.349648  ]\n",
      "   [ 1.8891538  -2.260959   -2.4615166   1.9467384 ]\n",
      "   [ 0.4268372  -0.6538068  -1.6192076   0.44938147]\n",
      "   [ 2.1330678  -5.0889425  -5.6528454   2.1717856 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.7592336  -0.87955266 -0.00951981 -0.576762  ]\n",
      "   [ 5.38469    -3.3206117  -4.0981503  -4.4255514 ]\n",
      "   [-1.3342757  -0.14784217  7.6798544   2.0427978 ]\n",
      "   [ 3.2525744  -1.5285779   0.16960649 -1.0395626 ]]\n",
      "\n",
      "  [[-0.12117013  0.21998915  0.44533262  1.8633864 ]\n",
      "   [-1.8788433  -0.13192056  0.2545483   3.7832806 ]\n",
      "   [-1.0328436   1.6697764   1.103431    2.6394668 ]\n",
      "   [ 0.8868122   1.2415353  -0.79364634  0.33945704]]\n",
      "\n",
      "  [[-3.0542457   3.7293096   1.4595202   1.8997238 ]\n",
      "   [ 0.3945418  -2.993016    0.6099984   4.181525  ]\n",
      "   [-1.4144495  -3.7894874   1.6604149   1.9274117 ]\n",
      "   [ 0.9517002  -1.5197383   0.64819217  7.4610047 ]]\n",
      "\n",
      "  [[-2.936668    4.049849    2.2889643   1.9815285 ]\n",
      "   [-3.7457092  -0.32726535  3.892179    4.9373293 ]\n",
      "   [ 0.8981065  -5.610611    0.92076606 -1.0158207 ]\n",
      "   [ 0.48950538 -0.70533055  0.09346531  2.736516  ]]\n",
      "\n",
      "  [[-0.6102953  -0.7092985  -0.19395944  1.0407317 ]\n",
      "   [ 2.0555217  -1.8799918  -1.1317616  -2.5900347 ]\n",
      "   [ 2.1910675  -1.835773   -0.66576225 -2.4578257 ]\n",
      "   [ 1.5110177   0.11601111  1.2768376  -2.2906756 ]]\n",
      "\n",
      "  [[-1.4482276   2.392811   -0.41352853 -0.06292001]\n",
      "   [ 2.2391675  -3.2877162  -2.7943733  -3.7890015 ]\n",
      "   [-1.0674446  -0.6383447  -1.3524429  -1.3689399 ]\n",
      "   [ 1.6714075  -2.4831796   0.37666214 -3.485249  ]]\n",
      "\n",
      "  [[-2.6406937   2.1987119   0.45130324  2.0597456 ]\n",
      "   [-0.79443204  1.1223845  -1.4550594   3.6012783 ]\n",
      "   [-0.71375805  1.466068   -0.46504158 -0.4466178 ]\n",
      "   [ 3.5965376   0.00654085 -4.2117333  -3.867798  ]]\n",
      "\n",
      "  [[-1.9032832   2.3328505   1.261809    0.68520564]\n",
      "   [-0.86601186  1.4450381   4.4363995   3.3688762 ]\n",
      "   [-0.0658852  -1.478034   -0.7516808  -2.1340375 ]\n",
      "   [ 1.6996344  -3.0175424  -3.9927328   1.8083292 ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "(1, 4, 256)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "split_heads()\n",
      "(1, 4, 256)\n",
      "(1, 4, 8, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "(1, 8, 4, 32)\n",
      "matmul_qk.shape = (1, 8, 4, 4)\n",
      "depth= tf.Tensor(32.0, shape=(), dtype=float32)\n",
      "logits= tf.Tensor(\n",
      "[[[[-0.4867603   2.4921799   1.7661256  -0.43306026]\n",
      "   [ 0.3170338  -1.4328523  -1.3162262   0.29860467]\n",
      "   [ 0.7253347  -0.612467   -0.7818124   0.7233737 ]\n",
      "   [ 1.8733263   0.89222586  1.417042    1.8756858 ]]\n",
      "\n",
      "  [[-0.5573432   1.9460608   1.0296916  -0.5577529 ]\n",
      "   [-0.14389874 -3.6176033  -2.738098   -0.12130013]\n",
      "   [-0.127844   -3.4491856  -2.2729616  -0.12366518]\n",
      "   [ 1.6123959  -7.478487   -5.0448256   1.6640153 ]]\n",
      "\n",
      "  [[-0.6439734   1.3698922   1.2463032  -0.5808168 ]\n",
      "   [ 0.73294854 -2.1355572  -2.0845227   0.6757104 ]\n",
      "   [ 0.8099066  -2.8337831  -2.8025274   0.7912951 ]\n",
      "   [ 2.0761864  -5.143691   -4.517317    2.052774  ]]\n",
      "\n",
      "  [[-1.6797403   4.254358    2.8988574  -1.7371421 ]\n",
      "   [ 2.275211   -8.141142   -7.0540643   2.3295786 ]\n",
      "   [ 2.5686255  -8.248572   -6.870314    2.5952196 ]\n",
      "   [ 3.120443   -6.341541   -4.9197607   3.2046647 ]]\n",
      "\n",
      "  [[-0.96380216  4.3040347   3.7092512  -1.0509831 ]\n",
      "   [ 1.1054089  -1.7995504  -0.8422471   1.116368  ]\n",
      "   [ 1.222806   -5.428864   -3.9772465   1.2649864 ]\n",
      "   [ 1.5145204  -2.165199   -1.2883748   1.5283496 ]]\n",
      "\n",
      "  [[-0.6760195   1.2851621   0.69724524 -0.69684   ]\n",
      "   [ 0.68708336 -3.1938713  -3.26205     0.73202604]\n",
      "   [-0.87278605 -3.3989472  -3.5290167  -0.91642433]\n",
      "   [ 1.0090908  -2.3928626  -2.3666325   1.044272  ]]\n",
      "\n",
      "  [[-0.22678642  2.485153    1.3785614  -0.21512853]\n",
      "   [ 0.5409065  -0.8980778  -0.4032854   0.52192277]\n",
      "   [ 0.33198366 -0.29659846  0.35777786  0.3142365 ]\n",
      "   [ 0.5317069  -2.8009124  -2.2721531   0.51372916]]\n",
      "\n",
      "  [[-1.6091075   4.8962135   4.7211685  -1.5641763 ]\n",
      "   [ 0.7027493  -3.672305   -3.1662157   0.69015825]\n",
      "   [-1.1087236  -0.15529133  0.09998485 -1.1015354 ]\n",
      "   [ 2.3846087  -4.781105   -3.9281719   2.37806   ]]]], shape=(1, 8, 4, 4), dtype=float32)\n",
      "attention_weights.shape = (1, 8, 4, 4)\n",
      "output.shape = (1, 8, 4, 32)\n",
      "scaled_attention.shape= (1, 4, 8, 32)\n",
      "concat_attention.shape= (1, 4, 256)\n",
      "outputs.shape= (1, 4, 256)\n",
      "Input: 게임하자\n",
      "Output: 게임하세요 !\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"게임하자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
